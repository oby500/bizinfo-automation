#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì •ë¶€ì§€ì›ì‚¬ì—… ê²€ìƒ‰ ì‹œìŠ¤í…œ - FastAPI ë°±ì—”ë“œ (ì‹¤ì œ DB ì—°ë™)
ê°œì„ ì‚¬í•­:
- ì‹¤ì œ Supabase ë°ì´í„° ì—°ë™
- ì§„í–‰/ë§ˆê°/ì¢…ë£Œ ìƒíƒœ í•„í„°ë§ 
- í†µê³„ ì •ë³´ ì‹¤ì‹œê°„ ê³„ì‚°
- ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
"""

from fastapi import FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from pathlib import Path
import os
from dotenv import load_dotenv
from supabase import create_client
import logging
import traceback
from openai import OpenAI
import asyncio

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (ë¡œê¹…ë³´ë‹¤ ë¨¼ì €)
load_dotenv()

# êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì • (JSON í˜•ì‹)
import json as json_lib
import sys

class StructuredFormatter(logging.Formatter):
    """JSON í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ë¡œê·¸ í¬ë§·í„°"""

    def format(self, record):
        log_data = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }

        # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í¬í•¨
        if hasattr(record, 'context'):
            log_data['context'] = record.context

        # ì—ëŸ¬ ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨
        if record.exc_info:
            log_data['exception'] = self.formatException(record.exc_info)

        return json_lib.dumps(log_data, ensure_ascii=False)

# í™˜ê²½ë³€ìˆ˜ë¡œ ë¡œê¹… í˜•ì‹ ì„ íƒ (JSON ë˜ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸)
LOG_FORMAT = os.getenv("LOG_FORMAT", "json")  # json or text

if LOG_FORMAT == "json":
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(StructuredFormatter())
    logging.basicConfig(level=logging.INFO, handlers=[handler])
    logger = logging.getLogger(__name__)
    logger.info("âœ… êµ¬ì¡°í™”ëœ ë¡œê¹… (JSON) í™œì„±í™”")
else:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)
    logger.info("âœ… ì¼ë°˜ í…ìŠ¤íŠ¸ ë¡œê¹… í™œì„±í™”")

# Rate Limiting (ë¡œê¹… ì´í›„ì— ì´ˆê¸°í™”)
try:
    from slowapi import Limiter, _rate_limit_exceeded_handler
    from slowapi.util import get_remote_address
    from slowapi.errors import RateLimitExceeded
    RATE_LIMIT_ENABLED = True
    logger.info("âœ… slowapi ì„¤ì¹˜ë¨ - Rate Limiting í™œì„±í™”")
except ImportError:
    RATE_LIMIT_ENABLED = False
    logger.warning("âš ï¸ slowapi ë¯¸ì„¤ì¹˜ - Rate Limiting ë¹„í™œì„±í™” (pip install slowapi í•„ìš”)")

# orjson import (í•œê¸€ ê¹¨ì§ ë°©ì§€)
try:
    import orjson
    from fastapi.responses import ORJSONResponse
    default_response_class = ORJSONResponse
    logger.info("âœ… orjson ì‚¬ìš© (í•œê¸€ ì¸ì½”ë”© ìµœì í™”)")
except ImportError:
    default_response_class = None
    logger.warning("âš ï¸ orjson ë¯¸ì„¤ì¹˜ - ê¸°ë³¸ JSON ì‚¬ìš©")

# API ë²„ì „ ê´€ë¦¬ ì„¤ì •
API_VERSION = "3.0.0"
API_VERSION_MAJOR = 3
API_VERSION_MINOR = 0
API_VERSION_PATCH = 0

# FastAPI ì•± ìƒì„± (í•œê¸€ ê¹¨ì§ ë°©ì§€)
app = FastAPI(
    title="ì •ë¶€ì§€ì›ì‚¬ì—… API",
    version=API_VERSION,
    description=f"K-Startup, BizInfo í†µí•© ê²€ìƒ‰ API (v{API_VERSION})",
    default_response_class=default_response_class,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# Rate Limiter ì„¤ì • (slowapi ì„¤ì¹˜ ì‹œì—ë§Œ í™œì„±í™”)
if RATE_LIMIT_ENABLED:
    limiter = Limiter(key_func=get_remote_address)
    app.state.limiter = limiter
    app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
    logger.info("âœ… Rate Limiting í™œì„±í™”: ë¶„ë‹¹ 60íšŒ ì œí•œ")
else:
    limiter = None
    logger.warning("âš ï¸ Rate Limiting ë¹„í™œì„±í™” ìƒíƒœ")

# CORS ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
CORS_ORIGINS = os.getenv("CORS_ORIGINS", "http://localhost:3000,http://localhost:8000").split(",")
logger.info(f"CORS allowed origins: {CORS_ORIGINS}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,  # í™˜ê²½ë³€ìˆ˜ì—ì„œ í—ˆìš© ë„ë©”ì¸ ëª©ë¡ ë¡œë“œ
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],  # ëª…ì‹œì  ë©”ì„œë“œ ì œí•œ
    allow_headers=["Content-Type", "Authorization", "X-Requested-With"],  # ëª…ì‹œì  í—¤ë” ì œí•œ
    max_age=3600,  # Preflight ìºì‹œ 1ì‹œê°„
)

# Gzip ì••ì¶• ë¯¸ë“¤ì›¨ì–´ (ì‘ë‹µ í¬ê¸° 1KB ì´ìƒì¼ ë•Œ ì••ì¶•)
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ë³´ì•ˆ í—¤ë” ë¯¸ë“¤ì›¨ì–´
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request

class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """ë³´ì•ˆ í—¤ë” ë° API ë²„ì „ ì •ë³´ ì¶”ê°€ ë¯¸ë“¤ì›¨ì–´"""
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        # XSS ê³µê²© ë°©ì–´
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        # HTTPS ê°•ì œ (í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œë§Œ)
        if os.getenv("ENV") == "production":
            response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        # Content Security Policy
        response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'"

        # API ë²„ì „ ì •ë³´ í—¤ë”
        response.headers["X-API-Version"] = API_VERSION
        response.headers["X-API-Version-Major"] = str(API_VERSION_MAJOR)
        response.headers["X-API-Version-Minor"] = str(API_VERSION_MINOR)
        response.headers["X-API-Version-Patch"] = str(API_VERSION_PATCH)

        return response

# ================================================
# API ì‘ë‹µ ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì  ì‹œìŠ¤í…œ
# ================================================
from collections import defaultdict
from typing import DefaultDict
import time

# ì—”ë“œí¬ì¸íŠ¸ë³„ ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
endpoint_metrics: DefaultDict[str, Dict[str, Any]] = defaultdict(lambda: {
    "count": 0,
    "total_time": 0.0,
    "min_time": float('inf'),
    "max_time": 0.0,
    "response_times": []  # ìµœê·¼ 100ê°œ ìš”ì²­ ì €ì¥ (íˆìŠ¤í† ê·¸ë¨ìš©)
})

def update_endpoint_metrics(path: str, method: str, response_time_ms: float):
    """ì—”ë“œí¬ì¸íŠ¸ë³„ ì‘ë‹µ ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
    endpoint_key = f"{method} {path}"
    metrics = endpoint_metrics[endpoint_key]

    # ì¹´ìš´íŠ¸ ë° í•©ê³„ ì—…ë°ì´íŠ¸
    metrics["count"] += 1
    metrics["total_time"] += response_time_ms

    # ìµœì†Œ/ìµœëŒ€ê°’ ì—…ë°ì´íŠ¸
    metrics["min_time"] = min(metrics["min_time"], response_time_ms)
    metrics["max_time"] = max(metrics["max_time"], response_time_ms)

    # ìµœê·¼ 100ê°œ ìš”ì²­ ì‹œê°„ ì €ì¥ (íˆìŠ¤í† ê·¸ë¨ ìƒì„±ìš©)
    metrics["response_times"].append(response_time_ms)
    if len(metrics["response_times"]) > 100:
        metrics["response_times"].pop(0)

def get_endpoint_metrics_summary() -> Dict[str, Any]:
    """ì—”ë“œí¬ì¸íŠ¸ë³„ ë©”íŠ¸ë¦­ ìš”ì•½ ë°˜í™˜"""
    summary = {}

    for endpoint, metrics in endpoint_metrics.items():
        if metrics["count"] > 0:
            avg_time = metrics["total_time"] / metrics["count"]

            # P50, P95, P99 ê³„ì‚° (ìµœê·¼ ìš”ì²­ ê¸°ì¤€)
            sorted_times = sorted(metrics["response_times"])
            n = len(sorted_times)

            p50 = sorted_times[int(n * 0.5)] if n > 0 else 0
            p95 = sorted_times[int(n * 0.95)] if n > 0 else 0
            p99 = sorted_times[int(n * 0.99)] if n > 0 else 0

            summary[endpoint] = {
                "count": metrics["count"],
                "avg_ms": round(avg_time, 2),
                "min_ms": round(metrics["min_time"], 2) if metrics["min_time"] != float('inf') else 0,
                "max_ms": round(metrics["max_time"], 2),
                "p50_ms": round(p50, 2),
                "p95_ms": round(p95, 2),
                "p99_ms": round(p99, 2)
            }

    return summary

logger.info("âœ… API ì‘ë‹µ ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì  ì‹œìŠ¤í…œ í™œì„±í™”")

class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """ìš”ì²­/ì‘ë‹µ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´ (ìš”ì²­ ê²€ì¦ + Correlation ID + ë©”íŠ¸ë¦­ ì¶”ì )"""
    async def dispatch(self, request: Request, call_next):
        # ìš”ì²­ ì‹œì‘ ì‹œê°„
        start_time = time.time()

        # Correlation ID ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        import uuid
        correlation_id = request.headers.get("X-Correlation-ID") or str(uuid.uuid4())

        # ìš”ì²­ ì •ë³´
        method = request.method
        url = str(request.url)
        path = request.url.path
        client_host = request.client.host if request.client else "unknown"

        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë¡œê¹…
        query_params = dict(request.query_params) if request.query_params else {}

        # ìš”ì²­ ë³¸ë¬¸ ë¡œê¹… (POST/PUT/PATCHë§Œ)
        request_body = None
        if method in ["POST", "PUT", "PATCH"]:
            try:
                # ìš”ì²­ ë³¸ë¬¸ ì½ê¸° (ë¹„ë™ê¸°)
                body_bytes = await request.body()
                if body_bytes:
                    # JSON íŒŒì‹± ì‹œë„
                    try:
                        import json
                        request_body = json.loads(body_bytes.decode())
                        # ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹ (ë¹„ë°€ë²ˆí˜¸, í† í° ë“±)
                        if isinstance(request_body, dict):
                            for key in ["password", "token", "api_key", "secret"]:
                                if key in request_body:
                                    request_body[key] = "***MASKED***"
                    except:
                        request_body = f"<{len(body_bytes)} bytes>"
            except Exception as e:
                request_body = f"<Error reading body: {e}>"

        # ì‘ë‹µ ì²˜ë¦¬
        response = await call_next(request)

        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        process_time = (time.time() - start_time) * 1000  # ë°€ë¦¬ì´ˆ

        # ì—”ë“œí¬ì¸íŠ¸ë³„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        update_endpoint_metrics(path, method, process_time)

        # ë¡œê·¸ ê¸°ë¡ (JSON í˜•ì‹ìœ¼ë¡œ - ê²€ì¦ ì •ë³´ + Correlation ID í¬í•¨)
        log_data = {
            "correlation_id": correlation_id,
            "method": method,
            "url": url,
            "path": path,
            "client": client_host,
            "query_params": query_params if query_params else None,
            "request_body": request_body if request_body else None,
            "status_code": response.status_code,
            "response_time_ms": round(process_time, 2)
        }

        # ìƒíƒœì½”ë“œì— ë”°ë¼ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
        if response.status_code >= 500:
            logger.error(f"[API] Request failed: {method} {path} [ID: {correlation_id[:8]}]", extra={"context": log_data})
        elif response.status_code >= 400:
            logger.warning(f"[API] Client error: {method} {path} [ID: {correlation_id[:8]}]", extra={"context": log_data})
        else:
            # ì¼ë°˜ ìš”ì²­ì€ ê°„ë‹¨í•˜ê²Œ ë¡œê¹…
            if query_params or request_body:
                logger.info(f"[API] {method} {path} - {response.status_code} - {round(process_time, 2)}ms [ID: {correlation_id[:8]}]")
            else:
                logger.info(f"[API] {method} {path} - {response.status_code} - {round(process_time, 2)}ms [ID: {correlation_id[:8]}]")

        # ì‘ë‹µ í—¤ë” ì¶”ê°€
        response.headers["X-Correlation-ID"] = correlation_id
        response.headers["X-Process-Time"] = str(round(process_time, 2))

        return response

class RateLimitMiddleware(BaseHTTPMiddleware):
    """ìš”ì²­ ì†ë„ ì œí•œ ë¯¸ë“¤ì›¨ì–´ (IP ê¸°ë°˜)"""
    def __init__(self, app, max_requests: int = 100, time_window: int = 60):
        """
        Args:
            app: FastAPI ì•±
            max_requests: ì‹œê°„ì°½ ë‚´ ìµœëŒ€ ìš”ì²­ ìˆ˜ (ê¸°ë³¸ê°’: 100)
            time_window: ì‹œê°„ì°½ í¬ê¸°(ì´ˆ) (ê¸°ë³¸ê°’: 60ì´ˆ)
        """
        super().__init__(app)
        self.max_requests = max_requests
        self.time_window = time_window
        self.request_counts = {}  # {ip: [(timestamp1, timestamp2, ...)]}
        self.cleanup_interval = 60  # 1ë¶„ë§ˆë‹¤ ë§Œë£Œëœ ê¸°ë¡ ì •ë¦¬
        self.last_cleanup = time.time()

    async def dispatch(self, request: Request, call_next):
        # í˜„ì¬ ì‹œê°„
        current_time = time.time()

        # IP ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
        client_ip = request.client.host if request.client else "unknown"

        # Health check ì—”ë“œí¬ì¸íŠ¸ëŠ” ì œí•œí•˜ì§€ ì•ŠìŒ
        if request.url.path in ["/health", "/api/health"]:
            return await call_next(request)

        # ì£¼ê¸°ì ìœ¼ë¡œ ë§Œë£Œëœ ê¸°ë¡ ì •ë¦¬
        if current_time - self.last_cleanup > self.cleanup_interval:
            self._cleanup_old_requests(current_time)
            self.last_cleanup = current_time

        # IPë³„ ìš”ì²­ ê¸°ë¡ í™•ì¸
        if client_ip not in self.request_counts:
            self.request_counts[client_ip] = []

        # ì‹œê°„ì°½ ë‚´ ìš”ì²­ë§Œ í•„í„°ë§
        window_start = current_time - self.time_window
        self.request_counts[client_ip] = [
            ts for ts in self.request_counts[client_ip]
            if ts > window_start
        ]

        # ìš”ì²­ íšŸìˆ˜ í™•ì¸
        if len(self.request_counts[client_ip]) >= self.max_requests:
            logger.warning(f"ğŸš¨ Rate limit exceeded for IP: {client_ip} ({len(self.request_counts[client_ip])} requests)")
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Too Many Requests",
                    "message": f"ìš”ì²­ íšŸìˆ˜ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. {self.time_window}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "retry_after": self.time_window
                },
                headers={"Retry-After": str(self.time_window)}
            )

        # í˜„ì¬ ìš”ì²­ ê¸°ë¡ ì¶”ê°€
        self.request_counts[client_ip].append(current_time)

        # ìš”ì²­ ì²˜ë¦¬
        response = await call_next(request)

        # Rate limit ì •ë³´ë¥¼ í—¤ë”ì— ì¶”ê°€
        response.headers["X-RateLimit-Limit"] = str(self.max_requests)
        response.headers["X-RateLimit-Remaining"] = str(self.max_requests - len(self.request_counts[client_ip]))
        response.headers["X-RateLimit-Reset"] = str(int(current_time + self.time_window))

        return response

    def _cleanup_old_requests(self, current_time: float):
        """ë§Œë£Œëœ ìš”ì²­ ê¸°ë¡ ì •ë¦¬"""
        window_start = current_time - self.time_window
        for ip in list(self.request_counts.keys()):
            self.request_counts[ip] = [
                ts for ts in self.request_counts[ip]
                if ts > window_start
            ]
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ëŠ” ì œê±°
            if not self.request_counts[ip]:
                del self.request_counts[ip]

        if self.request_counts:
            logger.info(f"ğŸ§¹ Rate limit ê¸°ë¡ ì •ë¦¬ ì™„ë£Œ ({len(self.request_counts)}ê°œ IP ì¶”ì  ì¤‘)")

    def get_stats(self) -> Dict[str, Any]:
        """Rate Limiter í†µê³„ ì •ë³´ ë°˜í™˜"""
        current_time = time.time()
        total_requests = sum(len(requests) for requests in self.request_counts.values())

        # ê°€ì¥ ë§ì´ ìš”ì²­í•œ IP ì°¾ê¸°
        top_ips = sorted(
            self.request_counts.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )[:5]  # ìƒìœ„ 5ê°œ IP

        return {
            "enabled": True,
            "max_requests": self.max_requests,
            "time_window_seconds": self.time_window,
            "tracked_ips": len(self.request_counts),
            "total_active_requests": total_requests,
            "top_ips": [
                {
                    "ip": ip,
                    "request_count": len(requests),
                    "percentage": round(len(requests) / self.max_requests * 100, 1)
                }
                for ip, requests in top_ips
            ],
            "last_cleanup": datetime.fromtimestamp(self.last_cleanup).isoformat()
        }

# í™˜ê²½ë³€ìˆ˜ì—ì„œ Rate Limit ì„¤ì • ê°€ì ¸ì˜¤ê¸°
RATE_LIMIT_MAX_REQUESTS = int(os.getenv("RATE_LIMIT_MAX_REQUESTS", "100"))
RATE_LIMIT_TIME_WINDOW = int(os.getenv("RATE_LIMIT_TIME_WINDOW", "60"))

# Rate Limiter ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥ (í—¬ìŠ¤ì²´í¬ì—ì„œ ì ‘ê·¼í•˜ê¸° ìœ„í•´)
_rate_limiter_instance = None

def get_rate_limiter_stats():
    """Rate Limiter í†µê³„ ë°˜í™˜ (í—¬ìŠ¤ì²´í¬ìš©)"""
    if _rate_limiter_instance:
        return _rate_limiter_instance.get_stats()
    return {"enabled": False, "message": "Rate limiter not initialized"}

# GZip ì••ì¶• ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ (í° ì‘ë‹µ ìµœì í™”)
from fastapi.middleware.gzip import GZipMiddleware

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì••ì¶• ì„¤ì • ê°€ì ¸ì˜¤ê¸°
GZIP_MINIMUM_SIZE = int(os.getenv("GZIP_MINIMUM_SIZE", "1000"))  # 1KB ì´ìƒë§Œ ì••ì¶•

app.add_middleware(GZipMiddleware, minimum_size=GZIP_MINIMUM_SIZE)
app.add_middleware(SecurityHeadersMiddleware)
app.add_middleware(RequestLoggingMiddleware)

# Rate Limiter ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ ë° ì¸ìŠ¤í„´ìŠ¤ ì €ì¥
class RateLimiterWrapper(RateLimitMiddleware):
    """Rate Limiterë¥¼ ë˜í•‘í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•¨"""
    def __init__(self, app, **kwargs):
        super().__init__(app, **kwargs)
        global _rate_limiter_instance
        _rate_limiter_instance = self

app.add_middleware(
    RateLimiterWrapper,
    max_requests=RATE_LIMIT_MAX_REQUESTS,
    time_window=RATE_LIMIT_TIME_WINDOW
)

# ================================================
# ì—ëŸ¬ ì‘ë‹µ í‘œì¤€í™” (Exception Handler)
# ================================================
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse

class StandardErrorResponse:
    """í‘œì¤€í™”ëœ ì—ëŸ¬ ì‘ë‹µ í¬ë§·"""
    @staticmethod
    def create(status_code: int, error_type: str, message: str, details: Any = None):
        response = {
            "success": False,
            "error": {
                "type": error_type,
                "message": message,
                "status_code": status_code,
                "timestamp": datetime.now().isoformat()
            }
        }
        if details:
            response["error"]["details"] = details
        return response

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTPException í‘œì¤€í™”"""
    return JSONResponse(
        status_code=exc.status_code,
        content=StandardErrorResponse.create(
            status_code=exc.status_code,
            error_type="HTTPException",
            message=exc.detail
        )
    )

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Request Validation Error í‘œì¤€í™”"""
    return JSONResponse(
        status_code=422,
        content=StandardErrorResponse.create(
            status_code=422,
            error_type="ValidationError",
            message="ìš”ì²­ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨",
            details=exc.errors()
        )
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """ì¼ë°˜ ì˜ˆì™¸ í‘œì¤€í™”"""
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content=StandardErrorResponse.create(
            status_code=500,
            error_type="InternalServerError",
            message="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )
    )

# ============================================================================
# ë¼ìš°í„° ë“±ë¡ (Routers Registration)
# ============================================================================

# Bookmark ë¼ìš°í„° ë“±ë¡
try:
    from routers import bookmark
    app.include_router(bookmark.router)
    logger.info("âœ… Bookmark ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
except Exception as e:
    logger.warning(f"âš ï¸ Bookmark ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {str(e)}")

# Application Writer ë¼ìš°í„° ë“±ë¡
try:
    from routers import application_impl
    app.include_router(application_impl.router)
    logger.info("âœ… Application Writer ë¼ìš°í„° ë“±ë¡ ì™„ë£Œ")
except Exception as e:
    logger.warning(f"âš ï¸ Application Writer ë¼ìš°í„° ë“±ë¡ ì‹¤íŒ¨: {str(e)}")

# ============================================================================

# Supabase ì—°ê²° ì¬ì‹œë„ í•¨ìˆ˜ (ì§€ìˆ˜ ë°±ì˜¤í”„)
def connect_to_supabase_with_retry(max_retries=3):
    """
    Supabase ì—°ê²° ì‹œë„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

    Args:
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)

    Returns:
        Supabase í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” None
    """
    import httpx

    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

    if not SUPABASE_URL or not SUPABASE_KEY:
        logger.error("âŒ Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return None

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì»¤ë„¥ì…˜ í’€ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    MAX_CONNECTIONS = int(os.getenv("DB_MAX_CONNECTIONS", "10"))
    REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "30"))

    for attempt in range(1, max_retries + 1):
        try:
            # httpx í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì»¤ë„¥ì…˜ í’€ ì„¤ì •)
            http_client = httpx.Client(
                limits=httpx.Limits(
                    max_connections=MAX_CONNECTIONS,
                    max_keepalive_connections=MAX_CONNECTIONS // 2
                ),
                timeout=httpx.Timeout(REQUEST_TIMEOUT)
            )

            # Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            from supabase.client import ClientOptions

            client = create_client(
                SUPABASE_URL,
                SUPABASE_KEY,
                options=ClientOptions(
                    auto_refresh_token=False,
                    persist_session=False
                )
            )

            # ì—°ê²° í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ ì¿¼ë¦¬ ì‹¤í–‰)
            test_result = client.table('kstartup_complete').select("announcement_id").limit(1).execute()

            logger.info(f"âœ… Supabase ì—°ê²° ì„±ê³µ (ì‹œë„: {attempt}/{max_retries}, ì»¤ë„¥ì…˜ í’€: {MAX_CONNECTIONS}, íƒ€ì„ì•„ì›ƒ: {REQUEST_TIMEOUT}ì´ˆ)")
            return client

        except Exception as e:
            wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„: 2ì´ˆ, 4ì´ˆ, 8ì´ˆ
            logger.warning(f"âš ï¸ Supabase ì—°ê²° ì‹œë„ {attempt}/{max_retries} ì‹¤íŒ¨: {e}")

            if attempt < max_retries:
                logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
            else:
                logger.error(f"âŒ Supabase ì—°ê²° ìµœì¢… ì‹¤íŒ¨ (ëª¨ë“  ì¬ì‹œë„ ì†Œì§„)")
                return None

    return None

# ================================================
# ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
# ================================================
from collections import defaultdict
from threading import Lock

# DB ì¿¼ë¦¬ í†µê³„ ì¶”ì 
db_query_stats = {
    "total_queries": 0,
    "successful_queries": 0,
    "failed_queries": 0,
    "total_query_time": 0.0,
    "queries_by_table": defaultdict(int),
    "errors": []
}
db_stats_lock = Lock()

def track_db_query(table_name: str, execution_time: float, success: bool, error: str = None):
    """DB ì¿¼ë¦¬ ì‹¤í–‰ ì¶”ì """
    with db_stats_lock:
        db_query_stats["total_queries"] += 1
        db_query_stats["total_query_time"] += execution_time

        if success:
            db_query_stats["successful_queries"] += 1
        else:
            db_query_stats["failed_queries"] += 1
            if error and len(db_query_stats["errors"]) < 100:  # ìµœê·¼ 100ê°œ ì—ëŸ¬ë§Œ ì €ì¥
                db_query_stats["errors"].append({
                    "table": table_name,
                    "error": str(error)[:200],  # ì—ëŸ¬ ë©”ì‹œì§€ 200ì ì œí•œ
                    "timestamp": datetime.now().isoformat()
                })

        db_query_stats["queries_by_table"][table_name] += 1

def get_db_connection_stats() -> Dict[str, Any]:
    """DB ì»¤ë„¥ì…˜ í’€ í†µê³„ ë°˜í™˜"""
    with db_stats_lock:
        total = db_query_stats["total_queries"]
        success_rate = (db_query_stats["successful_queries"] / total * 100) if total > 0 else 0
        avg_query_time = (db_query_stats["total_query_time"] / total) if total > 0 else 0

        return {
            "connection_pool": {
                "max_connections": MAX_CONNECTIONS,
                "timeout_seconds": REQUEST_TIMEOUT,
                "status": "healthy" if supabase else "disconnected"
            },
            "query_statistics": {
                "total_queries": total,
                "successful_queries": db_query_stats["successful_queries"],
                "failed_queries": db_query_stats["failed_queries"],
                "success_rate_percentage": round(success_rate, 2),
                "average_query_time_ms": round(avg_query_time * 1000, 2)
            },
            "queries_by_table": dict(db_query_stats["queries_by_table"]),
            "recent_errors": db_query_stats["errors"][-10:]  # ìµœê·¼ 10ê°œ ì—ëŸ¬ë§Œ ë°˜í™˜
        }

logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í™œì„±í™”")

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì»¤ë„¥ì…˜ í’€ë§ ìµœì í™” + ì¬ì‹œë„ ë¡œì§)
supabase = connect_to_supabase_with_retry(max_retries=3)
MAX_CONNECTIONS = int(os.getenv("DB_MAX_CONNECTIONS", "10"))
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "30"))

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    if not OPENAI_API_KEY:
        raise ValueError("OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    openai_client = OpenAI(api_key=OPENAI_API_KEY)
    logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    logger.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    openai_client = None

# ================================================
# ëŠë¦° ì¿¼ë¦¬ ë¡œê¹… (Query Performance Monitoring)
# ================================================
from functools import wraps
import uuid
from enum import Enum

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ëŠë¦° ì¿¼ë¦¬ ì„ê³„ê°’ ì„¤ì • (ê¸°ë³¸ 1ì´ˆ)
SLOW_QUERY_THRESHOLD = float(os.getenv("SLOW_QUERY_THRESHOLD", "1.0"))

# ================================================
# ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œìŠ¤í…œ
# ================================================
class TaskStatus(str, Enum):
    """ì‘ì—… ìƒíƒœ"""
    PENDING = "pending"      # ëŒ€ê¸° ì¤‘
    RUNNING = "running"      # ì‹¤í–‰ ì¤‘
    COMPLETED = "completed"  # ì™„ë£Œ
    FAILED = "failed"        # ì‹¤íŒ¨
    CANCELLED = "cancelled"  # ì·¨ì†Œë¨

# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
background_tasks_store: Dict[str, Dict[str, Any]] = {}
background_tasks_lock = Lock()

def create_background_task(task_type: str, description: str, params: Dict[str, Any] = None) -> str:
    """
    ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒì„±

    Args:
        task_type: ì‘ì—… ìœ í˜• (ì˜ˆ: "ai_summary", "bulk_update")
        description: ì‘ì—… ì„¤ëª…
        params: ì‘ì—… íŒŒë¼ë¯¸í„°

    Returns:
        task_id: ìƒì„±ëœ ì‘ì—… ID
    """
    task_id = str(uuid.uuid4())

    with background_tasks_lock:
        background_tasks_store[task_id] = {
            "task_id": task_id,
            "task_type": task_type,
            "description": description,
            "status": TaskStatus.PENDING,
            "params": params or {},
            "result": None,
            "error": None,
            "progress": 0,
            "total": 0,
            "created_at": datetime.now().isoformat(),
            "started_at": None,
            "completed_at": None
        }

    logger.info(f"ğŸ“‹ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒì„±: {task_id} ({task_type})")
    return task_id

def update_task_status(task_id: str, status: TaskStatus, progress: int = None, total: int = None,
                       result: Any = None, error: str = None):
    """ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸"""
    with background_tasks_lock:
        if task_id not in background_tasks_store:
            logger.warning(f"âš ï¸ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‘ì—… ID: {task_id}")
            return

        task = background_tasks_store[task_id]
        task["status"] = status

        if progress is not None:
            task["progress"] = progress
        if total is not None:
            task["total"] = total
        if result is not None:
            task["result"] = result
        if error is not None:
            task["error"] = error

        # ìƒíƒœë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
        if status == TaskStatus.RUNNING and not task["started_at"]:
            task["started_at"] = datetime.now().isoformat()
        elif status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
            task["completed_at"] = datetime.now().isoformat()

def get_task_status(task_id: str) -> Optional[Dict[str, Any]]:
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    with background_tasks_lock:
        return background_tasks_store.get(task_id)

def get_all_tasks() -> List[Dict[str, Any]]:
    """ëª¨ë“  ì‘ì—… ì¡°íšŒ (ìµœê·¼ ìˆœ)"""
    with background_tasks_lock:
        tasks = list(background_tasks_store.values())
        # ìƒì„± ì‹œê°„ ì—­ìˆœ ì •ë ¬
        tasks.sort(key=lambda x: x["created_at"], reverse=True)
        return tasks

async def execute_background_task(task_id: str, task_func, *args, **kwargs):
    """
    ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹¤í–‰

    Args:
        task_id: ì‘ì—… ID
        task_func: ì‹¤í–‰í•  ë¹„ë™ê¸° í•¨ìˆ˜
        *args, **kwargs: í•¨ìˆ˜ ì¸ì
    """
    try:
        # ì‘ì—… ì‹œì‘
        update_task_status(task_id, TaskStatus.RUNNING)
        logger.info(f"ğŸš€ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘: {task_id}")

        # ì‘ì—… ì‹¤í–‰
        result = await task_func(task_id, *args, **kwargs)

        # ì‘ì—… ì™„ë£Œ
        update_task_status(task_id, TaskStatus.COMPLETED, result=result)
        logger.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì™„ë£Œ: {task_id}")

    except Exception as e:
        # ì‘ì—… ì‹¤íŒ¨
        error_msg = f"{type(e).__name__}: {str(e)}"
        update_task_status(task_id, TaskStatus.FAILED, error=error_msg)
        logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹¤íŒ¨: {task_id} - {error_msg}")
        logger.error(traceback.format_exc())

def log_slow_query(threshold: float = SLOW_QUERY_THRESHOLD, table_name: str = "unknown"):
    """
    ëŠë¦° ì¿¼ë¦¬ ìë™ ë¡œê¹… ë° DB ì¿¼ë¦¬ ì¶”ì  ë°ì½”ë ˆì´í„°

    Args:
        threshold: ëŠë¦° ì¿¼ë¦¬ë¡œ ê°„ì£¼í•  ì„ê³„ê°’ (ì´ˆ)
        table_name: ì¿¼ë¦¬ ëŒ€ìƒ í…Œì´ë¸”ëª… (í†µê³„ ì¶”ì ìš©)
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            success = False
            error_msg = None

            try:
                result = await func(*args, **kwargs)
                success = True
                execution_time = time.time() - start_time

                # DB ì¿¼ë¦¬ í†µê³„ ì¶”ì 
                track_db_query(table_name, execution_time, success)

                if execution_time > threshold:
                    logger.warning(
                        f"ğŸŒ ëŠë¦° ì¿¼ë¦¬ ê°ì§€: {func.__name__} - {execution_time:.3f}ì´ˆ",
                        extra={
                            "context": {
                                "function": func.__name__,
                                "table": table_name,
                                "execution_time": round(execution_time, 3),
                                "threshold": threshold,
                                "args_count": len(args),
                                "kwargs": list(kwargs.keys())
                            }
                        }
                    )

                return result

            except Exception as e:
                execution_time = time.time() - start_time
                error_msg = str(e)

                # DB ì¿¼ë¦¬ ì‹¤íŒ¨ ì¶”ì 
                track_db_query(table_name, execution_time, success, error_msg)

                logger.error(
                    f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {func.__name__} - {error_msg}",
                    extra={
                        "context": {
                            "function": func.__name__,
                            "table": table_name,
                            "execution_time": round(execution_time, 3),
                            "error": error_msg
                        }
                    }
                )
                raise

        return wrapper
    return decorator

logger.info(f"âœ… ëŠë¦° ì¿¼ë¦¬ ëª¨ë‹ˆí„°ë§ í™œì„±í™” (ì„ê³„ê°’: {SLOW_QUERY_THRESHOLD}ì´ˆ)")

# ================================================
# ì¸ë©”ëª¨ë¦¬ ìºì‹œ ì‹œìŠ¤í…œ (ê°„ë‹¨í•œ Dict ê¸°ë°˜)
# ================================================
from typing import Tuple
import time

# ìºì‹œ ì €ì¥ì†Œ (key: (data, timestamp))
cache_store: Dict[str, Tuple[Any, float]] = {}

# ìºì‹œ íˆíŠ¸ìœ¨ ì¶”ì  (hits, misses)
cache_stats_tracker = {"hits": 0, "misses": 0, "expirations": 0}

# í™˜ê²½ë³€ìˆ˜ì—ì„œ TTL ì„¤ì • (ê¸°ë³¸ 60ì´ˆ)
CACHE_TTL = int(os.getenv("CACHE_TTL", "60"))
logger.info(f"âœ… ì¸ë©”ëª¨ë¦¬ ìºì‹œ í™œì„±í™” (TTL: {CACHE_TTL}ì´ˆ)")

def get_cache(key: str) -> Optional[Any]:
    """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ (íˆíŠ¸ìœ¨ ì¶”ì )"""
    if key not in cache_store:
        cache_stats_tracker["misses"] += 1
        return None

    data, timestamp = cache_store[key]

    # TTL ì²´í¬
    if time.time() - timestamp > CACHE_TTL:
        # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
        del cache_store[key]
        cache_stats_tracker["expirations"] += 1
        logger.debug(f"ğŸ—‘ï¸ ìºì‹œ ë§Œë£Œ ì‚­ì œ: {key}")
        return None

    cache_stats_tracker["hits"] += 1
    logger.debug(f"âœ… ìºì‹œ íˆíŠ¸: {key}")
    return data

def set_cache(key: str, data: Any) -> None:
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    cache_store[key] = (data, time.time())
    logger.debug(f"ğŸ’¾ ìºì‹œ ì €ì¥: {key}")

def clear_cache(pattern: Optional[str] = None) -> int:
    """
    ìºì‹œ ì‚­ì œ (íŒ¨í„´ ì§€ì›)

    Args:
        pattern: ì‚­ì œí•  ìºì‹œ í‚¤ íŒ¨í„´ (ì˜ˆ: "search_*", "api_*")
                Noneì´ë©´ ëª¨ë“  ìºì‹œ ì‚­ì œ

    Returns:
        ì‚­ì œëœ ìºì‹œ í•­ëª© ìˆ˜
    """
    if pattern is None:
        # ëª¨ë“  ìºì‹œ ì‚­ì œ
        count = len(cache_store)
        cache_store.clear()
        logger.info(f"ğŸ—‘ï¸ ëª¨ë“  ìºì‹œ ì‚­ì œë¨ ({count}ê°œ)")
        return count

    # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì‚­ì œ
    import fnmatch
    keys_to_delete = [key for key in cache_store.keys() if fnmatch.fnmatch(key, pattern)]

    for key in keys_to_delete:
        del cache_store[key]

    logger.info(f"ğŸ—‘ï¸ íŒ¨í„´ '{pattern}' ìºì‹œ ì‚­ì œë¨ ({len(keys_to_delete)}ê°œ)")
    return len(keys_to_delete)

def get_cache_stats() -> Dict[str, Any]:
    """ìºì‹œ í†µê³„ ì •ë³´ ì¡°íšŒ (íˆíŠ¸ìœ¨ í¬í•¨)"""
    current_time = time.time()

    # íˆíŠ¸ìœ¨ ê³„ì‚°
    total_requests = cache_stats_tracker["hits"] + cache_stats_tracker["misses"]
    hit_rate = (cache_stats_tracker["hits"] / total_requests * 100) if total_requests > 0 else 0

    stats = {
        "total_entries": len(cache_store),
        "ttl_seconds": CACHE_TTL,
        "hit_rate_percentage": round(hit_rate, 2),
        "performance": {
            "hits": cache_stats_tracker["hits"],
            "misses": cache_stats_tracker["misses"],
            "expirations": cache_stats_tracker["expirations"],
            "total_requests": total_requests
        },
        "entries": []
    }

    for key, (data, timestamp) in cache_store.items():
        age = current_time - timestamp
        remaining_ttl = max(0, CACHE_TTL - age)

        stats["entries"].append({
            "key": key,
            "age_seconds": round(age, 2),
            "remaining_ttl_seconds": round(remaining_ttl, 2),
            "size_estimate": len(str(data))  # ê°„ë‹¨í•œ í¬ê¸° ì¶”ì •
        })

    return stats

def cleanup_expired_cache() -> int:
    """ë§Œë£Œëœ ìºì‹œ í•­ëª© ìë™ ì •ë¦¬"""
    current_time = time.time()
    keys_to_delete = []

    for key, (data, timestamp) in cache_store.items():
        if current_time - timestamp > CACHE_TTL:
            keys_to_delete.append(key)

    for key in keys_to_delete:
        del cache_store[key]

    if keys_to_delete:
        logger.info(f"ğŸ§¹ ë§Œë£Œëœ ìºì‹œ ìë™ ì •ë¦¬ ({len(keys_to_delete)}ê°œ)")

    return len(keys_to_delete)

# í•„ìš”í•œ í•¨ìˆ˜ë“¤ ì¶”ê°€
@app.get("/")
async def serve_frontend():
    """í”„ë¡ íŠ¸ì—”ë“œ HTML ì„œë¹™"""
    # ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„
    possible_paths = [
        Path(__file__).parent / 'index.html',
        Path('index.html'),
        Path('frontend/index.html'),
        Path('E:/gov-support-automation/frontend/index.html')
    ]
    
    for html_path in possible_paths:
        if html_path.exists():
            logger.info(f"Found index.html at: {html_path}")
            return FileResponse(str(html_path))
    
    logger.warning("index.html not found, returning API status")
    return {
        "message": "API is running. Frontend file not found.",
        "api_docs": "http://localhost:8000/docs",
        "health": "http://localhost:8000/health",
        "stats": "http://localhost:8000/api/stats"
    }

@app.get("/metrics")
async def metrics():
    """
    Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸

    Prometheus í˜•ì‹ìœ¼ë¡œ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    from fastapi.responses import PlainTextResponse

    # ìºì‹œ í†µê³„
    cache_hits = cache_stats_tracker["hits"]
    cache_misses = cache_stats_tracker["misses"]
    cache_expirations = cache_stats_tracker["expirations"]
    cache_entries = len(cache_store)

    # Rate Limiter í†µê³„
    rate_limiter_stats = get_rate_limiter_stats()
    tracked_ips = rate_limiter_stats.get("tracked_ips", 0)
    total_active_requests = rate_limiter_stats.get("total_active_requests", 0)

    # DB ìƒíƒœ
    db_connected = 1 if supabase else 0
    openai_connected = 1 if openai_client else 0

    # API ì‘ë‹µ ì‹œê°„ ë©”íŠ¸ë¦­
    endpoint_stats = get_endpoint_metrics_summary()

    # DB ì»¤ë„¥ì…˜ í’€ í†µê³„
    db_stats = get_db_connection_stats()
    db_query_stats_data = db_stats["query_statistics"]
    db_queries_by_table = db_stats["queries_by_table"]

    # Prometheus í¬ë§· ë©”íŠ¸ë¦­
    metrics_text = f"""# HELP cache_hits_total Total number of cache hits
# TYPE cache_hits_total counter
cache_hits_total {cache_hits}

# HELP cache_misses_total Total number of cache misses
# TYPE cache_misses_total counter
cache_misses_total {cache_misses}

# HELP cache_expirations_total Total number of cache expirations
# TYPE cache_expirations_total counter
cache_expirations_total {cache_expirations}

# HELP cache_entries Current number of cache entries
# TYPE cache_entries gauge
cache_entries {cache_entries}

# HELP rate_limiter_tracked_ips Number of tracked IP addresses
# TYPE rate_limiter_tracked_ips gauge
rate_limiter_tracked_ips {tracked_ips}

# HELP rate_limiter_active_requests Total active requests across all IPs
# TYPE rate_limiter_active_requests gauge
rate_limiter_active_requests {total_active_requests}

# HELP database_connected Database connection status (1=connected, 0=disconnected)
# TYPE database_connected gauge
database_connected {db_connected}

# HELP openai_connected OpenAI client status (1=connected, 0=disconnected)
# TYPE openai_connected gauge
openai_connected {openai_connected}

# HELP db_queries_total Total number of database queries
# TYPE db_queries_total counter
db_queries_total {db_query_stats_data["total_queries"]}

# HELP db_queries_successful Successful database queries
# TYPE db_queries_successful counter
db_queries_successful {db_query_stats_data["successful_queries"]}

# HELP db_queries_failed Failed database queries
# TYPE db_queries_failed counter
db_queries_failed {db_query_stats_data["failed_queries"]}

# HELP db_query_success_rate_percentage Database query success rate
# TYPE db_query_success_rate_percentage gauge
db_query_success_rate_percentage {db_query_stats_data["success_rate_percentage"]}

# HELP db_query_avg_time_ms Average database query time in milliseconds
# TYPE db_query_avg_time_ms gauge
db_query_avg_time_ms {db_query_stats_data["average_query_time_ms"]}

# HELP api_requests_total Total number of API requests per endpoint
# TYPE api_requests_total counter
"""

    # ì—”ë“œí¬ì¸íŠ¸ë³„ ìš”ì²­ ìˆ˜
    for endpoint, stats in endpoint_stats.items():
        # Prometheus ë¼ë²¨ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        method, path = endpoint.split(" ", 1)
        metrics_text += f'api_requests_total{{method="{method}",path="{path}"}} {stats["count"]}\n'

    metrics_text += """
# HELP api_response_time_ms API response time in milliseconds
# TYPE api_response_time_ms summary
"""

    # ì—”ë“œí¬ì¸íŠ¸ë³„ ì‘ë‹µ ì‹œê°„ (min, avg, max, p50, p95, p99)
    for endpoint, stats in endpoint_stats.items():
        method, path = endpoint.split(" ", 1)
        metrics_text += f'api_response_time_ms{{method="{method}",path="{path}",quantile="min"}} {stats["min_ms"]}\n'
        metrics_text += f'api_response_time_ms{{method="{method}",path="{path}",quantile="avg"}} {stats["avg_ms"]}\n'
        metrics_text += f'api_response_time_ms{{method="{method}",path="{path}",quantile="max"}} {stats["max_ms"]}\n'
        metrics_text += f'api_response_time_ms{{method="{method}",path="{path}",quantile="0.5"}} {stats["p50_ms"]}\n'
        metrics_text += f'api_response_time_ms{{method="{method}",path="{path}",quantile="0.95"}} {stats["p95_ms"]}\n'
        metrics_text += f'api_response_time_ms{{method="{method}",path="{path}",quantile="0.99"}} {stats["p99_ms"]}\n'

    # í…Œì´ë¸”ë³„ ì¿¼ë¦¬ ì¹´ìš´íŠ¸
    metrics_text += "\n# HELP db_queries_by_table Database queries by table\n"
    metrics_text += "# TYPE db_queries_by_table counter\n"
    for table_name, count in db_queries_by_table.items():
        metrics_text += f'db_queries_by_table{{table="{table_name}"}} {count}\n'

    return PlainTextResponse(content=metrics_text, media_type="text/plain; version=0.0.4")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ - DB ì—°ê²°, ìºì‹œ ìƒíƒœ, API ë²„ì „ í¬í•¨"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": API_VERSION,
        "version_info": {
            "major": API_VERSION_MAJOR,
            "minor": API_VERSION_MINOR,
            "patch": API_VERSION_PATCH,
            "release_date": "2025-11-02"
        },
        "services": {
            "database": {
                "connected": supabase is not None,
                "status": "healthy" if supabase else "unavailable",
                "connection_pool": get_db_connection_stats() if supabase else {
                    "connection_pool": {
                        "max_connections": 0,
                        "timeout_seconds": 0,
                        "status": "disconnected"
                    },
                    "query_statistics": {
                        "total_queries": 0,
                        "successful_queries": 0,
                        "failed_queries": 0,
                        "success_rate_percentage": 0,
                        "average_query_time_ms": 0
                    },
                    "queries_by_table": {},
                    "recent_errors": []
                }
            },
            "cache": {
                "enabled": True,
                "ttl": CACHE_TTL,
                "entries": len(cache_store),
                "status": "healthy"
            },
            "rate_limiting": get_rate_limiter_stats(),
            "openai": {
                "connected": openai_client is not None,
                "status": "healthy" if openai_client else "unavailable"
            }
        }
    }

    # DB ì—°ê²° ì‹¤ì œ í…ŒìŠ¤íŠ¸
    if supabase:
        try:
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ DB ì—°ê²° í™•ì¸
            test_query = supabase.table('kstartup_complete').select("announcement_id", count='exact').limit(1).execute()
            health_status["services"]["database"]["test_query"] = "success"
            health_status["services"]["database"]["response_time_ms"] = "<50"
        except Exception as e:
            health_status["status"] = "degraded"
            health_status["services"]["database"]["status"] = "error"
            health_status["services"]["database"]["error"] = str(e)

    # ì „ì²´ ìƒíƒœ ê²°ì •
    if health_status["status"] == "degraded":
        return JSONResponse(status_code=503, content=health_status)

    return health_status

@app.get("/api/performance")
async def get_performance_metrics():
    """
    API ì‘ë‹µ ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ

    ì—”ë“œí¬ì¸íŠ¸ë³„ ì‘ë‹µ ì‹œê°„ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    endpoint_stats = get_endpoint_metrics_summary()

    # ìƒìœ„ 5ê°œ ëŠë¦° ì—”ë“œí¬ì¸íŠ¸
    slowest_endpoints = sorted(
        endpoint_stats.items(),
        key=lambda x: x[1]["avg_ms"],
        reverse=True
    )[:5]

    # ìƒìœ„ 5ê°œ ë§ì´ í˜¸ì¶œëœ ì—”ë“œí¬ì¸íŠ¸
    most_requested = sorted(
        endpoint_stats.items(),
        key=lambda x: x[1]["count"],
        reverse=True
    )[:5]

    return {
        "success": True,
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_endpoints": len(endpoint_stats),
            "total_requests": sum(stats["count"] for stats in endpoint_stats.values())
        },
        "endpoints": endpoint_stats,
        "top_slowest": [
            {
                "endpoint": endpoint,
                "avg_ms": stats["avg_ms"],
                "p95_ms": stats["p95_ms"],
                "count": stats["count"]
            }
            for endpoint, stats in slowest_endpoints
        ],
        "most_requested": [
            {
                "endpoint": endpoint,
                "count": stats["count"],
                "avg_ms": stats["avg_ms"]
            }
            for endpoint, stats in most_requested
        ]
    }

@app.get("/api/stats")
async def get_statistics():
    """ì‹¤ì‹œê°„ í†µê³„ ì •ë³´ ì¡°íšŒ (ìºì‹œ 60ì´ˆ) - ì¸ë©”ëª¨ë¦¬ ìºì‹œ ì ìš©"""
    # ìºì‹œ í™•ì¸
    cached_data = get_cache("api_stats")
    if cached_data:
        logger.info("[Stats] ğŸ’¨ ìºì‹œ íˆíŠ¸ (ì¦‰ì‹œ ì‘ë‹µ)")
        return cached_data

    if not supabase:
        return {
            "error": "Database not connected",
            "total": 0,
            "kstartup": 0,
            "bizinfo": 0
        }

    try:
        logger.info("[Stats] ğŸ“Š DB ì¡°íšŒ ì‹œì‘...")
        today = datetime.now().strftime("%Y-%m-%d")
        week_later = (datetime.now() + timedelta(days=7)).strftime("%Y-%m-%d")

        # K-Startup í†µê³„ (countë§Œ ì¡°íšŒ)
        ks_total = supabase.table('kstartup_complete').select("announcement_id", count='exact').execute()
        ks_ongoing = supabase.table('kstartup_complete')\
            .select("announcement_id", count='exact')\
            .gte('pbanc_rcpt_end_dt', today)\
            .execute()
        ks_deadline = supabase.table('kstartup_complete')\
            .select("announcement_id", count='exact')\
            .gte('pbanc_rcpt_end_dt', today)\
            .lte('pbanc_rcpt_end_dt', week_later)\
            .execute()

        # BizInfo í†µê³„ (countë§Œ ì¡°íšŒ)
        bi_total = supabase.table('bizinfo_complete').select("pblanc_id", count='exact').execute()
        bi_ongoing = supabase.table('bizinfo_complete')\
            .select("pblanc_id", count='exact')\
            .gte('reqst_end_ymd', today)\
            .execute()
        bi_deadline = supabase.table('bizinfo_complete')\
            .select("pblanc_id", count='exact')\
            .gte('reqst_end_ymd', today)\
            .lte('reqst_end_ymd', week_later)\
            .execute()

        # ì˜¤ëŠ˜ ë“±ë¡ëœ ê³µê³ 
        today_start = f"{today}T00:00:00"
        ks_today = supabase.table('kstartup_complete')\
            .select("announcement_id", count='exact')\
            .gte('created_at', today_start)\
            .execute()
        bi_today = supabase.table('bizinfo_complete')\
            .select("pblanc_id", count='exact')\
            .gte('created_at', today_start)\
            .execute()

        result = {
            "total": (ks_total.count or 0) + (bi_total.count or 0),
            "kstartup": ks_total.count or 0,
            "bizinfo": bi_total.count or 0,
            "today": (ks_today.count or 0) + (bi_today.count or 0),
            "ongoing": (ks_ongoing.count or 0) + (bi_ongoing.count or 0),
            "deadline": (ks_deadline.count or 0) + (bi_deadline.count or 0),
            "last_update": datetime.now().isoformat(),
            "cache_enabled": True,
            "cache_ttl": CACHE_TTL,
            "details": {
                "kstartup": {
                    "total": ks_total.count or 0,
                    "ongoing": ks_ongoing.count or 0,
                    "deadline": ks_deadline.count or 0,
                    "today": ks_today.count or 0
                },
                "bizinfo": {
                    "total": bi_total.count or 0,
                    "ongoing": bi_ongoing.count or 0,
                    "deadline": bi_deadline.count or 0,
                    "today": bi_today.count or 0
                }
            }
        }

        # ìºì‹œ ì €ì¥
        set_cache("api_stats", result)
        logger.info("[Stats] âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ")

        return result

    except Exception as e:
        logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return {
            "error": str(e),
            "total": 0,
            "kstartup": 0,
            "bizinfo": 0,
            "today": 0,
            "ongoing": 0,
            "deadline": 0,
            "last_update": datetime.now().isoformat()
        }

@app.get("/api/search")
@limiter.limit("60/minute") if RATE_LIMIT_ENABLED else lambda x: x
async def search_announcements(
    request: Request,
    q: Optional[str] = Query(None, description="ê²€ìƒ‰ì–´"),
    source: Optional[str] = Query("all", description="ì¶œì²˜: all, kstartup, bizinfo"),
    status: Optional[str] = Query("all", description="ìƒíƒœ: all, ongoing, deadline, closed"),
    sort: Optional[str] = Query("newest", description="ì •ë ¬: newest, deadline, title"),
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    limit: int = Query(10, ge=1, le=100, description="í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜")
):
    """ê³µê³  ê²€ìƒ‰ (ì‹¤ì œ DB ë°ì´í„°) - DB ë ˆë²¨ í˜ì´ì§€ë„¤ì´ì…˜ ì ìš© + Rate Limiting (60/min) + ìºì‹±"""
    if not supabase:
        raise HTTPException(status_code=500, detail="Database not connected")

    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"search_{q or 'all'}_{source}_{status}_{sort}_{page}_{limit}"
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"[Search] ğŸ’¨ ìºì‹œ íˆíŠ¸: {cache_key}")
        return cached_data

    try:
        today = datetime.now().strftime("%Y-%m-%d")
        week_later = (datetime.now() + timedelta(days=7)).strftime("%Y-%m-%d")

        # ë‹¨ì¼ ì†ŒìŠ¤ ê²€ìƒ‰ (sourceê°€ allì´ ì•„ë‹Œ ê²½ìš°)
        if source == "kstartup":
            return await _search_single_source_paginated(
                "kstartup_complete", q, status, sort, page, limit, today, week_later
            )
        elif source == "bizinfo":
            return await _search_single_source_paginated(
                "bizinfo_complete", q, status, sort, page, limit, today, week_later
            )

        # í†µí•© ê²€ìƒ‰ (sourceê°€ "all"ì¸ ê²½ìš°)
        # ë‘ í…Œì´ë¸”ì˜ ì´ ê°œìˆ˜ë¥¼ ë¨¼ì € ì¡°íšŒ
        ks_count = await _get_count("kstartup_complete", q, status, today, week_later)
        bi_count = await _get_count("bizinfo_complete", q, status, today, week_later)
        total_count = ks_count + bi_count

        # í˜ì´ì§€ë„¤ì´ì…˜ ê³„ì‚°
        offset = (page - 1) * limit

        # ë‘ í…Œì´ë¸”ì—ì„œ í•„ìš”í•œ ë§Œí¼ë§Œ ì¡°íšŒ
        all_results = []

        # K-Startup ê²€ìƒ‰
        ks_limit = min(limit, max(0, limit - len(all_results)))
        if ks_limit > 0:
            ks_results = await _fetch_announcements(
                "kstartup_complete", q, status, sort, offset, ks_limit, today, week_later
            )
            all_results.extend(ks_results)

        # BizInfo ê²€ìƒ‰ (K-Startup ê²°ê³¼ê°€ limitë³´ë‹¤ ì ì„ ë•Œë§Œ)
        bi_offset = max(0, offset - ks_count)
        bi_limit = min(limit - len(all_results), bi_count)
        if bi_limit > 0:
            bi_results = await _fetch_announcements(
                "bizinfo_complete", q, status, sort, bi_offset, bi_limit, today, week_later
            )
            all_results.extend(bi_results)

        # í†µí•© ì •ë ¬ì´ í•„ìš”í•œ ê²½ìš° (newest, deadline, title ì •ë ¬ ì‹œ)
        if sort == "deadline":
            all_results.sort(key=lambda x: (x.get('end_date') or '9999-99-99'))
        elif sort == "title":
            all_results.sort(key=lambda x: x.get('title', ''))
        else:  # newest
            all_results.sort(key=lambda x: x.get('created_at', ''), reverse=True)

        result = {
            "success": True,
            "results": all_results[:limit],  # ì •ë ¬ í›„ ë‹¤ì‹œ limit ì ìš©
            "total": total_count,
            "page": page,
            "limit": limit,
            "total_pages": (total_count + limit - 1) // limit,
            "has_more": (offset + limit) < total_count
        }

        # ìºì‹œ ì €ì¥
        set_cache(cache_key, result)
        logger.info(f"[Search] âœ… ìºì‹œ ì €ì¥: {cache_key}")

        return result

    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@log_slow_query(table_name="count_query")
async def _get_count(table_name: str, q: Optional[str], status: str, today: str, week_later: str) -> int:
    """í…Œì´ë¸”ì˜ ì´ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ"""
    query = supabase.table(table_name).select("*", count='exact')

    # ê²€ìƒ‰ì–´ í•„í„°
    if q:
        if table_name == "kstartup_complete":
            search_filter = f"biz_pbanc_nm.ilike.%{q}%,simple_summary.ilike.%{q}%"
        else:  # bizinfo_complete
            search_filter = f"pblanc_nm.ilike.%{q}%,organ_nm.ilike.%{q}%,sprt_trgt.ilike.%{q}%"
        query = query.or_(search_filter)

    # ìƒíƒœ í•„í„°
    date_col = 'pbanc_rcpt_end_dt' if table_name == "kstartup_complete" else 'reqst_end_ymd'
    if status == "ongoing":
        query = query.gte(date_col, today)
    elif status == "closed":
        query = query.lt(date_col, today)
    elif status == "deadline":
        query = query.gte(date_col, today).lte(date_col, week_later)

    result = query.execute()
    return result.count or 0

@log_slow_query(table_name="fetch_announcements")
async def _fetch_announcements(
    table_name: str, q: Optional[str], status: str, sort: str,
    offset: int, limit: int, today: str, week_later: str
) -> List[Dict[str, Any]]:
    """í…Œì´ë¸”ì—ì„œ ê³µê³  ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©)"""
    # ì»¬ëŸ¼ ì„ íƒ
    if table_name == "kstartup_complete":
        query = supabase.table(table_name).select(
            "announcement_id,biz_pbanc_nm,pbanc_ntrp_nm,pbanc_rcpt_bgng_dt,pbanc_rcpt_end_dt,simple_summary,created_at"
        )
    else:  # bizinfo_complete
        query = supabase.table(table_name).select(
            "pblanc_id,pblanc_nm,organ_nm,reqst_begin_ymd,reqst_end_ymd,simple_summary,created_at"
        )

    # ê²€ìƒ‰ì–´ í•„í„°
    if q:
        if table_name == "kstartup_complete":
            search_filter = f"biz_pbanc_nm.ilike.%{q}%,simple_summary.ilike.%{q}%"
        else:  # bizinfo_complete
            search_filter = f"pblanc_nm.ilike.%{q}%,organ_nm.ilike.%{q}%,sprt_trgt.ilike.%{q}%"
        query = query.or_(search_filter)

    # ìƒíƒœ í•„í„°
    date_col = 'pbanc_rcpt_end_dt' if table_name == "kstartup_complete" else 'reqst_end_ymd'
    if status == "ongoing":
        query = query.gte(date_col, today)
    elif status == "closed":
        query = query.lt(date_col, today)
    elif status == "deadline":
        query = query.gte(date_col, today).lte(date_col, week_later)

    # ì •ë ¬
    if table_name == "kstartup_complete":
        if sort == "deadline":
            query = query.order('pbanc_rcpt_end_dt', desc=False)
        elif sort == "title":
            query = query.order('biz_pbanc_nm', desc=False)
        else:  # newest
            query = query.order('created_at', desc=True)
    else:  # bizinfo_complete
        if sort == "deadline":
            query = query.order('reqst_end_ymd', desc=False)
        elif sort == "title":
            query = query.order('pblanc_nm', desc=False)
        else:  # newest
            query = query.order('created_at', desc=True)

    # í˜ì´ì§€ë„¤ì´ì…˜ (DB ë ˆë²¨)
    query = query.range(offset, offset + limit - 1)

    # ì‹¤í–‰
    result = query.execute()

    # ê²°ê³¼ í¬ë§·íŒ…
    formatted_results = []
    for item in result.data:
        if table_name == "kstartup_complete":
            title = item.get("biz_pbanc_nm")
            formatted_results.append({
                "id": item.get("announcement_id"),
                "title": title,
                "organization": item.get("pbanc_ntrp_nm") or "K-Startup",
                "category": extract_category_from_title(title),
                "start_date": item.get("pbanc_rcpt_bgng_dt"),
                "end_date": item.get("pbanc_rcpt_end_dt"),
                "source": "kstartup",
                "source_name": "K-Startup",
                "simple_summary": item.get("simple_summary"),
                "status": calculate_status(item.get("pbanc_rcpt_end_dt")),
                "created_at": item.get("created_at"),
                "days_left": calculate_days_left(item.get("pbanc_rcpt_end_dt"))
            })
        else:  # bizinfo_complete
            title = item.get("pblanc_nm")
            formatted_results.append({
                "id": item.get("pblanc_id"),
                "title": title,
                "organization": item.get("organ_nm"),
                "category": extract_category_from_title(title),
                "start_date": item.get("reqst_begin_ymd"),
                "end_date": item.get("reqst_end_ymd"),
                "source": "bizinfo",
                "source_name": "BizInfo",
                "simple_summary": item.get("simple_summary"),
                "status": calculate_status(item.get("reqst_end_ymd")),
                "created_at": item.get("created_at"),
                "days_left": calculate_days_left(item.get("reqst_end_ymd"))
            })

    return formatted_results

@log_slow_query(table_name="single_source_search")
async def _search_single_source_paginated(
    table_name: str, q: Optional[str], status: str, sort: str,
    page: int, limit: int, today: str, week_later: str
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì†ŒìŠ¤ ê²€ìƒ‰ (DB ë ˆë²¨ í˜ì´ì§€ë„¤ì´ì…˜)"""
    # ì´ ê°œìˆ˜ ì¡°íšŒ
    total_count = await _get_count(table_name, q, status, today, week_later)

    # í˜ì´ì§€ë„¤ì´ì…˜ ê³„ì‚°
    offset = (page - 1) * limit

    # ê²°ê³¼ ì¡°íšŒ
    results = await _fetch_announcements(
        table_name, q, status, sort, offset, limit, today, week_later
    )

    result = {
        "success": True,
        "results": results,
        "total": total_count,
        "page": page,
        "limit": limit,
        "total_pages": (total_count + limit - 1) // limit,
        "has_more": (offset + limit) < total_count
    }

    return result

@app.get("/api/search/semantic")
async def search_semantic(
    q: str = Query(..., description="ê²€ìƒ‰ì–´ (í•„ìˆ˜)"),
    source: Optional[str] = Query("all", description="ì¶œì²˜: all, kstartup, bizinfo"),
    threshold: float = Query(0.5, ge=0.0, le=1.0, description="ìœ ì‚¬ë„ ì„ê³„ê°’"),
    limit: int = Query(10, ge=1, le=50, description="ê²°ê³¼ ê°œìˆ˜")
):
    """ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ (Semantic Search)"""
    if not supabase:
        raise HTTPException(status_code=500, detail="Database not connected")

    if not openai_client:
        raise HTTPException(status_code=500, detail="OpenAI client not initialized")

    try:
        # 1. OpenAIë¡œ ê²€ìƒ‰ì–´ ì„ë² ë”© ìƒì„±
        logger.info(f"[Semantic Search] ê²€ìƒ‰ì–´: {q}")
        embedding_response = openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=q
        )
        query_embedding = embedding_response.data[0].embedding
        logger.info(f"[Semantic Search] ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì°¨ì›: {len(query_embedding)})")

        # 2. Supabase RPC í•¨ìˆ˜ í˜¸ì¶œ
        results = []

        if source in ["all", "kstartup"]:
            # K-Startup ê²€ìƒ‰
            ks_result = supabase.rpc(
                'match_kstartup_announcements',
                {
                    'query_embedding': query_embedding,
                    'match_threshold': threshold,
                    'match_count': limit
                }
            ).execute()

            # ê²°ê³¼ í¬ë§·íŒ…
            for item in ks_result.data:
                title = item.get("biz_pbanc_nm")
                results.append({
                    "id": item.get("announcement_id"),
                    "title": title,
                    "organization": item.get("pbanc_ntrp_nm") or "K-Startup",
                    "category": extract_category_from_title(title),
                    "start_date": item.get("pbanc_rcpt_bgng_dt"),
                    "end_date": item.get("pbanc_rcpt_end_dt"),
                    "source": "kstartup",
                    "source_name": "K-Startup",
                    "simple_summary": item.get("simple_summary"),
                    "detailed_summary": item.get("detailed_summary"),
                    "status": calculate_status(item.get("pbanc_rcpt_end_dt")),
                    "days_left": calculate_days_left(item.get("pbanc_rcpt_end_dt")),
                    "similarity": round(item.get("similarity", 0), 4)
                })

        if source in ["all", "bizinfo"]:
            # BizInfo ê²€ìƒ‰
            bi_result = supabase.rpc(
                'match_bizinfo_announcements',
                {
                    'query_embedding': query_embedding,
                    'match_threshold': threshold,
                    'match_count': limit
                }
            ).execute()

            # ê²°ê³¼ í¬ë§·íŒ…
            for item in bi_result.data:
                title = item.get("pblanc_nm")
                results.append({
                    "id": item.get("pblanc_id"),
                    "title": title,
                    "organization": item.get("organ_nm"),
                    "category": extract_category_from_title(title),
                    "start_date": item.get("reqst_begin_ymd"),
                    "end_date": item.get("reqst_end_ymd"),
                    "source": "bizinfo",
                    "source_name": "BizInfo",
                    "simple_summary": item.get("simple_summary"),
                    "detailed_summary": item.get("detailed_summary"),
                    "status": calculate_status(item.get("reqst_end_ymd")),
                    "days_left": calculate_days_left(item.get("reqst_end_ymd")),
                    "similarity": round(item.get("similarity", 0), 4)
                })

        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x.get('similarity', 0), reverse=True)

        # ì œí•œëœ ê°œìˆ˜ë§Œ ë°˜í™˜
        results = results[:limit]

        logger.info(f"[Semantic Search] ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")

        return {
            "success": True,
            "data": results,
            "total": len(results),
            "query": q,
            "threshold": threshold,
            "search_type": "semantic"
        }

    except Exception as e:
        logger.error(f"ì˜ë¯¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/announcement/{announcement_id}")
async def get_announcement_detail(announcement_id: str):
    """ê³µê³  ìƒì„¸ ì¡°íšŒ - ì¸ë©”ëª¨ë¦¬ ìºì‹œ ì ìš©"""
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"announcement_{announcement_id}"
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"[Announcement Detail] ğŸ’¨ ìºì‹œ íˆíŠ¸: {announcement_id}")
        return cached_data

    if not supabase:
        raise HTTPException(status_code=500, detail="Database not connected")

    try:
        # ID ì ‘ë‘ì‚¬ë¡œ í…Œì´ë¸” êµ¬ë¶„
        if announcement_id.startswith("KS_"):
            result = supabase.table('kstartup_complete')\
                .select("*")\
                .eq('announcement_id', announcement_id)\
                .execute()

            if result.data:
                formatted_data = format_announcement(result.data[0], "kstartup")
                # ìºì‹œ ì €ì¥
                set_cache(cache_key, formatted_data)
                logger.info(f"[Announcement Detail] âœ… ìºì‹œ ì €ì¥: {announcement_id}")
                return formatted_data

        elif announcement_id.startswith("PBLN_"):
            result = supabase.table('bizinfo_complete')\
                .select("*")\
                .eq('pblanc_id', announcement_id)\
                .execute()

            if result.data:
                formatted_data = format_announcement(result.data[0], "bizinfo")
                # ìºì‹œ ì €ì¥
                set_cache(cache_key, formatted_data)
                logger.info(f"[Announcement Detail] âœ… ìºì‹œ ì €ì¥: {announcement_id}")
                return formatted_data

        raise HTTPException(status_code=404, detail="ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/announcements/bulk")
async def get_announcements_bulk(request: Dict[str, List[str]]):
    """ë¶ë§ˆí¬ ID ë¦¬ìŠ¤íŠ¸ë¡œ ì—¬ëŸ¬ ê³µê³  ì¡°íšŒ"""
    if not supabase:
        raise HTTPException(status_code=500, detail="Database not connected")

    try:
        announcement_ids = request.get("announcement_ids", [])

        if not announcement_ids:
            return {
                "success": True,
                "announcements": [],
                "total": 0
            }

        results = []

        # IDë³„ë¡œ ì¡°íšŒ
        for announcement_id in announcement_ids:
            try:
                if announcement_id.startswith("KS_"):
                    # K-Startup ì¡°íšŒ
                    result = supabase.table('kstartup_complete')\
                        .select("*")\
                        .eq('announcement_id', announcement_id)\
                        .execute()

                    if result.data:
                        results.append(format_announcement(result.data[0], "kstartup"))

                elif announcement_id.startswith("PBLN_"):
                    # BizInfo ì¡°íšŒ
                    result = supabase.table('bizinfo_complete')\
                        .select("*")\
                        .eq('pblanc_id', announcement_id)\
                        .execute()

                    if result.data:
                        results.append(format_announcement(result.data[0], "bizinfo"))

            except Exception as e:
                logger.error(f"ê³µê³  ì¡°íšŒ ì‹¤íŒ¨ ({announcement_id}): {e}")
                continue

        logger.info(f"[Bulk] {len(announcement_ids)}ê°œ ìš”ì²­, {len(results)}ê°œ ì¡°íšŒ ì„±ê³µ")

        return {
            "success": True,
            "announcements": results,
            "total": len(results)
        }

    except Exception as e:
        logger.error(f"ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/filters")
async def get_filter_options():
    """í•„í„° ì˜µì…˜ ì¡°íšŒ (ì§€ì›ë¶„ì•¼, ì§€ì—­, ëŒ€ìƒ, ì—°ë ¹, ì—…ë ¥ ë“±) - ì¸ë©”ëª¨ë¦¬ ìºì‹œ ì ìš©"""
    # ìºì‹œ í™•ì¸
    cached_data = get_cache("api_filters")
    if cached_data:
        logger.info("[Filters] ğŸ’¨ ìºì‹œ íˆíŠ¸")
        return cached_data

    if not supabase:
        raise HTTPException(status_code=500, detail="Database not connected")

    try:
        logger.info("[Filters] ğŸ“Š DB ì¡°íšŒ ì‹œì‘...")
        filters = {
            "categories": [],
            "regions": [],
            "targets": [],
            "ages": [],
            "business_years": []
        }

        # ì¹´í…Œê³ ë¦¬ëŠ” ë„¤ì´ë²„ ì¹´í˜ 11ê°œ ì¹´í…Œê³ ë¦¬ ì‚¬ìš© (K-Startup + ê¸°ì—…ë§ˆë‹¹ í†µí•©)
        filters["categories"] = [
            "ìê¸ˆì§€ì›(ë³´ì¡°ê¸ˆ/ì§€ì›ê¸ˆ)",
            "ì •ì±…ìê¸ˆ(ìœµì/ëŒ€ì¶œ)",
            "ì‹œì„¤/ê³µê°„ ì§€ì›",
            "êµìœ¡/ì»¨ì„¤íŒ…/ë©˜í† ë§",
            "ì¸ë ¥ì§€ì›/ì¼ìë¦¬",
            "ê¸°ìˆ ê°œë°œ (R&D)",
            "í•´ì™¸ì§„ì¶œ/ìˆ˜ì¶œì§€ì›",
            "íŒë¡œ/ë§ˆì¼€íŒ… ì§€ì›",
            "ë„¤íŠ¸ì›Œí‚¹/ì»¤ë®¤ë‹ˆí‹°",
            "ë†ë¦¼ì¶•ìˆ˜ì‚°ì—… íŠ¹ë³„ì§€ì›",
            "ê¸°íƒ€ ì§€ì›ì‚¬ì—…"
        ]

        # ì§€ì—­ê³¼ ëŒ€ìƒë§Œ ê°€ë²¼ìš´ ì¿¼ë¦¬ë¡œ ì¡°íšŒ (100ê°œë§Œ)
        bi_data = supabase.table('bizinfo_complete')\
            .select("organ_nm")\
            .limit(100)\
            .execute()

        # ì§€ì—­ ì¶”ì¶œ (ì¡°ì§ëª…ì—ì„œ)
        regions_set = set()
        for item in bi_data.data:
            org_name = item.get("organ_nm", "")
            if org_name:
                for region in ["ì„œìš¸", "ê²½ê¸°", "ì¸ì²œ", "ë¶€ì‚°", "ëŒ€êµ¬", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…",
                              "ê°•ì›", "ì „ë¶", "ì œì£¼"]:
                    if region in org_name:
                        regions_set.add(region)
                        break

        # ì§€ì—­ ìˆœì„œë¥¼ ì´ë¯¸ì§€ì™€ ë™ì¼í•˜ê²Œ (ì „êµ­ì´ ë§¨ ìœ„)
        filters["regions"] = [
            "ì „êµ­",
            "ì„œìš¸",
            "ë¶€ì‚°",
            "ëŒ€êµ¬",
            "ì¸ì²œ",
            "ê´‘ì£¼",
            "ëŒ€ì „",
            "ìš¸ì‚°",
            "ì„¸ì¢…",
            "ê°•ì›",
            "ê²½ê¸°",
            "ê²½ë‚¨",
            "ê²½ë¶",
            "ì „ë‚¨",
            "ì „ë¶",
            "ì¶©ë‚¨",
            "ì¶©ë¶",
            "ì œì£¼"
        ]
        # ëŒ€ìƒ í•„í„° (ì´ë¯¸ì§€ì™€ ë™ì¼í•œ ìˆœì„œ)
        filters["targets"] = [
            "ì²­ì†Œë…„",
            "ëŒ€í•™ìƒ",
            "ì¼ë°˜ì¸",
            "ëŒ€í•™",
            "ì—°êµ¬ê¸°ê´€",
            "ì¼ë°˜ê¸°ì—…",
            "1ì¸ ì°½ì¡°ê¸°ì—…"
        ]

        # ê³ ì • ì˜µì…˜ë“¤ - ì—°ë ¹ (ì´ë¯¸ì§€ì™€ ë™ì¼)
        filters["ages"] = [
            "ë§Œ 20ì„¸ ë¯¸ë§Œ",
            "ë§Œ 20ì„¸ ì´ìƒ ~ ë§Œ 39ì„¸ ì´í•˜",
            "ë§Œ 39ì„¸ ì´í•˜",
            "ë§Œ 40ì„¸ ì´ìƒ"
        ]
        # ì°½ì—…ì—…ë ¥ (ì´ë¯¸ì§€ì™€ ë™ì¼)
        filters["business_years"] = [
            "ì˜ˆë¹„ì°½ì—…ì",
            "1ë…„ë¯¸ë§Œ",
            "2ë…„ë¯¸ë§Œ",
            "3ë…„ë¯¸ë§Œ",
            "5ë…„ë¯¸ë§Œ",
            "7ë…„ë¯¸ë§Œ",
            "10ë…„ë¯¸ë§Œ"
        ]

        logger.info(f"[Filters] í•„í„° ì˜µì…˜ ì¡°íšŒ ì™„ë£Œ: ì¹´í…Œê³ ë¦¬ {len(filters['categories'])}ê°œ, ì§€ì—­ {len(filters['regions'])}ê°œ, ëŒ€ìƒ {len(filters['targets'])}ê°œ")

        result = {
            "success": True,
            "filters": filters
        }

        # ìºì‹œ ì €ì¥ (ì¸ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©)
        set_cache("api_filters", result)
        logger.info("[Filters] âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ")

        return result

    except Exception as e:
        logger.error(f"í•„í„° ì˜µì…˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/suggestions")
async def get_search_suggestions(
    q: str = Query(..., min_length=1, description="ê²€ìƒ‰ì–´ ì¼ë¶€")
):
    """ê²€ìƒ‰ì–´ ìë™ì™„ì„± ì œì•ˆ - ì¸ë©”ëª¨ë¦¬ ìºì‹œ ì ìš©"""
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"suggestions_{q.lower()}"
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"[Suggestions] ğŸ’¨ ìºì‹œ íˆíŠ¸: {q}")
        return cached_data

    if not supabase:
        raise HTTPException(status_code=500, detail="Database not connected")

    try:
        suggestions = []

        # K-Startup ê³µê³  ì œëª©ì—ì„œ ê²€ìƒ‰
        ks_result = supabase.table('kstartup_complete')\
            .select("biz_pbanc_nm")\
            .ilike('biz_pbanc_nm', f'%{q}%')\
            .limit(5)\
            .execute()

        for item in ks_result.data:
            title = item.get("biz_pbanc_nm", "")
            if title and title not in suggestions:
                suggestions.append(title)

        # BizInfo ê³µê³  ì œëª©ì—ì„œ ê²€ìƒ‰
        bi_result = supabase.table('bizinfo_complete')\
            .select("pblanc_nm")\
            .ilike('pblanc_nm', f'%{q}%')\
            .limit(5)\
            .execute()

        for item in bi_result.data:
            title = item.get("pblanc_nm", "")
            if title and title not in suggestions:
                suggestions.append(title)

        # ì¤‘ë³µ ì œê±° ë° ì œí•œ
        suggestions = list(set(suggestions))[:10]

        # ì¸ê¸° í‚¤ì›Œë“œ (ê³ ì •)
        popular_keywords = ["ì°½ì—…", "R&D", "ê¸°ìˆ ê°œë°œ", "ë§ˆì¼€íŒ…", "ìˆ˜ì¶œ", "ì¸ë ¥", "ì»¨ì„¤íŒ…", "íŠ¹í—ˆ", "ë””ìì¸"]

        # ê²€ìƒ‰ì–´ê°€ ì§§ìœ¼ë©´ ì¸ê¸° í‚¤ì›Œë“œ ì¶”ê°€
        if len(q) <= 2:
            matching_popular = [kw for kw in popular_keywords if q in kw]
            suggestions = matching_popular + suggestions

        logger.info(f"[Suggestions] ê²€ìƒ‰ì–´: {q}, ì œì•ˆ ê°œìˆ˜: {len(suggestions)}")

        result = {
            "success": True,
            "query": q,
            "suggestions": suggestions[:10]
        }

        # ìºì‹œ ì €ì¥ (ì¸ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©)
        set_cache(cache_key, result)
        logger.info(f"[Suggestions] âœ… ìºì‹œ ì €ì¥: {q}")

        return result

    except Exception as e:
        logger.error(f"ê²€ìƒ‰ì–´ ì œì•ˆ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "query": q,
            "suggestions": []
        }

@app.get("/api/recent")
async def get_recent_announcements(
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    limit: int = Query(10, ge=1, le=50, description="í˜ì´ì§€ë‹¹ ê°œìˆ˜"),
    status: Optional[str] = Query(None, description="ìƒíƒœ í•„í„° (ongoing, expired, unknown)")
):
    """ìµœê·¼ ë“±ë¡ ê³µê³  ì¡°íšŒ - í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›"""
    # ìºì‹œ í‚¤ì— status í¬í•¨
    cache_key = f"api_recent_{status or 'all'}"
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"[Recent] ğŸ’¨ ìºì‹œ íˆíŠ¸ (status={status})")
        # ìƒíƒœ í•„í„°ë§
        filtered = cached_data if not status else [x for x in cached_data if x.get('status') == status]
        total_count = len(filtered)
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        return {
            "success": True,
            "results": filtered[start_idx:end_idx],
            "total": total_count,
            "page": page,
            "limit": limit
        }

    if not supabase:
        raise HTTPException(status_code=500, detail="Database not connected")

    try:
        logger.info("[Recent] ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒˆë¡œ ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬)")
        recent_list = []

        # K-Startup + BizInfo ë³‘ë ¬ ì¡°íšŒ (ê°ê° limitì˜ ì ˆë°˜ì”© ì¡°íšŒ)
        per_source_limit = max(30, limit * 2)  # ìµœì†Œ 30ê°œ, ë˜ëŠ” ìš”ì²­ limitì˜ 2ë°°

        async def fetch_ks():
            return supabase.table('kstartup_complete')\
                .select("announcement_id,biz_pbanc_nm,pbanc_ntrp_nm,pbanc_rcpt_bgng_dt,pbanc_rcpt_end_dt,created_at")\
                .order('created_at', desc=True)\
                .limit(per_source_limit)\
                .execute()

        async def fetch_bi():
            return supabase.table('bizinfo_complete')\
                .select("pblanc_id,pblanc_nm,organ_nm,reqst_begin_ymd,reqst_end_ymd,created_at")\
                .order('created_at', desc=True)\
                .limit(per_source_limit)\
                .execute()

        # ë³‘ë ¬ ì‹¤í–‰
        ks_recent, bi_recent = await asyncio.gather(fetch_ks(), fetch_bi())

        # K-Startup ë°ì´í„° ì²˜ë¦¬
        for item in ks_recent.data:
            title = item.get("biz_pbanc_nm")
            recent_list.append({
                "id": item.get("announcement_id"),
                "title": title,
                "organization": item.get("pbanc_ntrp_nm") or "K-Startup",
                "category": extract_category_from_title(title),
                "start_date": item.get("pbanc_rcpt_bgng_dt"),
                "end_date": item.get("pbanc_rcpt_end_dt"),
                "source": "kstartup",
                "status": calculate_status(item.get("pbanc_rcpt_end_dt")),
                "created_at": item.get("created_at")
            })

        # BizInfo ë°ì´í„° ì²˜ë¦¬
        for item in bi_recent.data:
            title = item.get("pblanc_nm")
            recent_list.append({
                "id": item.get("pblanc_id"),
                "title": title,
                "organization": item.get("organ_nm"),
                "category": extract_category_from_title(title),
                "start_date": item.get("reqst_begin_ymd"),
                "end_date": item.get("reqst_end_ymd"),
                "source": "bizinfo",
                "status": calculate_status(item.get("reqst_end_ymd")),
                "created_at": item.get("created_at")
            })

        # í†µí•© ì •ë ¬
        recent_list.sort(key=lambda x: x.get('created_at', ''), reverse=True)

        # ìºì‹œ ì €ì¥ (status ë³„ë¡œ êµ¬ë¶„)
        set_cache("api_recent_all", recent_list)
        logger.info(f"[Recent] âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ (ì „ì²´ {len(recent_list)}ê°œ)")

        # ìƒíƒœ í•„í„°ë§
        filtered = recent_list if not status else [x for x in recent_list if x.get('status') == status]
        total_count = len(filtered)

        # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit

        logger.info(f"[Recent] í˜ì´ì§€={page}, limit={limit}, total={total_count}, status={status}")

        return {
            "success": True,
            "results": filtered[start_idx:end_idx],
            "total": total_count,
            "page": page,
            "limit": limit
        }

    except Exception as e:
        logger.error(f"ìµœê·¼ ê³µê³  ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def calculate_status(end_date):
    """ë§ˆê°ì¼ ê¸°ì¤€ ìƒíƒœ ê³„ì‚°"""
    if not end_date:
        return "unknown"
    
    try:
        end = datetime.strptime(end_date, "%Y-%m-%d")
        today = datetime.now()
        days_left = (end - today).days
        
        if days_left < 0:
            return "closed"
        elif days_left <= 7:
            return "deadline"
        else:
            return "ongoing"
    except:
        return "unknown"

def calculate_days_left(end_date):
    """ë§ˆê°ì¼ê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ ê³„ì‚°"""
    if not end_date:
        return None
    
    try:
        end = datetime.strptime(end_date, "%Y-%m-%d")
        today = datetime.now()
        days = (end - today).days
        return max(0, days) if days >= 0 else None
    except:
        return None

def extract_category_from_title(title):
    """ì œëª©ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (12ê°œ ì¹´í…Œê³ ë¦¬ - ìŠ¤í¬ë¦°ìƒ· ê¸°ì¤€)"""
    if not title:
        return None

    # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
    # ìš°ì„ ìˆœìœ„: êµ¬ì²´ì  í‚¤ì›Œë“œ ë¨¼ì € ë§¤ì¹­, ì¼ë°˜ì  í‚¤ì›Œë“œëŠ” ë‚˜ì¤‘ì—
    category_keywords = [
        # 1. ìê¸ˆì§€ì›(ë³´ì¡°ê¸ˆ/ì§€ì›ê¸ˆ) - ë¬´ìƒì§€ì›
        (["ë³´ì¡°ê¸ˆ", "ì§€ì›ê¸ˆ", "ì¶œì—°ê¸ˆ", "ë¬´ìƒì§€ì›", "ë¬´ìƒìê¸ˆ"], "ìê¸ˆì§€ì›(ë³´ì¡°ê¸ˆ/ì§€ì›ê¸ˆ)"),

        # 2. ì •ì±…ìê¸ˆ(ìœµì/ëŒ€ì¶œ) - ìƒí™˜ í•„ìš” ìê¸ˆ
        (["ìœµì", "ëŒ€ì¶œ", "ì €ê¸ˆë¦¬", "ì‹ ìš©ë³´ì¦", "ì •ì±…ìê¸ˆ"], "ì •ì±…ìê¸ˆ(ìœµì/ëŒ€ì¶œ)"),

        # 3. ì‹œì„¤/ê³µê°„ ì§€ì›
        (["ì‹œì„¤", "ê³µê°„", "ì…ì£¼", "ë©", "ì¸í”„ë¼", "ì¥ë¹„", "ì„¤ë¹„", "ì„ëŒ€"], "ì‹œì„¤/ê³µê°„ ì§€ì›"),

        # 4. êµìœ¡/ì»¨ì„¤íŒ…/ë©˜í† ë§
        (["êµìœ¡", "ì»¨ì„¤íŒ…", "ë©˜í† ë§", "ì½”ì¹­", "ìë¬¸", "ì§„ë‹¨", "ì•„ì¹´ë°ë¯¸", "ìŠ¤ì¿¨"], "êµìœ¡/ì»¨ì„¤íŒ…/ë©˜í† ë§"),

        # 5. ì¸ë ¥ì§€ì›/ì¼ìë¦¬
        (["ì¸ë ¥", "ì±„ìš©", "ì¸í„´", "ê³ ìš©", "ì¼ìë¦¬", "êµ¬ì¸", "ì±„ìš©ì§€ì›"], "ì¸ë ¥ì§€ì›/ì¼ìë¦¬"),

        # 6. ê¸°ìˆ ê°œë°œ (R&D)
        (["R&D", "ê¸°ìˆ ê°œë°œ", "ì—°êµ¬ê°œë°œ", "ê¸°ìˆ í˜ì‹ ", "R D", "ì—°êµ¬", "ê°œë°œê³¼ì œ", "íŠ¹í—ˆ"], "ê¸°ìˆ ê°œë°œ (R&D)"),

        # 7. í•´ì™¸ì§„ì¶œ/ìˆ˜ì¶œì§€ì›
        (["í•´ì™¸ì§„ì¶œ", "ìˆ˜ì¶œ", "ê¸€ë¡œë²Œ", "êµ­ì œ", "í•´ì™¸ì‹œì¥", "ë¬´ì—­"], "í•´ì™¸ì§„ì¶œ/ìˆ˜ì¶œì§€ì›"),

        # 8. íŒë¡œ/ë§ˆì¼€íŒ… ì§€ì›
        (["íŒë¡œ", "ë§ˆì¼€íŒ…", "íŒë§¤", "ìœ í†µ", "ë‚´ìˆ˜", "í™ë³´", "ë¸Œëœë“œ"], "íŒë¡œ/ë§ˆì¼€íŒ… ì§€ì›"),

        # 9. ë„¤íŠ¸ì›Œí‚¹/ì»¤ë®¤ë‹ˆí‹°
        (["ë„¤íŠ¸ì›Œí¬", "í–‰ì‚¬", "ë°•ëŒíšŒ", "IR", "í”¼ì¹­", "ë°ëª¨ë°ì´", "ì»¨í¼ëŸ°ìŠ¤", "ì„¤ëª…íšŒ", "ìƒë‹´íšŒ"], "ë„¤íŠ¸ì›Œí‚¹/ì»¤ë®¤ë‹ˆí‹°"),

        # 10. ë†ë¦¼ì¶•ìˆ˜ì‚°ì—… íŠ¹ë³„ì§€ì›
        (["ë†ì—…", "ë†ì´Œ", "ì„ì—…", "ì¶•ì‚°", "ìˆ˜ì‚°", "ì–´ì—…", "ë†ë¦¼", "6ì°¨ì‚°ì—…"], "ë†ë¦¼ì¶•ìˆ˜ì‚°ì—… íŠ¹ë³„ì§€ì›"),

        # 11. ê¸°íƒ€ ì§€ì›ì‚¬ì—… (ê°€ì¥ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œëŠ” ë§ˆì§€ë§‰ì—)
        (["ì‚¬ì—…í™”", "ìƒìš©í™”", "ì œí’ˆí™”", "ì°½ì—…", "ìŠ¤íƒ€íŠ¸ì—…", "ê¸°ìˆ ", "ì§€ì›ì‚¬ì—…"], "ê¸°íƒ€ ì§€ì›ì‚¬ì—…"),
    ]

    # ê° ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ë§¤ì¹­
    for keywords, category in category_keywords:
        for keyword in keywords:
            if keyword in title:
                return category

    return None

def format_announcement(data, source):
    """ê³µê³  ë°ì´í„° í¬ë§·íŒ…"""
    if source == "kstartup":
        title = data.get("biz_pbanc_nm")
        # summary ì»¬ëŸ¼ ìš°ì„  ì‚¬ìš©
        simple_summary = data.get("simple_summary")
        detailed_summary = data.get("detailed_summary")
        summary = data.get("summary")  # summary ì»¬ëŸ¼ ì¶”ê°€

        return {
            "id": data.get("announcement_id"),
            "title": title,
            "organization": data.get("pbanc_ntrp_nm") or "K-Startup",
            "category": extract_category_from_title(title),
            "start_date": data.get("pbanc_rcpt_bgng_dt"),
            "end_date": data.get("pbanc_rcpt_end_dt"),
            "content": data.get("full_text") or data.get("pbanc_ctnt") or "",
            "source": "kstartup",
            "source_name": "K-Startup",
            "simple_summary": simple_summary,
            "detailed_summary": detailed_summary or summary,  # detailed_summaryê°€ ì—†ìœ¼ë©´ summary ì‚¬ìš©
            "summary": summary,  # summary ì»¬ëŸ¼ ì¶”ê°€
            "attachments": data.get("attachment_urls"),
            "pdf_url": data.get("pdf_storage_url"),
            "original_url": data.get("detl_pg_url"),  # K-Startup detl_pg_url ì»¬ëŸ¼ ì‚¬ìš©
            "status": calculate_status(data.get("pbanc_rcpt_end_dt")),
            "days_left": calculate_days_left(data.get("pbanc_rcpt_end_dt")),
            "created_at": data.get("created_at"),
            "updated_at": data.get("updated_at")
        }
    else:  # bizinfo
        title = data.get("pblanc_nm")
        # summary ì»¬ëŸ¼ ìš°ì„  ì‚¬ìš©
        simple_summary = data.get("simple_summary")
        detailed_summary = data.get("detailed_summary")
        summary = data.get("summary")  # summary ì»¬ëŸ¼ ì¶”ê°€

        return {
            "id": data.get("pblanc_id"),
            "title": title,
            "organization": data.get("organ_nm"),
            "category": extract_category_from_title(title),
            "start_date": data.get("reqst_begin_ymd"),
            "end_date": data.get("reqst_end_ymd"),
            "source": "bizinfo",
            "source_name": "BizInfo",
            "simple_summary": simple_summary,
            "content": data.get("full_text") or data.get("pblanc_cn") or "",
            "detailed_summary": detailed_summary or summary,  # detailed_summaryê°€ ì—†ìœ¼ë©´ summary ì‚¬ìš©
            "summary": summary,  # summary ì»¬ëŸ¼ ì¶”ê°€
            "attachments": data.get("attachment_urls"),
            "pdf_url": data.get("pdf_storage_url"),
            "original_url": data.get("dtl_url"),  # BizInfo dtl_url ì»¬ëŸ¼ ì‚¬ìš©
            "status": calculate_status(data.get("reqst_end_ymd")),
            "days_left": calculate_days_left(data.get("reqst_end_ymd")),
            "created_at": data.get("created_at"),
            "updated_at": data.get("updated_at"),
            "extra_info": {
                "target": data.get("sprt_trgt"),
                "scale": data.get("sport_scale_cn"),
                "contact": data.get("rqut_mn_cn")
            }
        }

# ================================================
# ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ API
# ================================================
@app.get("/api/admin/dashboard")
async def admin_dashboard():
    """ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ - ì‹œìŠ¤í…œ ìƒíƒœ ë° í†µê³„"""
    if not supabase:
        raise HTTPException(status_code=500, detail="Database not connected")

    try:
        # ìºì‹œëœ stats ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        stats_data = get_cache("api_stats")
        if not stats_data:
            # ìºì‹œê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ì¡°íšŒ
            stats_response = await get_statistics()
            stats_data = stats_response

        # ì‹œìŠ¤í…œ ì •ë³´
        system_info = {
            "cache": {
                "enabled": True,
                "ttl": CACHE_TTL,
                "entries": len(cache_store),
                "keys": list(cache_store.keys())
            },
            "rate_limiting": {
                "enabled": RATE_LIMIT_ENABLED,
                "per_minute": 60 if RATE_LIMIT_ENABLED else None
            },
            "logging": {
                "format": LOG_FORMAT,
                "level": "INFO"
            },
            "database": {
                "connected": supabase is not None,
                "url": SUPABASE_URL[:50] + "..." if SUPABASE_URL else None
            },
            "openai": {
                "connected": openai_client is not None
            }
        }

        # ìµœê·¼ í™œë™ (ìµœê·¼ 10ê°œ ê³µê³ )
        recent_ks = supabase.table('kstartup_complete')\
            .select("announcement_id,biz_pbanc_nm,pbanc_rcpt_end_dt,created_at")\
            .order('created_at', desc=True)\
            .limit(5)\
            .execute()

        recent_bi = supabase.table('bizinfo_complete')\
            .select("pblanc_id,pblanc_nm,reqst_end_ymd,created_at")\
            .order('created_at', desc=True)\
            .limit(5)\
            .execute()

        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "stats": stats_data,
            "system": system_info,
            "recent_activity": {
                "kstartup": recent_ks.data if recent_ks else [],
                "bizinfo": recent_bi.data if recent_bi else []
            }
        }

    except Exception as e:
        logger.error(f"âŒ ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/admin/cache/stats")
async def get_cache_stats_endpoint():
    """ìºì‹œ í†µê³„ ì¡°íšŒ (ê´€ë¦¬ììš©)"""
    try:
        stats = get_cache_stats()
        return {
            "success": True,
            "stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/admin/cache/clear")
async def clear_cache_endpoint(pattern: Optional[str] = Query(None, description="ì‚­ì œí•  ìºì‹œ í‚¤ íŒ¨í„´ (ì˜ˆ: search_*, api_*)")):
    """
    ìºì‹œ ì‚­ì œ (ê´€ë¦¬ììš©)

    - patternì´ ì—†ìœ¼ë©´ ì „ì²´ ìºì‹œ ì‚­ì œ
    - patternì´ ìˆìœ¼ë©´ í•´ë‹¹ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ìºì‹œë§Œ ì‚­ì œ
    """
    try:
        deleted_count = clear_cache(pattern)
        logger.info(f"ğŸ—‘ï¸ ê´€ë¦¬ì ìš”ì²­ìœ¼ë¡œ ìºì‹œ ì‚­ì œë¨ (íŒ¨í„´: {pattern or 'all'}, ê°œìˆ˜: {deleted_count})")
        return {
            "success": True,
            "message": f"ìºì‹œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤ (íŒ¨í„´: {pattern or 'all'})",
            "deleted_count": deleted_count,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/admin/cache/cleanup")
async def cleanup_cache_endpoint():
    """ë§Œë£Œëœ ìºì‹œ í•­ëª© ì •ë¦¬ (ê´€ë¦¬ììš©)"""
    try:
        deleted_count = cleanup_expired_cache()
        logger.info(f"ğŸ§¹ ê´€ë¦¬ì ìš”ì²­ìœ¼ë¡œ ë§Œë£Œ ìºì‹œ ì •ë¦¬ë¨ ({deleted_count}ê°œ)")
        return {
            "success": True,
            "message": f"ë§Œë£Œëœ ìºì‹œ í•­ëª©ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
            "deleted_count": deleted_count,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ================================================
# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… API
# ================================================
@app.get("/api/tasks")
async def get_tasks_list(
    status: Optional[str] = Query(None, description="ì‘ì—… ìƒíƒœ í•„í„° (pending/running/completed/failed)"),
    limit: int = Query(50, ge=1, le=200, description="ë°˜í™˜í•  ìµœëŒ€ ì‘ì—… ìˆ˜")
):
    """
    ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ëª©ë¡ ì¡°íšŒ

    - **status**: ì‘ì—… ìƒíƒœ í•„í„° (ì„ íƒ)
    - **limit**: ë°˜í™˜í•  ìµœëŒ€ ì‘ì—… ìˆ˜ (ê¸°ë³¸ 50)
    """
    try:
        tasks = get_all_tasks()

        # ìƒíƒœ í•„í„°ë§
        if status:
            tasks = [t for t in tasks if t["status"] == status]

        # ê°œìˆ˜ ì œí•œ
        tasks = tasks[:limit]

        # ë¯¼ê°í•œ ì •ë³´ ì œê±° (result ìƒì„¸ ë‚´ìš©ì€ ê°œë³„ ì¡°íšŒì—ì„œë§Œ ì œê³µ)
        summary_tasks = []
        for task in tasks:
            summary_task = {
                **task,
                "result": "..." if task["result"] else None  # ê²°ê³¼ ìš”ì•½
            }
            summary_tasks.append(summary_task)

        return {
            "success": True,
            "total": len(summary_tasks),
            "tasks": summary_tasks
        }

    except Exception as e:
        logger.error(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/tasks/{task_id}")
async def get_task_status_endpoint(task_id: str):
    """
    íŠ¹ì • ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒíƒœ ì¡°íšŒ

    - **task_id**: ì‘ì—… ID (UUID)
    """
    try:
        task = get_task_status(task_id)

        if not task:
            raise HTTPException(status_code=404, detail=f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {task_id}")

        return {
            "success": True,
            "task": task
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/tasks/{task_id}/cancel")
async def cancel_task_endpoint(task_id: str):
    """
    ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì·¨ì†Œ (ì‘ì—…ì´ ì•„ì§ ì‹¤í–‰ ì¤‘ì´ì§€ ì•Šì€ ê²½ìš°ë§Œ ê°€ëŠ¥)

    - **task_id**: ì‘ì—… ID (UUID)
    """
    try:
        task = get_task_status(task_id)

        if not task:
            raise HTTPException(status_code=404, detail=f"ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {task_id}")

        if task["status"] in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
            raise HTTPException(status_code=400, detail=f"ì´ë¯¸ ì™„ë£Œëœ ì‘ì—…ì€ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ìƒíƒœ: {task['status']})")

        if task["status"] == TaskStatus.RUNNING:
            raise HTTPException(status_code=400, detail="ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…ì€ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ì‘ì—… ì·¨ì†Œ
        update_task_status(task_id, TaskStatus.CANCELLED)
        logger.info(f"ğŸš« ì‘ì—… ì·¨ì†Œë¨: {task_id}")

        return {
            "success": True,
            "message": "ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "task_id": task_id
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì‘ì—… ì·¨ì†Œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/tasks/stats/summary")
async def get_tasks_stats():
    """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í†µê³„ ì¡°íšŒ"""
    try:
        tasks = get_all_tasks()

        stats = {
            "total": len(tasks),
            "pending": sum(1 for t in tasks if t["status"] == TaskStatus.PENDING),
            "running": sum(1 for t in tasks if t["status"] == TaskStatus.RUNNING),
            "completed": sum(1 for t in tasks if t["status"] == TaskStatus.COMPLETED),
            "failed": sum(1 for t in tasks if t["status"] == TaskStatus.FAILED),
            "cancelled": sum(1 for t in tasks if t["status"] == TaskStatus.CANCELLED)
        }

        # ì‘ì—… ìœ í˜•ë³„ í†µê³„
        task_types = {}
        for task in tasks:
            task_type = task["task_type"]
            if task_type not in task_types:
                task_types[task_type] = 0
            task_types[task_type] += 1

        return {
            "success": True,
            "stats": stats,
            "by_type": task_types,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ ì‘ì—… í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ================================================
# API ë²„ì „ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸
# ================================================
@app.get("/api/version")
async def get_api_version():
    """
    API ë²„ì „ ì •ë³´ ì¡°íšŒ

    Returns:
        - version: í˜„ì¬ API ë²„ì „
        - major/minor/patch: ë²„ì „ êµ¬ì„± ìš”ì†Œ
        - release_date: ë¦´ë¦¬ì¦ˆ ë‚ ì§œ
        - features: ì£¼ìš” ê¸°ëŠ¥ ëª©ë¡
        - changelog_url: ë³€ê²½ì‚¬í•­ URL
    """
    return {
        "success": True,
        "version": API_VERSION,
        "version_info": {
            "major": API_VERSION_MAJOR,
            "minor": API_VERSION_MINOR,
            "patch": API_VERSION_PATCH
        },
        "release_date": "2025-11-02",
        "features": [
            "ì •ë¶€ì§€ì›ì‚¬ì—… í†µí•© ê²€ìƒ‰ (K-Startup, BizInfo)",
            "ì‹¤ì‹œê°„ ê³µê³  ìˆ˜ì§‘ ë° ì—…ë°ì´íŠ¸",
            "AI ê¸°ë°˜ ìš”ì•½ ìƒì„±",
            "ì „ë¬¸ ê²€ìƒ‰ (í‚¤ì›Œë“œ, ì¹´í…Œê³ ë¦¬, ë‚ ì§œ)",
            "Prometheus ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§",
            "ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì  (API ì‘ë‹µì‹œê°„, DB ì¿¼ë¦¬)",
            "ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ëª¨ë‹ˆí„°ë§",
            "ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œìŠ¤í…œ",
            "API ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ"
        ],
        "api_documentation": "/docs",
        "health_check": "/health",
        "metrics": "/metrics"
    }

if __name__ == "__main__":
    import uvicorn

    print("\n" + "="*60)
    print(f"[FastAPI] ì •ë¶€ì§€ì›ì‚¬ì—… ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„œë²„ ì‹œì‘ (v{API_VERSION})")
    print("="*60)
    print("\n[INFO] ì ‘ì† ì£¼ì†Œ:")
    print("   - ì›¹ ì¸í„°í˜ì´ìŠ¤: http://localhost:8000")
    print("   - API ë¬¸ì„œ: http://localhost:8000/docs")
    print("   - í—¬ìŠ¤ì²´í¬: http://localhost:8000/health")
    print("   - API ë²„ì „: http://localhost:8000/api/version")
    print("   - Prometheus ë©”íŠ¸ë¦­: http://localhost:8000/metrics")
    print("   - ì„±ëŠ¥ ë©”íŠ¸ë¦­: http://localhost:8000/api/performance")
    print("   - ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…: http://localhost:8000/api/tasks")
    print(f"\n[FEATURES] v{API_VERSION} ì£¼ìš” ê¸°ëŠ¥:")
    print("   [OK] ì •ë¶€ì§€ì›ì‚¬ì—… í†µí•© ê²€ìƒ‰ (K-Startup, BizInfo)")
    print("   [OK] ì‹¤ì‹œê°„ ê³µê³  ìˆ˜ì§‘ ë° ì—…ë°ì´íŠ¸")
    print("   [OK] AI ê¸°ë°˜ ìš”ì•½ ìƒì„±")
    print("   [OK] Prometheus ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§")
    print("   [OK] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì  (API ì‘ë‹µì‹œê°„, DB ì¿¼ë¦¬)")
    print("   [OK] ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ëª¨ë‹ˆí„°ë§")
    print("   [OK] ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œìŠ¤í…œ")
    print("   [OK] API ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ (HTTP í—¤ë”, ë²„ì „ ì—”ë“œí¬ì¸íŠ¸)")
    print(f"\n[VERSION] API ë²„ì „: {API_VERSION} (Major: {API_VERSION_MAJOR}, Minor: {API_VERSION_MINOR}, Patch: {API_VERSION_PATCH})")
    print("\n[API] ì£¼ìš” API:")
    print("   - í†µê³„: GET /api/stats")
    print("   - ê²€ìƒ‰: GET /api/search?q=í‚¤ì›Œë“œ&status=ongoing")
    print("   - ìƒì„¸: GET /api/announcement/{id}")
    print("   - ìµœê·¼: GET /api/recent?limit=5")
    print("\n[ADMIN] ê´€ë¦¬ì API:")
    print("   - ëŒ€ì‹œë³´ë“œ: GET /api/admin/dashboard")
    print("   - ìºì‹œ ì‚­ì œ: POST /api/admin/cache/clear")
    print("\n[DEBUG] ë””ë²„ê·¸:")
    print(f"   - Supabase URL: {os.getenv('SUPABASE_URL')}")
    print(f"   - DB ì—°ê²° ìƒíƒœ: {'ì—°ê²°ë¨' if supabase else 'ë¯¸ì—°ê²°'}")
    print(f"   - ìºì‹œ TTL: {CACHE_TTL}ì´ˆ")
    print(f"   - Rate Limiting: {'í™œì„±í™”' if RATE_LIMIT_ENABLED else 'ë¹„í™œì„±í™”'}")
    print(f"   - ë¡œê·¸ í˜•ì‹: {LOG_FORMAT}")
    print("\nì¢…ë£Œ: Ctrl+C")
    print("="*60 + "\n")

    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)
