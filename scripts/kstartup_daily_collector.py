#!/usr/bin/env python3
"""
K-Startup ê³µê³µë°ì´í„° API ìˆ˜ì§‘ê¸° (ì²¨ë¶€íŒŒì¼ í¬í•¨ ë²„ì „)
data.go.kr API + ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì²¨ë¶€íŒŒì¼ ìˆ˜ì§‘
"""
import sys

def get_kst_time():
    """í•œêµ­ ì‹œê°„(KST) ë°˜í™˜"""
    from datetime import datetime, timedelta
    utc_now = datetime.utcnow()
    kst_now = utc_now + timedelta(hours=9)
    return kst_now

sys.stdout.reconfigure(encoding='utf-8')
import os
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from supabase import create_client
from dotenv import load_dotenv
from datetime import datetime
from urllib.parse import unquote
import time
import re

load_dotenv()

# Supabase ì„¤ì •
url = os.environ.get('SUPABASE_URL', 'https://csuziaogycciwgxxmahm.supabase.co')
key = os.environ.get('SUPABASE_SERVICE_KEY') or os.environ.get('SUPABASE_KEY')

# í‚¤ê°€ ì—†ìœ¼ë©´ í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš©
if not key:
    key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImNzdXppYW9neWNjaXdneHhtYWhtIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MzYxNTc4MCwiZXhwIjoyMDY5MTkxNzgwfQ.HnhM7zSLzi7lHVPd2IVQKIACDq_YA05mBMgZbSN1c9Q'

supabase = create_client(url, key)

# ìˆ˜ì§‘ ëª¨ë“œ
COLLECTION_MODE = os.environ.get('COLLECTION_MODE', 'daily')

# API ì„¤ì •
API_URL = "http://apis.data.go.kr/B552735/kisedKstartupService01/getAnnouncementInformation01"
SERVICE_KEY = "rHwfm51FIrtIJjqRL2fJFJFvNsVEng7v7Ud0T44EKQpgKoMEJmN06LZ+KQ2wbTfW29XZSm8OzMuNCUQi+MTlsQ=="

# ì„¸ì…˜ ì„¤ì • (ì›¹ ìŠ¤í¬ë˜í•‘ìš©)
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
})

def parse_xml_item(item):
    """XML item íŒŒì‹± (col name í˜•ì‹)"""
    data = {}
    
    # col íƒœê·¸ë“¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ
    cols = item.findall('col')
    for col in cols:
        name = col.get('name')
        value = col.text if col.text else ''
        data[name] = value.strip()
    
    return data

def fetch_attachments_from_detail_page(detail_url):
    """ìƒì„¸í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ"""
    try:
        response = session.get(detail_url, timeout=10)
        if response.status_code != 200:
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        attachments = []
        
        # ë‹¤ì–‘í•œ ì²¨ë¶€íŒŒì¼ ë§í¬ íŒ¨í„´ ì°¾ê¸°
        # 1. ì§ì ‘ ë‹¤ìš´ë¡œë“œ ë§í¬
        download_links = soup.find_all('a', href=re.compile(r'(/cmm/fms/FileDown\.do|/afile/fileDownload/|download\.do)'))
        
        for link in download_links:
            href = link.get('href', '')
            text = link.get_text(strip=True) or 'ì²¨ë¶€íŒŒì¼'
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if href.startswith('/'):
                href = f"https://www.k-startup.go.kr{href}"
            
            # ì¤‘ë³µ ì œê±°
            if href not in [a.get('url') for a in attachments]:
                attachments.append({
                    'url': href,
                    'filename': text,
                    'type': 'FILE'
                })
        
        # 2. onclick í˜•íƒœì˜ ë‹¤ìš´ë¡œë“œ
        onclick_links = soup.find_all('a', onclick=re.compile(r'fileDown|download'))
        for link in onclick_links:
            onclick = link.get('onclick', '')
            text = link.get_text(strip=True) or 'ì²¨ë¶€íŒŒì¼'
            
            # onclickì—ì„œ íŒŒì¼ ID ì¶”ì¶œ
            match = re.search(r"['\"](\d+)['\"]", onclick)
            if match:
                file_id = match.group(1)
                url = f"https://www.k-startup.go.kr/cmm/fms/FileDown.do?fileNo={file_id}"
                
                if url not in [a.get('url') for a in attachments]:
                    attachments.append({
                        'url': url,
                        'filename': text,
                        'type': 'FILE'
                    })
        
        return attachments
        
    except Exception as e:
        print(f"    [ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì˜¤ë¥˜]: {str(e)[:50]}")
        return []

def fetch_page(page_no, num_of_rows=200):  # êµ¬ê¸€ì‹œíŠ¸ì²˜ëŸ¼ 200ê°œì”© ê°€ì ¸ì˜¤ê¸°
    """APIì—ì„œ í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    try:
        params = {
            'serviceKey': unquote(SERVICE_KEY),
            'page': page_no,  # pageNo â†’ pageë¡œ ë³€ê²½ (êµ¬ê¸€ì‹œíŠ¸ì™€ ë™ì¼)
            'perPage': num_of_rows  # numOfRows â†’ perPageë¡œ ë³€ê²½
        }
        
        response = requests.get(API_URL, params=params, timeout=30)
        
        if response.status_code != 200:
            return None, 0, 0
        
        # XML íŒŒì‹±
        try:
            root = ET.fromstring(response.content)
            
            # ì „ì²´ ê°œìˆ˜ í™•ì¸
            total_count_elem = root.find('totalCount')
            total = int(total_count_elem.text) if total_count_elem is not None else 0
            
            # ë°ì´í„° ì¶”ì¶œ
            data_elem = root.find('data')
            if data_elem is None:
                return [], total, 0
            
            # ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
            items = data_elem.findall('item')
            
            announcements = []
            for item in items:
                raw_data = parse_xml_item(item)
                
                # í•„ë“œ ë§¤í•‘ (í…Œì´ë¸” ì»¬ëŸ¼ì— ë§ê²Œ)
                ann = {}
                
                # pbanc_sn (ê³µê³ ë²ˆí˜¸)
                pbanc_sn = raw_data.get('pbanc_sn', '')
                if not pbanc_sn:
                    continue
                    
                ann['announcement_id'] = f"KS_{pbanc_sn}"
                # ann['pbanc_sn'] = pbanc_sn  # ìš°ë¦¬ í…Œì´ë¸”ì—ëŠ” ì´ ì»¬ëŸ¼ì´ ì—†ìŒ
                
                # í•„ìˆ˜ í•„ë“œ
                ann['biz_pbanc_nm'] = raw_data.get('biz_pbanc_nm') or raw_data.get('intg_pbanc_biz_nm', '')
                ann['pbanc_ctnt'] = raw_data.get('pbanc_ctnt', '')  # ê³µê³ ë‚´ìš©
                ann['supt_biz_clsfc'] = raw_data.get('supt_biz_clsfc', '')  # ì§€ì›ì‚¬ì—…ë¶„ë¥˜
                ann['aply_trgt_ctnt'] = raw_data.get('aply_trgt_ctnt', '')  # ì§€ì›ëŒ€ìƒë‚´ìš©
                ann['supt_regin'] = raw_data.get('supt_regin', '')  # ì§€ì›ì§€ì—­
                ann['pbanc_rcpt_bgng_dt'] = raw_data.get('pbanc_rcpt_bgng_dt', '')  # ì ‘ìˆ˜ì‹œì‘ì¼
                ann['pbanc_rcpt_end_dt'] = raw_data.get('pbanc_rcpt_end_dt', '')  # ì ‘ìˆ˜ì¢…ë£Œì¼
                ann['pbanc_ntrp_nm'] = raw_data.get('pbanc_ntrp_nm', '')  # ê³µê³ ê¸°ê´€ëª…
                ann['biz_gdnc_url'] = raw_data.get('biz_gdnc_url', '')  # ì‚¬ì—…ì•ˆë‚´URL
                ann['detl_pg_url'] = raw_data.get('detl_pg_url', '')  # ìƒì„¸í˜ì´ì§€URL
                
                # bsns_sumryëŠ” pbanc_ctnt ì‚¬ìš©
                ann['bsns_sumry'] = ann['pbanc_ctnt'][:5000] if ann['pbanc_ctnt'] else ''
                
                # ìƒíƒœ
                if raw_data.get('rcrt_prgs_yn') == 'Y':
                    ann['status'] = 'ëª¨ì§‘ì¤‘'
                else:
                    ann['status'] = 'ë§ˆê°'
                    
                # íƒ€ì„ìŠ¤íƒ¬í”„
                ann['created_at'] = get_kst_time().isoformat()
                
                announcements.append(ann)
            
            return announcements, total, len(announcements)
            
        except ET.ParseError:
            return None, 0, 0
            
    except Exception:
        return None, 0, 0

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print(f"ğŸš€ K-Startup ê³µê³µë°ì´í„° API ìˆ˜ì§‘ ì‹œì‘ ({COLLECTION_MODE} ëª¨ë“œ)")
    print("ğŸ“ ì²¨ë¶€íŒŒì¼ ìˆ˜ì§‘ í¬í•¨")
    print("="*60)
    
    # ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ (ì „ì²´ ê°€ì ¸ì˜¤ê¸° - SupabaseëŠ” ê¸°ë³¸ 1000ê°œ ì œí•œì´ ìˆìŒ)
    # ì—¬ëŸ¬ í˜ì´ì§€ë¡œ ë‚˜ëˆ ì„œ ê°€ì ¸ì˜¤ê¸°
    existing_ids = set()
    offset = 0
    limit = 1000
    while True:
        existing = supabase.table('kstartup_complete').select('announcement_id').range(offset, offset + limit - 1).execute()
        if not existing.data:
            break
        for item in existing.data:
            existing_ids.add(item['announcement_id'])
        if len(existing.data) < limit:
            break
        offset += limit
    print(f"âœ… ê¸°ì¡´ ë°ì´í„°: {len(existing_ids)}ê°œ\n")
    
    # ì²« í˜ì´ì§€ë¡œ ì „ì²´ ê°œìˆ˜ í™•ì¸
    items, total_count, _ = fetch_page(1, 10)
    
    if items is None:
        print("[ERROR] API ì ‘ê·¼ ì‹¤íŒ¨")
        return
    
    print(f"ğŸ“Š ì „ì²´ ê³µê³  ìˆ˜: {total_count}ê°œ")
    
    # ëª¨ë“œë³„ ì„¤ì •
    if COLLECTION_MODE == 'full':
        # Full ëª¨ë“œëŠ” ìµœëŒ€ 20í˜ì´ì§€ê¹Œì§€ë§Œ (2000ê°œ)
        total_pages = min(20, (total_count // 100) + 1)
        print(f"ğŸ“Š Full ëª¨ë“œ: {total_pages}í˜ì´ì§€ ìˆ˜ì§‘ (ìµœëŒ€ 2000ê°œ)")
    else:
        # Daily ëª¨ë“œëŠ” ìµœëŒ€ 3í˜ì´ì§€ (300ê°œ)
        total_pages = min(3, (total_count // 100) + 1)
        print(f"ğŸ“Š Daily ëª¨ë“œ: {total_pages}í˜ì´ì§€ ìˆ˜ì§‘ (ìµœëŒ€ 300ê°œ)")
    
    all_new = 0
    all_updated = 0
    all_attachments = 0
    errors = 0
    
    # ì—°ì† ì¤‘ë³µ ì¹´ìš´í„° (êµ¬ê¸€ì‹œíŠ¸ ë°©ì‹)
    consecutive_duplicates = 0
    
    # í˜ì´ì§€ë³„ ìˆ˜ì§‘
    for page in range(1, total_pages + 1):
        print(f"\ní˜ì´ì§€ {page}/{total_pages} ì²˜ë¦¬ì¤‘...")
        items, _, count = fetch_page(page, 200)  # 200ê°œì”© ê°€ì ¸ì˜¤ê¸°
        
        if items is None:
            errors += 1
            continue
        
        if not items:
            print("  ë°ì´í„° ì—†ìŒ")
            break
        
        new_count = 0
        update_count = 0
        attach_count = 0
        page_errors = 0
        page_duplicates = 0  # í˜ì´ì§€ë³„ ì¤‘ë³µ ìˆ˜
        
        for item in items:
            try:
                # ì²¨ë¶€íŒŒì¼ ìˆ˜ì§‘ (ìƒˆë¡œìš´ ê³µê³ ë§Œ)
                if item['announcement_id'] not in existing_ids and item.get('detl_pg_url'):
                    attachments = fetch_attachments_from_detail_page(item['detl_pg_url'])
                    item['attachment_urls'] = attachments
                    # attachment_count ì œê±° - attachment_urls ê¸¸ì´ë¡œ ê³„ì‚° ê°€ëŠ¥
                    if attachments:
                        attach_count += 1
                else:
                    item['attachment_urls'] = []
                
                if item['announcement_id'] in existing_ids:
                    # ê¸°ì¡´ ë°ì´í„° - ì¤‘ë³µ ì¹´ìš´íŠ¸
                    page_duplicates += 1
                    consecutive_duplicates += 1
                    
                    # ì—°ì† 50ê°œ ì¤‘ë³µ ì‹œ ì¢…ë£Œ (êµ¬ê¸€ì‹œíŠ¸ëŠ” 10ê°œì§€ë§Œ ìš°ë¦¬ëŠ” ì¢€ ë” ì—¬ìœ ìˆê²Œ)
                    if consecutive_duplicates >= 50:
                        print(f"\nğŸ“Œ ì—°ì† {consecutive_duplicates}ê°œ ì¤‘ë³µ â†’ ìˆ˜ì§‘ ì¢…ë£Œ")
                        all_new += new_count
                        all_updated += update_count
                        all_attachments += attach_count
                        # ìµœì¢… ë³´ê³ ì„œë¡œ ì´ë™
                        break
                    
                    # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸ (ì²¨ë¶€íŒŒì¼ì´ ì—†ë˜ ê²½ìš°ì—ë§Œ)
                    if len(item.get('attachment_urls', [])) > 0:
                        existing_attach = supabase.table('kstartup_complete').select('attachment_urls').eq('announcement_id', item['announcement_id']).execute()
                        if existing_attach.data and len(existing_attach.data[0].get('attachment_urls', [])) == 0:
                            # ê¸°ì¡´ì— ì²¨ë¶€íŒŒì¼ì´ ì—†ì—ˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                            result = supabase.table('kstartup_complete').update({
                                'attachment_urls': item['attachment_urls']
                            }).eq('announcement_id', item['announcement_id']).execute()
                            
                            if result.data:
                                update_count += 1
                else:
                    # ì‹ ê·œ ë°ì´í„° ì‚½ì…
                    consecutive_duplicates = 0  # ì‹ ê·œ ë°ì´í„°ë©´ ì¤‘ë³µ ì¹´ìš´í„° ë¦¬ì…‹
                    result = supabase.table('kstartup_complete').insert(item).execute()
                    
                    if result.data:
                        new_count += 1
                        existing_ids.add(item['announcement_id'])
                        
            except Exception as e:
                page_errors += 1
                if page_errors <= 2:  # ì²˜ìŒ 2ê°œë§Œ ì—ëŸ¬ í‘œì‹œ
                    print(f"    [ERROR] {item['announcement_id']}: {str(e)[:100]}")
        
        all_new += new_count
        all_updated += update_count
        all_attachments += attach_count
        errors += page_errors
        
        print(f"  ê²°ê³¼: ì‹ ê·œ {new_count}ê°œ, ì—…ë°ì´íŠ¸ {update_count}ê°œ, ì¤‘ë³µ {page_duplicates}ê°œ")
        print(f"  ì²¨ë¶€íŒŒì¼: {attach_count}ê°œ ê³µê³ ì—ì„œ ìˆ˜ì§‘")
        if page_errors > 0:
            print(f"  ì˜¤ë¥˜: {page_errors}ê°œ")
        
        # ì—°ì† ì¤‘ë³µìœ¼ë¡œ ì¢…ë£Œëœ ê²½ìš°
        if consecutive_duplicates >= 50:
            break
        
        time.sleep(0.5)  # API ë¶€í•˜ ë°©ì§€
    
    # ìµœì¢… ë³´ê³ 
    print("\n" + "="*60)
    print("ğŸ“Š K-Startup ê³µê³µë°ì´í„° API ìˆ˜ì§‘ ì™„ë£Œ")
    print("="*60)
    print(f"âœ… ì‹ ê·œ: {all_new}ê°œ")
    print(f"ğŸ“ ì—…ë°ì´íŠ¸: {all_updated}ê°œ")
    print(f"ğŸ“ ì²¨ë¶€íŒŒì¼ ìˆ˜ì§‘: {all_attachments}ê°œ ê³µê³ ")
    print(f"âŒ ì˜¤ë¥˜: {errors}ê°œ")
    print(f"ğŸ“Š ì „ì²´: {all_new + all_updated}ê°œ ì²˜ë¦¬")
    print("="*60)

if __name__ == "__main__":
    main()