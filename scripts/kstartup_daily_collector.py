#!/usr/bin/env python3
"""
K-Startup ì¼ì¼ ìˆ˜ì§‘ê¸° (ì›Œí¬í”Œë¡œìš° í˜¸í™˜ ë²„ì „)
- daily/full ëª¨ë“œ ì§€ì›
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê³ ì† ìˆ˜ì§‘
- ì¤‘ë³µ ì²´í¬ ë° ì¦ë¶„ ì—…ë°ì´íŠ¸
"""
import sys
sys.stdout.reconfigure(encoding='utf-8')
import os
import requests
from bs4 import BeautifulSoup
import json
from supabase import create_client
from dotenv import load_dotenv
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time

load_dotenv()

# Supabase ì„¤ì •
url = os.environ.get('SUPABASE_URL')
key = os.environ.get('SUPABASE_KEY')
supabase = create_client(url, key)

# ìˆ˜ì§‘ ëª¨ë“œ
COLLECTION_MODE = os.environ.get('COLLECTION_MODE', 'daily')

# ì „ì—­ ë³€ìˆ˜
lock = threading.Lock()
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
})

progress = {
    'total': 0,
    'new': 0,
    'updated': 0,
    'skipped': 0,
    'errors': 0
}

def fetch_page(page):
    """í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    url = "https://www.k-startup.go.kr/apigateway/ksus/bsns/anm/list"
    params = {
        'schClsfCd': 'PBC010',
        'sortType': 'recent',
        'currentPage': page,
        'perPage': 200,
        'searchStatus': '',
        'schStr': '',
        'schEdate': '',
        'returnType': 'JSON'
    }
    
    try:
        response = session.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if data and 'dataList' in data:
            return data['dataList']
    except Exception as e:
        print(f"   âŒ í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
    
    return []

def parse_detail_page(url, announcement_id):
    """ìƒì„¸í˜ì´ì§€ íŒŒì‹±"""
    try:
        response = session.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # bsns_sumry ì¶”ì¶œ
        content_sections = []
        for selector in ['.content_wrap', '.detail_content', '.board_view']:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                if text and len(text) > 100:
                    content_sections.append(text)
        
        bsns_sumry = ' '.join(content_sections[:3])[:5000] if content_sections else None
        
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
        attachments = []
        download_links = soup.find_all('a', href=lambda x: x and '/afile/fileDownload/' in x)
        
        for link in download_links:
            href = link.get('href', '')
            text = link.get_text(strip=True) or 'ì²¨ë¶€íŒŒì¼'
            
            if href.startswith('/'):
                href = f"https://www.k-startup.go.kr{href}"
            
            attachments.append({
                'url': href,
                'text': text,
                'type': 'FILE'
            })
        
        return bsns_sumry, attachments
        
    except Exception as e:
        print(f"   âŒ ìƒì„¸í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None, []

def process_announcement(item):
    """ê³µê³  ì²˜ë¦¬"""
    try:
        # ë°ì´í„° ë§¤í•‘
        announcement_id = f"KS_{item.get('pbancSn', '')}"
        
        # ìƒì„¸í˜ì´ì§€ URL ê²°ì •
        status = item.get('pbancSttsCd', '')
        pbanc_sn = item.get('pbancSn', '')
        
        if status == 'PBC030':  # ë§ˆê°
            detail_url = f'https://www.k-startup.go.kr/web/contents/bizpbanc-deadline.do?schM=view&pbancSn={pbanc_sn}'
        else:  # ì§„í–‰ì¤‘
            detail_url = f'https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do?schM=view&pbancSn={pbanc_sn}'
        
        # ìƒì„¸í˜ì´ì§€ íŒŒì‹±
        bsns_sumry, attachments = parse_detail_page(detail_url, announcement_id)
        
        # ë°ì´í„° ì¤€ë¹„
        data = {
            'announcement_id': announcement_id,
            'pbanc_sn': item.get('pbancSn'),
            'biz_pbanc_nm': item.get('bizPbancNm', ''),
            'detl_pg_url': detail_url,
            'spt_fld_cn': item.get('sprtFldCn', ''),
            'spt_trgt_cn': item.get('pbancSuptTrgtCn', ''),
            'pbanc_bgng_dt': item.get('pbancBgngDt', ''),
            'pbanc_ddln_dt': item.get('pbancDdlnDt', ''),
            'bsns_sumry': bsns_sumry or item.get('bizPbancDtlCn', ''),
            'attachment_urls': attachments if attachments else [],
            'attachment_count': len(attachments),
            'status': 'ëª¨ì§‘ì¤‘' if status != 'PBC030' else 'ë§ˆê°',
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        # DB ì—…ì„œíŠ¸
        result = supabase.table('kstartup_complete').upsert(
            data,
            on_conflict='announcement_id'
        ).execute()
        
        if result.data:
            with lock:
                if attachments:
                    progress['updated'] += 1
                else:
                    progress['new'] += 1
            return True
            
    except Exception as e:
        with lock:
            progress['errors'] += 1
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print(f"ğŸš€ K-Startup ìˆ˜ì§‘ ì‹œì‘ ({COLLECTION_MODE} ëª¨ë“œ)")
    print("="*60)
    
    # ëª¨ë“œë³„ í˜ì´ì§€ ì„¤ì •
    if COLLECTION_MODE == 'full':
        max_pages = 259  # ì „ì²´
        print("ğŸ“Š Full ëª¨ë“œ: ì „ì²´ ë°ì´í„° ìˆ˜ì§‘")
    else:
        max_pages = 3  # dailyëŠ” ìµœê·¼ 3í˜ì´ì§€ë§Œ
        print("ğŸ“Š Daily ëª¨ë“œ: ìµœê·¼ 600ê°œë§Œ í™•ì¸")
    
    # ê¸°ì¡´ ë°ì´í„° ID ìˆ˜ì§‘
    existing = supabase.table('kstartup_complete').select('announcement_id').execute()
    existing_ids = {item['announcement_id'] for item in existing.data} if existing.data else set()
    print(f"âœ… ê¸°ì¡´ ë°ì´í„°: {len(existing_ids)}ê°œ\n")
    
    all_items = []
    consecutive_duplicates = 0
    
    # í˜ì´ì§€ë³„ ìˆ˜ì§‘
    for page in range(1, max_pages + 1):
        items = fetch_page(page)
        
        if not items:
            print(f"   í˜ì´ì§€ {page}: ë°ì´í„° ì—†ìŒ")
            break
        
        # ì¤‘ë³µ ì²´í¬
        new_items = []
        page_duplicates = 0
        
        for item in items:
            ann_id = f"KS_{item.get('pbancSn', '')}"
            if ann_id not in existing_ids:
                new_items.append(item)
            else:
                page_duplicates += 1
        
        all_items.extend(new_items)
        
        if new_items:
            print(f"   í˜ì´ì§€ {page}: {len(new_items)}ê°œ ì‹ ê·œ (ì¤‘ë³µ {page_duplicates}ê°œ)")
            consecutive_duplicates = 0
        else:
            print(f"   í˜ì´ì§€ {page}: ëª¨ë‘ ì¤‘ë³µ ({page_duplicates}ê°œ)")
            consecutive_duplicates += 1
        
        # ì—°ì† ì¤‘ë³µ ì‹œ ì¢…ë£Œ
        if consecutive_duplicates >= 3 and COLLECTION_MODE == 'daily':
            print(f"\nâš¡ ì—°ì† 3í˜ì´ì§€ ì¤‘ë³µ - ì¡°ê¸° ì¢…ë£Œ")
            break
    
    progress['total'] = len(all_items)
    
    if not all_items:
        print("\nâœ… ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ")
        return
    
    print(f"\nğŸ“Š ì²˜ë¦¬í•  ì‹ ê·œ ë°ì´í„°: {len(all_items)}ê°œ")
    print("ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...\n")
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = [executor.submit(process_announcement, item) for item in all_items]
        
        for i, future in enumerate(as_completed(futures), 1):
            try:
                future.result()
                if i % 50 == 0:
                    print(f"   ì§„í–‰: {i}/{len(all_items)} ({i*100//len(all_items)}%)")
            except:
                pass
    
    # ìµœì¢… ë³´ê³ 
    print("\n" + "="*60)
    print("ğŸ“Š K-Startup ìˆ˜ì§‘ ì™„ë£Œ")
    print("="*60)
    print(f"âœ… ì‹ ê·œ ì¶”ê°€: {progress['new']}ê°œ")
    print(f"ğŸ“ ì—…ë°ì´íŠ¸: {progress['updated']}ê°œ")
    print(f"â­ï¸  ê±´ë„ˆëœ€: {progress['skipped']}ê°œ")
    print(f"âŒ ì˜¤ë¥˜: {progress['errors']}ê°œ")
    print("="*60)

if __name__ == "__main__":
    main()