#!/usr/bin/env python3
"""
ê¸°ì—…ë§ˆë‹¹ ì „ì²´ ê³µê³  ì²¨ë¶€íŒŒì¼ ì •ë³´ í¬ë¡¤ë§ - ì†ë„ ê°•í™” ë²„ì „
- ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ë™ì‹œ ì²˜ë¦¬
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- ì—°ê²° í’€ ì‚¬ìš©
"""
import os
import sys
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, parse_qs, urlparse
import time
import re
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import queue
from supabase import create_client

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(threadName)s] - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'bizinfo_fast_crawler_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

# ì„±ëŠ¥ ì„¤ì •
MAX_WORKERS = 10  # ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜
BATCH_SIZE = 100  # ë°°ì¹˜ í¬ê¸°
REQUEST_TIMEOUT = 10  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
RETRY_COUNT = 2  # ì¬ì‹œë„ íšŸìˆ˜

# ì„¸ì…˜ í’€ ê´€ë¦¬
session_pool = queue.Queue(maxsize=MAX_WORKERS)
for _ in range(MAX_WORKERS):
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Connection': 'keep-alive'
    })
    session_pool.put(session)

# í†µê³„ ê´€ë¦¬
stats_lock = Lock()
stats = {
    'processed': 0,
    'with_attachments': 0,
    'with_hashtags': 0,
    'failed': 0,
    'total': 0
}

def get_session():
    """ì„¸ì…˜ í’€ì—ì„œ ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°"""
    return session_pool.get()

def return_session(session):
    """ì„¸ì…˜ í’€ì— ì„¸ì…˜ ë°˜í™˜"""
    session_pool.put(session)

def extract_file_type(text):
    """íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¸¡"""
    text_lower = text.lower()
    if '.hwp' in text_lower or 'í•œê¸€' in text_lower:
        return 'HWP'
    elif '.pdf' in text_lower:
        return 'PDF'
    elif '.doc' in text_lower:
        return 'DOCX'
    elif '.xls' in text_lower:
        return 'XLSX'
    elif '.zip' in text_lower:
        return 'ZIP'
    elif '.ppt' in text_lower:
        return 'PPT'
    elif any(ext in text_lower for ext in ['.jpg', '.jpeg', '.png', '.gif']):
        return 'IMAGE'
    else:
        return 'UNKNOWN'

def clean_filename(text):
    """íŒŒì¼ëª… ì •ë¦¬"""
    if not text:
        return None
    
    # íŒŒì¼ëª… íŒ¨í„´ ë§¤ì¹­
    patterns = [
        r'([^\/\\:*?"<>|\n\r\t]+\.(?:hwp|hwpx|pdf|doc|docx|xls|xlsx|ppt|pptx|zip|jpg|jpeg|png|gif|txt|rtf))\b',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            filename = match.group(1).strip()
            filename = re.sub(r'^(ì²¨ë¶€íŒŒì¼\s*|ë‹¤ìš´ë¡œë“œ\s*)', '', filename)
            filename = re.sub(r'\s*(ë‹¤ìš´ë¡œë“œ|ì²¨ë¶€íŒŒì¼)\s*$', '', filename)
            return filename
    
    return None

def process_single_announcement(data):
    """ë‹¨ì¼ ê³µê³  ì²˜ë¦¬ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
    pblanc_id = data['pblanc_id']
    pblanc_nm = data['pblanc_nm']
    dtl_url = data.get('dtl_url')
    
    if not dtl_url:
        return pblanc_id, [], [], "NO_URL"
    
    session = get_session()
    try:
        # ì¬ì‹œë„ ë¡œì§
        for attempt in range(RETRY_COUNT):
            try:
                response = session.get(dtl_url, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                break
            except Exception as e:
                if attempt == RETRY_COUNT - 1:
                    raise e
                time.sleep(1)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
        attachments = []
        unique_files = {}
        
        # ëª¨ë“  ë§í¬ì—ì„œ ì²¨ë¶€íŒŒì¼ íŒ¨í„´ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # atchFileId íŒ¨í„´ ì°¾ê¸°
            if 'atchFileId=' in href:
                # URL íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                atch_file_id = href.split('atchFileId=')[1].split('&')[0]
                file_sn = '0'
                if 'fileSn=' in href:
                    file_sn = href.split('fileSn=')[1].split('&')[0]
                
                unique_key = f"{atch_file_id}_{file_sn}"
                
                if unique_key not in unique_files:
                    # ì§ì ‘ URL êµ¬ì„±
                    direct_url = f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={atch_file_id}&fileSn={file_sn}"
                    
                    file_type = extract_file_type(text)
                    display_filename = clean_filename(text) or text
                    
                    attachment = {
                        'url': direct_url,
                        'type': file_type,
                        'safe_filename': f"{pblanc_id}_{len(attachments)+1:02d}.{file_type.lower()}",
                        'display_filename': display_filename,
                        'original_filename': display_filename,
                        'text': text,
                        'params': {
                            'atchFileId': atch_file_id,
                            'fileSn': file_sn
                        }
                    }
                    
                    unique_files[unique_key] = attachment
                    attachments.append(attachment)
        
        # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
        hashtags = []
        tag_list = soup.find('ul', class_='tag_ul_list')
        if tag_list:
            tag_items = tag_list.find_all('li', class_=re.compile(r'tag_li_list\d'))
            for item in tag_items:
                link = item.find('a')
                if link:
                    tag_text = link.get_text(strip=True)
                    if tag_text and tag_text not in hashtags:
                        hashtags.append(tag_text)
        
        return pblanc_id, attachments, hashtags, "SUCCESS"
        
    except requests.exceptions.Timeout:
        return pblanc_id, [], [], "TIMEOUT"
    except Exception as e:
        logging.debug(f"í¬ë¡¤ë§ ì˜¤ë¥˜ ({pblanc_id}): {e}")
        return pblanc_id, [], [], f"ERROR: {str(e)[:50]}"
    finally:
        return_session(session)

def update_batch_to_db(supabase, results):
    """ë°°ì¹˜ ê²°ê³¼ë¥¼ DBì— ì—…ë°ì´íŠ¸"""
    try:
        for pblanc_id, attachments, hashtags, status in results:
            if status == "SUCCESS" or (status == "NO_URL"):
                update_data = {
                    'attachment_urls': attachments,
                    'attachment_processing_status': {
                        'processed': True,
                        'count': len(attachments),
                        'hashtag_count': len(hashtags),
                        'processed_at': datetime.now().isoformat(),
                        'status': status
                    }
                }
                
                if hashtags:
                    update_data['hash_tag'] = ', '.join(hashtags)
                
                # Supabase ì—…ë°ì´íŠ¸
                supabase.table('bizinfo_complete').update(
                    update_data
                ).eq('pblanc_id', pblanc_id).execute()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                with stats_lock:
                    stats['processed'] += 1
                    if attachments:
                        stats['with_attachments'] += 1
                    if hashtags:
                        stats['with_hashtags'] += 1
            else:
                with stats_lock:
                    stats['failed'] += 1
        
        return True
        
    except Exception as e:
        logging.error(f"DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return False

def process_batch_parallel(batch):
    """ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # ëª¨ë“  ì‘ì—… ì œì¶œ
        future_to_data = {
            executor.submit(process_single_announcement, data): data 
            for data in batch
        }
        
        # ì™„ë£Œëœ ì‘ì—… ìˆ˜ì§‘
        for future in as_completed(future_to_data):
            data = future_to_data[future]
            try:
                result = future.result(timeout=REQUEST_TIMEOUT * 2)
                results.append(result)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                pblanc_id = result[0]
                attachments = result[1]
                hashtags = result[2]
                status = result[3]
                
                with stats_lock:
                    current = stats['processed'] + stats['failed'] + 1
                    total = stats['total']
                
                if status == "SUCCESS":
                    logging.info(f"[{current}/{total}] {pblanc_id}: ì²¨ë¶€ {len(attachments)}ê°œ, íƒœê·¸ {len(hashtags)}ê°œ")
                elif status == "NO_URL":
                    logging.info(f"[{current}/{total}] {pblanc_id}: URL ì—†ìŒ")
                else:
                    logging.warning(f"[{current}/{total}] {pblanc_id}: {status}")
                    
            except Exception as e:
                logging.error(f"ì‘ì—… ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                results.append((data['pblanc_id'], [], [], f"EXCEPTION: {str(e)[:50]}"))
    
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print(" ê¸°ì—…ë§ˆë‹¹ ì²¨ë¶€íŒŒì¼ ê³ ì† í¬ë¡¤ë§")
    print("=" * 60)
    print(f"ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ: {MAX_WORKERS}ê°œ")
    print(f"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}ê°œ")
    print("=" * 60)
    
    # Supabase ì—°ê²°
    supabase_url = os.environ.get('SUPABASE_URL')
    supabase_key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
    
    if not supabase_url or not supabase_key:
        print("âŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("SUPABASE_URLê³¼ SUPABASE_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)
    
    supabase = create_client(supabase_url, supabase_key)
    logging.info("âœ… Supabase ì—°ê²° ì„±ê³µ")
    
    # ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ
    print("\n1. ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ ì¤‘...")
    try:
        # attachment_urlsê°€ nullì´ê±°ë‚˜ ë¹ˆ ë°°ì—´ì¸ ë°ì´í„° ìš°ì„  ì²˜ë¦¬
        response = supabase.table('bizinfo_complete').select(
            'id', 'pblanc_id', 'pblanc_nm', 'dtl_url'
        ).or_(
            'attachment_urls.is.null',
            'attachment_urls.eq.[]'
        ).execute()
        
        unprocessed = response.data
        
        # ì „ì²´ ë°ì´í„°ë„ ê°€ì ¸ì˜¤ê¸° (ì¬ì²˜ë¦¬ìš©)
        response_all = supabase.table('bizinfo_complete').select(
            'id', 'pblanc_id', 'pblanc_nm', 'dtl_url'
        ).execute()
        
        all_data = response_all.data
        
        print(f"âœ… ì „ì²´ ë°ì´í„°: {len(all_data)}ê°œ")
        print(f"âœ… ë¯¸ì²˜ë¦¬ ë°ì´í„°: {len(unprocessed)}ê°œ")
        
        # ë¯¸ì²˜ë¦¬ ë°ì´í„° ìš°ì„ , ê·¸ ë‹¤ìŒ ì „ì²´ ì¬ì²˜ë¦¬
        targets = unprocessed + [d for d in all_data if d not in unprocessed]
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    if not targets:
        print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í†µê³„ ì´ˆê¸°í™”
    stats['total'] = len(targets)
    
    print(f"\n2. í¬ë¡¤ë§ ì‹œì‘ (ì´ {len(targets)}ê°œ)")
    print("-" * 60)
    
    start_time = time.time()
    
    try:
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(targets), BATCH_SIZE):
            batch = targets[i:i+BATCH_SIZE]
            batch_num = (i // BATCH_SIZE) + 1
            total_batches = (len(targets) + BATCH_SIZE - 1) // BATCH_SIZE
            
            logging.info(f"\në°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬
            results = process_batch_parallel(batch)
            
            # DB ì—…ë°ì´íŠ¸
            if results:
                update_batch_to_db(supabase, results)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            with stats_lock:
                print(f"\nì§„í–‰ ìƒí™©: {stats['processed']}/{stats['total']} ì™„ë£Œ")
                print(f"  - ì²¨ë¶€íŒŒì¼ ìˆìŒ: {stats['with_attachments']}ê°œ")
                print(f"  - í•´ì‹œíƒœê·¸ ìˆìŒ: {stats['with_hashtags']}ê°œ")
                print(f"  - ì‹¤íŒ¨: {stats['failed']}ê°œ")
            
            # ë‹¤ìŒ ë°°ì¹˜ ì „ ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i + BATCH_SIZE < len(targets):
                time.sleep(2)
                
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logging.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    
    # ìµœì¢… ê²°ê³¼
    elapsed_time = time.time() - start_time
    
    print("\n" + "=" * 60)
    print(" í¬ë¡¤ë§ ì™„ë£Œ")
    print("=" * 60)
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {stats['processed']}ê°œ")
    print(f"ğŸ“ ì²¨ë¶€íŒŒì¼ ìˆìŒ: {stats['with_attachments']}ê°œ")
    print(f"ğŸ·ï¸ í•´ì‹œíƒœê·¸ ìˆìŒ: {stats['with_hashtags']}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {stats['failed']}ê°œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    print(f"ğŸ“Š ì²˜ë¦¬ ì†ë„: {stats['processed']/elapsed_time:.1f}ê°œ/ì´ˆ")
    print("=" * 60)

if __name__ == "__main__":
    main()