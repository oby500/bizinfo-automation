#!/usr/bin/env python3
"""
ê¸°ì—…ë§ˆë‹¹ ì „ì²´ ê³µê³  ì²¨ë¶€íŒŒì¼ ì •ë³´ í¬ë¡¤ë§ - ì†ë„ ê°•í™” ë²„ì „
- ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ë™ì‹œ ì²˜ë¦¬
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ë¡œ ì‹¤ì œ íƒ€ì… ê°ì§€
"""
import os
import sys
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, parse_qs, urlparse
import time
import re
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import queue
from supabase import create_client

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(threadName)s] - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'bizinfo_fast_crawler_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

# ì„±ëŠ¥ ì„¤ì •
MAX_WORKERS = 10  # ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜
BATCH_SIZE = 100  # ë°°ì¹˜ í¬ê¸°
REQUEST_TIMEOUT = 10  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
RETRY_COUNT = 2  # ì¬ì‹œë„ íšŸìˆ˜

# ì„¸ì…˜ í’€ ê´€ë¦¬
session_pool = queue.Queue(maxsize=MAX_WORKERS)
for _ in range(MAX_WORKERS):
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Connection': 'keep-alive'
    })
    session_pool.put(session)

# í†µê³„ ê´€ë¦¬
stats_lock = Lock()
stats = {
    'processed': 0,
    'with_attachments': 0,
    'with_hashtags': 0,
    'failed': 0,
    'total': 0
}

def get_session():
    """ì„¸ì…˜ í’€ì—ì„œ ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°"""
    return session_pool.get()

def return_session(session):
    """ì„¸ì…˜ í’€ì— ì„¸ì…˜ ë°˜í™˜"""
    session_pool.put(session)

def get_file_type_by_signature(url, session=None):
    """íŒŒì¼ì˜ ì²˜ìŒ ëª‡ ë°”ì´íŠ¸ë¥¼ ì½ì–´ ì‹¤ì œ íƒ€ì… íŒë‹¨"""
    if session is None:
        session = requests.Session()
    
    try:
        # íŒŒì¼ì˜ ì²˜ìŒ ë¶€ë¶„ë§Œ ë‹¤ìš´ë¡œë“œ
        headers = {'Range': 'bytes=0-2048'}
        response = session.get(url, headers=headers, timeout=10, stream=True)
        
        if response.status_code in [200, 206]:
            content = response.content[:2048]
        else:
            return 'UNKNOWN'
        
        # íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ë¡œ íƒ€ì… íŒë‹¨
        if len(content) >= 4:
            # PDF
            if content[:4] == b'%PDF':
                return 'PDF'
            # ZIP (DOCX, XLSX, PPTXë„ ZIP ê¸°ë°˜)
            elif content[:2] == b'PK':
                # ë” ë§ì´ ì½ì–´ì„œ Office íŒŒì¼ êµ¬ë¶„
                if b'word/' in content:
                    return 'DOCX'
                elif b'xl/' in content:
                    return 'XLSX'
                elif b'ppt/' in content:
                    return 'PPTX'
                else:
                    return 'ZIP'
            # MS Office 97-2003
            elif content[:8] == b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1':
                # í™•ì¥ìë¡œ êµ¬ë¶„ í•„ìš”
                if 'Content-Disposition' in response.headers:
                    disposition = response.headers['Content-Disposition'].lower()
                    if '.xls' in disposition:
                        return 'XLS'
                    elif '.doc' in disposition:
                        return 'DOC'
                    elif '.ppt' in disposition:
                        return 'PPT'
                return 'DOC'  # ê¸°ë³¸ê°’
            # HWP
            elif content[:4] == b'\xd0\xcf\x11\xe0' or b'HWP Document' in content[:100]:
                return 'HWP'
            # JPEG
            elif content[:3] == b'\xff\xd8\xff':
                return 'JPG'
            # PNG
            elif content[:8] == b'\x89PNG\r\n\x1a\n':
                return 'PNG'
            # GIF
            elif content[:6] in [b'GIF87a', b'GIF89a']:
                return 'GIF'
            # RTF
            elif content[:5] == b'{\\rtf':
                return 'RTF'
            # HTML (ì˜ëª»ëœ ì‘ë‹µ)
            elif b'<html' in content[:100].lower() or b'<!doctype' in content[:100].lower():
                return 'HTML'  # ë‚˜ì¤‘ì— HWPë¡œ ë³€ê²½ ì˜ˆì •
        
        return 'UNKNOWN'
        
    except Exception as e:
        logging.debug(f"íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ ì‹¤íŒ¨: {str(e)[:30]}")
        return 'UNKNOWN'

def extract_file_type_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ íƒ€ì… íŒíŠ¸ ì¶”ì¶œ"""
    if not text:
        return 'UNKNOWN'
    
    text_lower = text.lower()
    
    # ëª…í™•í•œ í™•ì¥ìê°€ ìˆëŠ” ê²½ìš°
    if '.hwp' in text_lower or '.hwpx' in text_lower:
        return 'HWP'
    elif '.pdf' in text_lower:
        return 'PDF'
    elif '.docx' in text_lower:
        return 'DOCX'
    elif '.doc' in text_lower:
        return 'DOC'
    elif '.xlsx' in text_lower:
        return 'XLSX'
    elif '.xls' in text_lower:
        return 'XLS'
    elif '.pptx' in text_lower:
        return 'PPTX'
    elif '.ppt' in text_lower:
        return 'PPT'
    elif '.zip' in text_lower:
        return 'ZIP'
    elif any(ext in text_lower for ext in ['.jpg', '.jpeg', '.png', '.gif']):
        return 'IMAGE'
    
    # í…ìŠ¤íŠ¸ íŒíŠ¸
    elif 'í•œê¸€' in text_lower or 'hwp' in text_lower:
        return 'HWP'
    elif 'pdf' in text_lower:
        return 'PDF'
    elif 'word' in text_lower or 'ì›Œë“œ' in text_lower:
        return 'DOCX'
    elif 'excel' in text_lower or 'ì—‘ì…€' in text_lower:
        return 'XLSX'
    elif 'powerpoint' in text_lower or 'íŒŒì›Œí¬ì¸íŠ¸' in text_lower:
        return 'PPT'
    elif 'ì–‘ì‹' in text_lower or 'ì„œì‹' in text_lower or 'ì‹ ì²­ì„œ' in text_lower:
        return 'HWP'  # í•œêµ­ ê³µê³µê¸°ê´€ ì–‘ì‹ì€ ëŒ€ë¶€ë¶„ HWP
    elif 'ë¶™ì„' in text_lower or 'ì²¨ë¶€' in text_lower:
        return 'HWP'  # í•œêµ­ ê³µê³µê¸°ê´€ ê¸°ë³¸
    
    return 'UNKNOWN'

def clean_filename(text):
    """íŒŒì¼ëª… ì •ë¦¬"""
    if not text:
        return None
    
    # íŒŒì¼ëª… íŒ¨í„´ ë§¤ì¹­
    patterns = [
        r'([^\/\\:*?"<>|\n\r\t]+\.(?:hwp|hwpx|pdf|doc|docx|xls|xlsx|ppt|pptx|zip|jpg|jpeg|png|gif|txt|rtf))\b',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            filename = match.group(1).strip()
            filename = re.sub(r'^(ì²¨ë¶€íŒŒì¼\s*|ë‹¤ìš´ë¡œë“œ\s*)', '', filename)
            filename = re.sub(r'\s*(ë‹¤ìš´ë¡œë“œ|ì²¨ë¶€íŒŒì¼)\s*$', '', filename)
            return filename
    
    return None

def process_single_announcement(data):
    """ë‹¨ì¼ ê³µê³  ì²˜ë¦¬ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
    pblanc_id = data['pblanc_id']
    pblanc_nm = data['pblanc_nm']
    dtl_url = data.get('dtl_url')
    
    if not dtl_url:
        return pblanc_id, [], [], "NO_URL"
    
    session = get_session()
    try:
        # ì¬ì‹œë„ ë¡œì§
        for attempt in range(RETRY_COUNT):
            try:
                response = session.get(dtl_url, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                break
            except Exception as e:
                if attempt == RETRY_COUNT - 1:
                    raise e
                time.sleep(1)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
        attachments = []
        unique_files = {}
        
        # ëª¨ë“  ë§í¬ì—ì„œ ì²¨ë¶€íŒŒì¼ íŒ¨í„´ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # atchFileId íŒ¨í„´ ì°¾ê¸°
            if 'atchFileId=' in href:
                # URL íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                atch_file_id = href.split('atchFileId=')[1].split('&')[0]
                file_sn = '0'
                if 'fileSn=' in href:
                    file_sn = href.split('fileSn=')[1].split('&')[0]
                
                unique_key = f"{atch_file_id}_{file_sn}"
                
                if unique_key not in unique_files:
                    # ì§ì ‘ URL êµ¬ì„±
                    direct_url = f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={atch_file_id}&fileSn={file_sn}"
                    
                    # íŒŒì¼ íƒ€ì… ê°ì§€ - 3ë‹¨ê³„ ì „ëµ
                    # 1. í…ìŠ¤íŠ¸ì—ì„œ íŒíŠ¸ ì°¾ê¸°
                    file_type = extract_file_type_from_text(text)
                    
                    # 2. UNKNOWNì´ë©´ íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ë¡œ í™•ì¸
                    if file_type == 'UNKNOWN':
                        file_type = get_file_type_by_signature(direct_url, session)
                        
                        # 3. HTMLì´ë©´ HWPë¡œ ê°€ì • (í•œêµ­ ê³µê³µê¸°ê´€ íŠ¹ì„±)
                        if file_type == 'HTML':
                            # í…ìŠ¤íŠ¸ íŒíŠ¸ë¡œ ë‹¤ì‹œ í™•ì¸
                            if any(keyword in text for keyword in ['ì–‘ì‹', 'ì„œì‹', 'ì‹ ì²­', 'ê³„íš', 'ë¶™ì„']):
                                file_type = 'HWP'
                            else:
                                file_type = 'HWP'  # ê¸°ë³¸ê°’ HWP
                    
                    display_filename = clean_filename(text) or text
                    
                    attachment = {
                        'url': direct_url,
                        'type': file_type,
                        'safe_filename': f"{pblanc_id}_{len(attachments)+1:02d}.{file_type.lower()}",
                        'display_filename': display_filename,
                        'original_filename': display_filename,
                        'text': text,
                        'params': {
                            'atchFileId': atch_file_id,
                            'fileSn': file_sn
                        }
                    }
                    
                    unique_files[unique_key] = attachment
                    attachments.append(attachment)
        
        # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
        hashtags = []
        tag_list = soup.find('ul', class_='tag_ul_list')
        if tag_list:
            tag_items = tag_list.find_all('li', class_=re.compile(r'tag_li_list\d'))
            for item in tag_items:
                link = item.find('a')
                if link:
                    tag_text = link.get_text(strip=True)
                    if tag_text and tag_text not in hashtags:
                        hashtags.append(tag_text)
        
        return pblanc_id, attachments, hashtags, "SUCCESS"
        
    except requests.exceptions.Timeout:
        return pblanc_id, [], [], "TIMEOUT"
    except Exception as e:
        logging.debug(f"í¬ë¡¤ë§ ì˜¤ë¥˜ ({pblanc_id}): {e}")
        return pblanc_id, [], [], f"ERROR: {str(e)[:50]}"
    finally:
        return_session(session)

def update_batch_to_db(supabase, results):
    """ë°°ì¹˜ ê²°ê³¼ë¥¼ DBì— ì—…ë°ì´íŠ¸"""
    try:
        for pblanc_id, attachments, hashtags, status in results:
            if status == "SUCCESS" or (status == "NO_URL"):
                update_data = {
                    'attachment_urls': attachments,
                    'attachment_processing_status': {
                        'processed': True,
                        'count': len(attachments),
                        'hashtag_count': len(hashtags),
                        'processed_at': datetime.now().isoformat(),
                        'status': status
                    }
                }
                
                if hashtags:
                    update_data['hash_tag'] = ', '.join(hashtags)
                
                # Supabase ì—…ë°ì´íŠ¸
                supabase.table('bizinfo_complete').update(
                    update_data
                ).eq('pblanc_id', pblanc_id).execute()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                with stats_lock:
                    stats['processed'] += 1
                    if attachments:
                        stats['with_attachments'] += 1
                        # íŒŒì¼ íƒ€ì… ë¡œê¹…
                        type_counts = {}
                        for att in attachments:
                            file_type = att.get('type', 'UNKNOWN')
                            type_counts[file_type] = type_counts.get(file_type, 0) + 1
                        if type_counts:
                            logging.info(f"  íŒŒì¼ íƒ€ì…: {type_counts}")
                    if hashtags:
                        stats['with_hashtags'] += 1
            else:
                with stats_lock:
                    stats['failed'] += 1
        
        return True
        
    except Exception as e:
        logging.error(f"DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return False

def process_batch_parallel(batch):
    """ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # ëª¨ë“  ì‘ì—… ì œì¶œ
        future_to_data = {
            executor.submit(process_single_announcement, data): data 
            for data in batch
        }
        
        # ì™„ë£Œëœ ì‘ì—… ìˆ˜ì§‘
        for future in as_completed(future_to_data):
            data = future_to_data[future]
            try:
                result = future.result(timeout=REQUEST_TIMEOUT * 2)
                results.append(result)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                pblanc_id = result[0]
                attachments = result[1]
                hashtags = result[2]
                status = result[3]
                
                with stats_lock:
                    current = stats['processed'] + stats['failed'] + 1
                    total = stats['total']
                
                if status == "SUCCESS":
                    logging.info(f"[{current}/{total}] {pblanc_id}: ì²¨ë¶€ {len(attachments)}ê°œ, íƒœê·¸ {len(hashtags)}ê°œ")
                elif status == "NO_URL":
                    logging.info(f"[{current}/{total}] {pblanc_id}: URL ì—†ìŒ")
                else:
                    logging.warning(f"[{current}/{total}] {pblanc_id}: {status}")
                    
            except Exception as e:
                logging.error(f"ì‘ì—… ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                results.append((data['pblanc_id'], [], [], f"EXCEPTION: {str(e)[:50]}"))
    
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print(" ê¸°ì—…ë§ˆë‹¹ ì²¨ë¶€íŒŒì¼ ê³ ì† í¬ë¡¤ë§ (íŒŒì¼ íƒ€ì… ìë™ ê°ì§€)")
    print("=" * 60)
    print(f"ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ: {MAX_WORKERS}ê°œ")
    print(f"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}ê°œ")
    print("íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ë¡œ ì‹¤ì œ íƒ€ì… ê°ì§€")
    print("=" * 60)
    
    # Supabase ì—°ê²°
    supabase_url = os.environ.get('SUPABASE_URL')
    supabase_key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
    
    if not supabase_url or not supabase_key:
        print("âŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("SUPABASE_URLê³¼ SUPABASE_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)
    
    supabase = create_client(supabase_url, supabase_key)
    logging.info("âœ… Supabase ì—°ê²° ì„±ê³µ")
    
    # ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ
    print("\n1. ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ ì¤‘...")
    try:
        # attachment_urlsê°€ nullì´ê±°ë‚˜ ë¹ˆ ë°°ì—´ì¸ ë°ì´í„° ìš°ì„  ì²˜ë¦¬
        response = supabase.table('bizinfo_complete').select(
            'id', 'pblanc_id', 'pblanc_nm', 'dtl_url'
        ).or_(
            'attachment_urls.is.null',
            'attachment_urls.eq.[]'
        ).execute()
        
        unprocessed = response.data
        
        # ì „ì²´ ë°ì´í„°ë„ ê°€ì ¸ì˜¤ê¸° (ì¬ì²˜ë¦¬ìš©)
        response_all = supabase.table('bizinfo_complete').select(
            'id', 'pblanc_id', 'pblanc_nm', 'dtl_url'
        ).execute()
        
        all_data = response_all.data
        
        print(f"âœ… ì „ì²´ ë°ì´í„°: {len(all_data)}ê°œ")
        print(f"âœ… ë¯¸ì²˜ë¦¬ ë°ì´í„°: {len(unprocessed)}ê°œ")
        
        # ë¯¸ì²˜ë¦¬ ë°ì´í„° ìš°ì„ , ê·¸ ë‹¤ìŒ ì „ì²´ ì¬ì²˜ë¦¬
        targets = unprocessed + [d for d in all_data if d not in unprocessed]
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    if not targets:
        print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í†µê³„ ì´ˆê¸°í™”
    stats['total'] = len(targets)
    
    print(f"\n2. í¬ë¡¤ë§ ì‹œì‘ (ì´ {len(targets)}ê°œ)")
    print("-" * 60)
    
    start_time = time.time()
    
    try:
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(targets), BATCH_SIZE):
            batch = targets[i:i+BATCH_SIZE]
            batch_num = (i // BATCH_SIZE) + 1
            total_batches = (len(targets) + BATCH_SIZE - 1) // BATCH_SIZE
            
            logging.info(f"\në°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬
            results = process_batch_parallel(batch)
            
            # DB ì—…ë°ì´íŠ¸
            if results:
                update_batch_to_db(supabase, results)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            with stats_lock:
                print(f"\nì§„í–‰ ìƒí™©: {stats['processed']}/{stats['total']} ì™„ë£Œ")
                print(f"  - ì²¨ë¶€íŒŒì¼ ìˆìŒ: {stats['with_attachments']}ê°œ")
                print(f"  - í•´ì‹œíƒœê·¸ ìˆìŒ: {stats['with_hashtags']}ê°œ")
                print(f"  - ì‹¤íŒ¨: {stats['failed']}ê°œ")
            
            # ë‹¤ìŒ ë°°ì¹˜ ì „ ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i + BATCH_SIZE < len(targets):
                time.sleep(2)
                
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logging.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    
    # ìµœì¢… ê²°ê³¼
    elapsed_time = time.time() - start_time
    
    print("\n" + "=" * 60)
    print(" í¬ë¡¤ë§ ì™„ë£Œ")
    print("=" * 60)
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {stats['processed']}ê°œ")
    print(f"ğŸ“ ì²¨ë¶€íŒŒì¼ ìˆìŒ: {stats['with_attachments']}ê°œ")
    print(f"ğŸ·ï¸ í•´ì‹œíƒœê·¸ ìˆìŒ: {stats['with_hashtags']}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {stats['failed']}ê°œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    print(f"ğŸ“Š ì²˜ë¦¬ ì†ë„: {stats['processed']/elapsed_time:.1f}ê°œ/ì´ˆ")
    print("=" * 60)

if __name__ == "__main__":
    main()
