#!/usr/bin/env python3
"""
BizInfo ì²¨ë¶€íŒŒì¼ URL ìˆ˜ì§‘ - URLë§Œ ìˆ˜ì§‘ (K-Startup ë°©ì‹ê³¼ ë™ì¼)
- ì²¨ë¶€íŒŒì¼ URLë§Œ ìˆ˜ì§‘
- íŒŒì¼ëª…ê³¼ íƒ€ì… ì •ë³´ëŠ” ë‹¤ìš´ë¡œë“œ ì‹œ HTTP í—¤ë”ì—ì„œ ì¶”ì¶œ
"""
import sys
sys.stdout.reconfigure(encoding='utf-8')
import os
import requests
from bs4 import BeautifulSoup
import re
from supabase import create_client
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from urllib.parse import urljoin

load_dotenv()

# Supabase ì„¤ì •
url = os.environ.get('SUPABASE_URL')
key = os.environ.get('SUPABASE_SERVICE_KEY')
supabase = create_client(url, key)

lock = threading.Lock()
progress = {
    'success': 0, 
    'error': 0, 
    'total': 0,
    'new_files': 0
}

session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': '*/*',
    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
    'Referer': 'https://www.bizinfo.go.kr/'
})

def extract_attachment_urls_only(detail_url):
    """BizInfo ì²¨ë¶€íŒŒì¼ URLë§Œ ì¶”ì¶œ - ìˆœìˆ˜ URLë§Œ"""
    all_urls = []
    
    try:
        response = session.get(detail_url, timeout=15)
        if response.status_code != 200:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. onclick íŒ¨í„´ë“¤ (fnFileDown, fileLoad, fileBlank)
        onclick_links = soup.find_all('a', onclick=True)
        
        for link in onclick_links:
            onclick = link.get('onclick', '')
            
            # fnFileDown íŒ¨í„´: fnFileDown('íŒŒì¼ID')
            if 'fnFileDown' in onclick:
                match = re.search(r"fnFileDown\('([^']+)'", onclick)
                if match:
                    file_id = match.group(1)
                    # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URL í˜•ì‹
                    url = f"https://www.bizinfo.go.kr/webapp/download.do?file_id={file_id}"
                    all_urls.append({'url': url})
            
            # fileLoad/fileBlank íŒ¨í„´: fileLoad('/path/to/file.pdf', ...)
            elif 'fileLoad' in onclick or 'fileBlank' in onclick:
                # ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ê°€ íŒŒì¼ ê²½ë¡œ
                match = re.search(r"(fileLoad|fileBlank)\s*\(\s*'([^']+)'", onclick)
                if match:
                    file_path = match.group(2)
                    # ê²½ë¡œê°€ /ë¡œ ì‹œì‘í•˜ë©´ ê·¸ëŒ€ë¡œ, ì•„ë‹ˆë©´ /webapp/upload/ ì¶”ê°€
                    if file_path.startswith('/'):
                        url = f"https://www.bizinfo.go.kr{file_path}"
                    else:
                        url = f"https://www.bizinfo.go.kr/webapp/upload/{file_path}"
                    all_urls.append({'url': url})
                else:
                    # ë¬¸ìì—´ ì¡°í•© íŒ¨í„´ ì²˜ë¦¬: '/path' + '/' + 'filename.pdf'
                    parts = re.findall(r"'([^']+)'", onclick)
                    if parts and len(parts) >= 2:
                        # íŒŒì¼ ê²½ë¡œ ì¡°í•©
                        file_path = ''.join(parts[:3] if len(parts) >= 3 else parts)
                        if file_path:
                            url = f"https://www.bizinfo.go.kr{file_path}"
                            all_urls.append({'url': url})
        
        # 2. href ì§ì ‘ ë§í¬ íŒ¨í„´
        href_patterns = [
            r'/webapp/download\.do',
            r'/webapp/upload/',
            r'/down\.jsp',
            r'/download/',
            r'\.hwp$',
            r'\.pdf$',
            r'\.zip$',
            r'\.doc[x]?$',
            r'\.xls[x]?$'
        ]
        
        for pattern in href_patterns:
            links = soup.find_all('a', href=re.compile(pattern, re.IGNORECASE))
            for link in links:
                href = link.get('href', '')
                if href and href != '#':
                    if href.startswith('/'):
                        url = f"https://www.bizinfo.go.kr{href}"
                    elif href.startswith('http'):
                        url = href
                    else:
                        url = f"https://www.bizinfo.go.kr/{href}"
                    all_urls.append({'url': url})
        
        # 3. class="fileDown" ë§í¬ë“¤
        file_down_links = soup.find_all('a', class_='fileDown')
        for link in file_down_links:
            href = link.get('href', '')
            if href and href != '#' and href != 'javascript:void(0);':
                if href.startswith('/'):
                    url = f"https://www.bizinfo.go.kr{href}"
                elif href.startswith('http'):
                    url = href
                else:
                    continue
                all_urls.append({'url': url})
    
    except Exception as e:
        print(f"    ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}")
        return []
    
    # ì¤‘ë³µ ì œê±°
    seen_urls = set()
    unique_urls = []
    for item in all_urls:
        if item['url'] not in seen_urls:
            seen_urls.add(item['url'])
            unique_urls.append(item)
    
    return unique_urls

def process_record(record):
    """ë ˆì½”ë“œ ì²˜ë¦¬ - URLë§Œ ìˆ˜ì§‘"""
    try:
        pblanc_id = record['pblanc_id']
        title = record.get('pblanc_nm', '')
        detail_url = record.get('detail_url') or record.get('dtl_url')
        
        if not detail_url:
            return False
        
        print(f"ì²˜ë¦¬ ì¤‘: {pblanc_id} - {title[:50]}...")
        
        # ì²¨ë¶€íŒŒì¼ URLë§Œ ì¶”ì¶œ
        attachments = extract_attachment_urls_only(detail_url)
        
        if attachments:
            # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ - URLë§Œ ì €ì¥
            result = supabase.table('bizinfo_complete')\
                .update({
                    'attachment_urls': attachments
                })\
                .eq('pblanc_id', pblanc_id)\
                .execute()
            
            if result.data:
                with lock:
                    progress['success'] += 1
                    progress['new_files'] += len(attachments)
                print(f"  âœ… {len(attachments)}ê°œ URL ìˆ˜ì§‘ ì™„ë£Œ")
                return True
        else:
            # ì²¨ë¶€íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°°ì—´ë¡œ ì €ì¥
            result = supabase.table('bizinfo_complete')\
                .update({
                    'attachment_urls': []
                })\
                .eq('pblanc_id', pblanc_id)\
                .execute()
            
            if result.data:
                with lock:
                    progress['success'] += 1
                print(f"  ğŸ“ ì²¨ë¶€íŒŒì¼ ì—†ìŒ (ë¹ˆ ë°°ì—´ ì €ì¥)")
                return True
        
        with lock:
            progress['error'] += 1
        return False
        
    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
        with lock:
            progress['error'] += 1
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*70)
    print("ğŸ“ BizInfo ì²¨ë¶€íŒŒì¼ URL ìˆ˜ì§‘ (ìˆœìˆ˜ URLë§Œ)")
    print("="*70)
    
    # ì²˜ë¦¬ ì œí•œ í™•ì¸ - ê¸°ë³¸ê°’ 200ê°œ
    processing_limit = int(os.environ.get('PROCESSING_LIMIT', '200'))
    
    # ì „ì²´ ë°ì´í„° ë¡œë“œ
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    all_data = []
    offset = 0
    page_size = 1000
    
    while True:
        batch = supabase.table('bizinfo_complete')\
            .select('pblanc_id, pblanc_nm, detail_url, dtl_url, attachment_urls, created_at')\
            .range(offset, offset + page_size - 1)\
            .execute()
        
        if not batch.data:
            break
            
        all_data.extend(batch.data)
        print(f"  ë¡œë”©: {len(all_data)}ê°œ...")
        
        if len(batch.data) < page_size:
            break
        offset += page_size
    
    print(f"ğŸ“Œ ì „ì²´ ë°ì´í„°: {len(all_data)}ê°œ")
    
    needs_processing = []
    
    for record in all_data:
        # URLì´ ìˆëŠ”ì§€ í™•ì¸
        detail_url = record.get('detail_url') or record.get('dtl_url')
        if not detail_url:
            continue
        
        # ì²¨ë¶€íŒŒì¼ ì •ë³´ í™•ì¸
        attachment_urls = record.get('attachment_urls')
        
        # NULLì´ê±°ë‚˜ ë¹ˆ ë°°ì—´ì¸ ê²½ìš° í•­ìƒ ì¬ì²˜ë¦¬
        if attachment_urls is None or attachment_urls == []:
            needs_processing.append(record)
    
    # ì²˜ë¦¬ ì œí•œ ì ìš© (ìµœì‹  ë°ì´í„° ìš°ì„ )
    if processing_limit > 0 and len(needs_processing) > processing_limit:
        # ìµœì‹  ë°ì´í„°ë¶€í„° ì²˜ë¦¬í•˜ë„ë¡ ì •ë ¬
        needs_processing.sort(key=lambda x: x.get('created_at', ''), reverse=True)
        needs_processing = needs_processing[:processing_limit]
        print(f"ğŸ“Œ ì œí•œ ëª¨ë“œ: ìµœëŒ€ {processing_limit}ê°œë§Œ ì²˜ë¦¬ (ìµœì‹  ë°ì´í„° ìš°ì„ )")
    
    progress['total'] = len(needs_processing)
    
    print(f"âœ… ê²€í†  ëŒ€ìƒ: {len(all_data)}ê°œ")
    print(f"ğŸ“ ì²˜ë¦¬ í•„ìš”: {progress['total']}ê°œ")
    
    if progress['total'] == 0:
        print("ğŸ‰ ëª¨ë“  ë ˆì½”ë“œê°€ ì´ë¯¸ ì •ìƒ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return
    
    print(f"ğŸ”¥ {progress['total']}ê°œ ì²˜ë¦¬ ì‹œì‘ (20 workers)...\n")
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(process_record, record): record for record in needs_processing}
        
        for i, future in enumerate(as_completed(futures), 1):
            try:
                future.result()
                if i % 50 == 0:
                    print(f"ì§„í–‰: {i}/{progress['total']} | ì„±ê³µ: {progress['success']} | URL: {progress['new_files']}ê°œ")
            except:
                pass
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“Š BizInfo ì²¨ë¶€íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ")
    print("="*70)
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {progress['success']}/{progress['total']}")
    print(f"ğŸ“ ìˆ˜ì§‘ëœ URL: {progress['new_files']}ê°œ")
    print(f"ğŸ“ ì²¨ë¶€íŒŒì¼ ì—†ìŒ: ë¹ˆ ë°°ì—´ []ë¡œ ì €ì¥ë¨")
    print("\nğŸ”§ ê°œì„ ì‚¬í•­:")
    print("  - ìˆœìˆ˜ ë‹¤ìš´ë¡œë“œ URLë§Œ ì €ì¥")
    print("  - íƒ€ì…, íŒŒì¼ëª… ë“± ë¶ˆí•„ìš”í•œ ì •ë³´ ì „ë¶€ ì œê±°")
    print("  - K-Startupê³¼ ë™ì¼í•œ ë°©ì‹")
    print("  - ìµœì‹  ë°ì´í„° ìš°ì„  ì²˜ë¦¬")
    print("="*70)

if __name__ == "__main__":
    main()