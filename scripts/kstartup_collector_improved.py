#!/usr/bin/env python3
"""
K-Startup ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (URL ë¬¸ì œ í•´ê²°)
- JavaScript URLì„ ì‹¤ì œ URLë¡œ ë³€í™˜
- ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
"""
import os
import sys
import requests
import json
import re
from datetime import datetime
from urllib.parse import urljoin
from supabase import create_client, Client
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from bs4 import BeautifulSoup

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class KStartupCollectorFixed:
    def __init__(self):
        url = os.environ.get('SUPABASE_URL')
        key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
        
        if not url or not key:
            logging.error("í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        self.supabase: Client = create_client(url, key)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.api_base_url = "http://www.k-startup.go.kr/web/module/bizpbanc-list.do"
        logging.info("=== K-Startup ìˆ˜ì§‘ ì‹œì‘ (URL ë¬¸ì œ í•´ê²°) ===")
    
    def fix_detail_url(self, url_or_js):
        """JavaScript URLì„ ì‹¤ì œ URLë¡œ ë³€í™˜"""
        if not url_or_js:
            return None
            
        # javascript:go_view(174538); í˜•íƒœ ì²˜ë¦¬
        if 'go_view' in url_or_js:
            match = re.search(r'go_view\((\d+)\)', url_or_js)
            if match:
                pbancSn = match.group(1)
                return f"http://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do?schM=view&pbancSn={pbancSn}"
        
        # ì´ë¯¸ ì •ìƒ URLì¸ ê²½ìš°
        if url_or_js.startswith('http'):
            return url_or_js
        
        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš°
        if url_or_js.startswith('/'):
            return f"http://www.k-startup.go.kr{url_or_js}"
            
        return None
    
    def fetch_list_data(self, page_num=1):
        """ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì¡°íšŒ"""
        try:
            params = {
                'page': page_num,
                'pageSize': 100,
                'searchType': 'all',
                'searchPbancSttsCd': '01',  # ëª¨ì§‘ì¤‘
                'orderBy': 'recent'
            }
            
            response = self.session.post(self.api_base_url, json=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if 'resultList' in data:
                    logging.info(f"í˜ì´ì§€ {page_num}: {len(data['resultList'])}ê°œ ì¡°íšŒ")
                    return data['resultList']
            return []
        except Exception as e:
            logging.error(f"í˜ì´ì§€ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def fetch_detail_page(self, announcement):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
        try:
            # URL ìˆ˜ì •
            detail_url = self.fix_detail_url(announcement.get('detlPgUrl'))
            
            if not detail_url:
                # URLì´ ì—†ìœ¼ë©´ IDë¡œ ì§ì ‘ ìƒì„±
                pbancSn = announcement.get('bizPbancSn', '')
                if pbancSn:
                    detail_url = f"http://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do?schM=view&pbancSn={pbancSn}"
                else:
                    return announcement
            
            logging.debug(f"ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§: {detail_url}")
            
            response = self.session.get(detail_url, timeout=10)
            if response.status_code != 200:
                return announcement
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
            attachments = []
            file_idx = 1
            
            # ë°©ë²• 1: ì²¨ë¶€íŒŒì¼ ì˜ì—­ ì°¾ê¸°
            file_area = soup.find('div', class_=['file_area', 'attach_file', 'file_list'])
            if file_area:
                links = file_area.find_all('a', href=True)
                for link in links:
                    href = link['href']
                    link_text = link.get_text(strip=True)
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ
                    filename = link_text if link_text else f"attachment_{file_idx}"
                    
                    # í™•ì¥ì ì¶”ì¸¡
                    file_ext = 'pdf'  # ê¸°ë³¸ê°’
                    if '.hwp' in filename.lower() or 'í•œê¸€' in filename:
                        file_ext = 'hwp'
                    elif '.doc' in filename.lower() or 'ì›Œë“œ' in filename:
                        file_ext = 'docx'
                    elif '.xls' in filename.lower() or 'ì—‘ì…€' in filename:
                        file_ext = 'xlsx'
                    elif '.pdf' in filename.lower():
                        file_ext = 'pdf'
                    elif '.zip' in filename.lower():
                        file_ext = 'zip'
                    
                    # ë‹¤ìš´ë¡œë“œ URL ìƒì„±
                    if 'fileDownload' in href or 'download' in href:
                        if not href.startswith('http'):
                            download_url = f"http://www.k-startup.go.kr{href}"
                        else:
                            download_url = href
                    else:
                        download_url = urljoin(detail_url, href)
                    
                    attachments.append({
                        'url': download_url,
                        'type': file_ext.upper(),
                        'safe_filename': f"KS_{announcement.get('bizPbancSn')}_{file_idx:02d}.{file_ext}",
                        'display_filename': filename,
                        'original_filename': filename
                    })
                    file_idx += 1
            
            # ë°©ë²• 2: ëª¨ë“  ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
            if not attachments:
                download_links = soup.find_all('a', href=re.compile(r'(fileDownload|download|atchFile|\.pdf|\.hwp|\.doc|\.xls|\.zip)', re.I))
                for link in download_links[:5]:  # ìµœëŒ€ 5ê°œ
                    href = link['href']
                    link_text = link.get_text(strip=True)
                    
                    # í™•ì¥ì ì¶”ì¶œ
                    ext_match = re.search(r'\.([a-zA-Z]{3,4})(?:\?|$|#)', href)
                    if ext_match:
                        file_ext = ext_match.group(1).lower()
                    else:
                        file_ext = 'pdf'
                    
                    filename = link_text if link_text and link_text != 'ë‹¤ìš´ë¡œë“œ' else f"attachment_{file_idx}.{file_ext}"
                    
                    if not href.startswith('http'):
                        download_url = f"http://www.k-startup.go.kr{href}"
                    else:
                        download_url = href
                    
                    attachments.append({
                        'url': download_url,
                        'type': file_ext.upper(),
                        'safe_filename': f"KS_{announcement.get('bizPbancSn')}_{file_idx:02d}.{file_ext}",
                        'display_filename': filename,
                        'original_filename': filename
                    })
                    file_idx += 1
            
            if attachments:
                announcement['attachment_urls'] = attachments
                announcement['attachment_count'] = len(attachments)
                logging.info(f"  - ì²¨ë¶€íŒŒì¼ {len(attachments)}ê°œ ë°œê²¬")
            
            # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            info_table = soup.find('table', class_=['view_tbl', 'detail_table', 'tbl_view'])
            if info_table:
                for row in info_table.find_all('tr'):
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.get_text(strip=True)
                        value = td.get_text(strip=True)
                        
                        if 'ì§€ì›ëŒ€ìƒ' in key:
                            announcement['aply_trgt_ctnt'] = value
                        elif 'ì‹ ì²­ê¸°ê°„' in key or 'ì ‘ìˆ˜ê¸°ê°„' in key:
                            # ë‚ ì§œ ì¶”ì¶œ
                            dates = re.findall(r'\d{4}[-./]\d{1,2}[-./]\d{1,2}', value)
                            if dates:
                                announcement['pbanc_rcpt_bgng_dt'] = dates[0].replace('.', '-').replace('/', '-')
                                if len(dates) > 1:
                                    announcement['pbanc_rcpt_end_dt'] = dates[1].replace('.', '-').replace('/', '-')
            
            return announcement
            
        except Exception as e:
            logging.error(f"ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return announcement
    
    def generate_summary(self, announcement):
        """ìš”ì•½ ìƒì„±"""
        parts = []
        
        title = announcement.get('bizPbancNm', '')
        # ì“°ë ˆê¸° ì œëª© í•„í„°ë§
        if title and title not in ['ëª¨ì§‘ì¤‘', 'URLë³µì‚¬', 'í™ˆí˜ì´ì§€ ë°”ë¡œê°€ê¸°', 'ëª¨ì§‘ë§ˆê°', 'ê³ ê°ì„¼í„°', 'ë²•ë¥ ì§€ì›']:
            parts.append(f"ğŸ“‹ {title}")
        
        org = announcement.get('pbancNtrpNm', '')
        if org:
            parts.append(f"ğŸ¢ ì£¼ê´€: {org}")
        
        target = announcement.get('aply_trgt_ctnt', '')
        if target:
            parts.append(f"ğŸ‘¥ ëŒ€ìƒ: {target[:80]}")
        
        end_date = announcement.get('pbancRcptEndDt')
        if end_date:
            parts.append(f"ğŸ“… ë§ˆê°: {end_date}")
            try:
                end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                days_left = (end_dt - datetime.now()).days
                if days_left >= 0:
                    parts.append(f"â° D-{days_left}")
            except:
                pass
        
        attach_count = announcement.get('attachment_count', 0)
        if attach_count > 0:
            parts.append(f"ğŸ“ ì²¨ë¶€: {attach_count}ê°œ")
        
        return '\n'.join(parts) if parts else f"ğŸ“‹ {title}"
    
    def generate_hashtags(self, announcement):
        """í•´ì‹œíƒœê·¸ ìƒì„±"""
        title = announcement.get('bizPbancNm', '')
        content = announcement.get('pbancCtnt', '')
        
        text = (title + ' ' + content).lower()
        
        tags = []
        if 'ì°½ì—…' in text:
            tags.append('#ì°½ì—…')
        if 'ìŠ¤íƒ€íŠ¸ì—…' in text:
            tags.append('#ìŠ¤íƒ€íŠ¸ì—…')
        if 'r&d' in text.lower() or 'ì—°êµ¬' in text:
            tags.append('#ì—°êµ¬ê°œë°œ')
        if 'íˆ¬ì' in text:
            tags.append('#íˆ¬ì')
        if 'êµìœ¡' in text:
            tags.append('#êµìœ¡')
        if 'ë©˜í† ' in text:
            tags.append('#ë©˜í† ë§')
        
        if not tags:
            tags = ['#ì •ë¶€ì§€ì›ì‚¬ì—…']
        
        return ' '.join(tags[:5])
    
    def process_announcements(self, announcements):
        """ê³µê³  ì²˜ë¦¬"""
        processed = []
        
        # ì“°ë ˆê¸° ë°ì´í„° í•„í„°ë§
        valid_announcements = []
        for ann in announcements:
            title = ann.get('bizPbancNm', '')
            if title not in ['ëª¨ì§‘ì¤‘', 'URLë³µì‚¬', 'í™ˆí˜ì´ì§€ ë°”ë¡œê°€ê¸°', 'ëª¨ì§‘ë§ˆê°', 'ê³ ê°ì„¼í„°', 'ë²•ë¥ ì§€ì›', '']:
                valid_announcements.append(ann)
        
        logging.info(f"ìœ íš¨í•œ ê³µê³ : {len(valid_announcements)}ê°œ (ì „ì²´: {len(announcements)}ê°œ)")
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(self.fetch_detail_page, ann): ann 
                      for ann in valid_announcements}
            
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=15)
                    result['bsns_sumry'] = self.generate_summary(result)
                    result['hash_tag'] = self.generate_hashtags(result)
                    processed.append(result)
                except Exception as e:
                    logging.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return processed
    
    def save_to_database(self, announcements):
        """DB ì €ì¥"""
        if not announcements:
            return 0
        
        # ê¸°ì¡´ ID ì¡°íšŒ
        existing = self.supabase.table('kstartup_complete').select('announcement_id').execute()
        existing_ids = {item['announcement_id'] for item in existing.data} if existing.data else set()
        
        new_records = []
        for ann in announcements:
            announcement_id = f"KS_{ann.get('bizPbancSn')}"
            
            if announcement_id not in existing_ids:
                # detl_pg_url ìˆ˜ì •
                detail_url = self.fix_detail_url(ann.get('detlPgUrl'))
                
                record = {
                    'announcement_id': announcement_id,
                    'biz_pbanc_nm': ann.get('bizPbancNm', ''),
                    'pbanc_ctnt': ann.get('pbancCtnt', ''),
                    'aply_trgt_ctnt': ann.get('aply_trgt_ctnt', ''),
                    'pbanc_rcpt_bgng_dt': ann.get('pbancRcptBgngDt'),
                    'pbanc_rcpt_end_dt': ann.get('pbancRcptEndDt'),
                    'pbanc_ntrp_nm': ann.get('pbancNtrpNm', ''),
                    'detl_pg_url': detail_url or '',
                    'attachment_urls': ann.get('attachment_urls', []),
                    'attachment_count': ann.get('attachment_count', 0),
                    'bsns_sumry': ann.get('bsns_sumry', ''),
                    'hash_tag': ann.get('hash_tag', ''),
                    'created_at': datetime.now().isoformat()
                }
                new_records.append(record)
        
        # ë°°ì¹˜ ì €ì¥
        success_count = 0
        batch_size = 50
        for i in range(0, len(new_records), batch_size):
            batch = new_records[i:i+batch_size]
            try:
                result = self.supabase.table('kstartup_complete').insert(batch).execute()
                if result.data:
                    success_count += len(result.data)
                    logging.info(f"ì €ì¥ ì™„ë£Œ: {len(result.data)}ê°œ")
            except Exception as e:
                logging.error(f"ì €ì¥ ì˜¤ë¥˜: {e}")
        
        return success_count
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # 1. ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ
            all_announcements = []
            for page in range(1, 4):  # 3í˜ì´ì§€ë§Œ
                page_data = self.fetch_list_data(page)
                if not page_data:
                    break
                all_announcements.extend(page_data)
            
            logging.info(f"ì „ì²´ ì¡°íšŒ: {len(all_announcements)}ê°œ")
            
            if not all_announcements:
                logging.info("ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # 2. ìƒì„¸ ì •ë³´ ì²˜ë¦¬
            logging.info("ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘...")
            processed = self.process_announcements(all_announcements)
            
            # 3. DB ì €ì¥
            saved_count = self.save_to_database(processed)
            
            # 4. ê²°ê³¼
            elapsed = time.time() - start_time
            logging.info("\n" + "="*50)
            logging.info(f"âœ… ì‹ ê·œ ì €ì¥: {saved_count}ê°œ")
            logging.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
            logging.info("="*50)
            
            return True
            
        except Exception as e:
            logging.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False
        finally:
            self.session.close()

if __name__ == "__main__":
    collector = KStartupCollectorFixed()
    success = collector.run()
    sys.exit(0 if success else 1)
