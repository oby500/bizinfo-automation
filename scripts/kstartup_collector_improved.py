#!/usr/bin/env python3
"""
K-Startup ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (BizInfo ë°©ì‹ìœ¼ë¡œ ê°œì„ )
- ìƒì„¸ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
- ì •í™•í•œ íŒŒì¼ëª…ê³¼ í™•ì¥ì ìˆ˜ì§‘
- ìš”ì•½ í’ˆì§ˆ ê°œì„ 
"""
import os
import sys
import requests
import json
from datetime import datetime, timedelta
from urllib.parse import urljoin, parse_qs, urlparse
from supabase import create_client, Client
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from bs4 import BeautifulSoup
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

class KStartupCollectorImproved:
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # Supabase ì—°ê²°
        url = os.environ.get('SUPABASE_URL')
        key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
        
        if not url or not key:
            logging.error("í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        self.supabase: Client = create_client(url, key)
        
        # ì„¸ì…˜ ì¬ì‚¬ìš©
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Content-Type': 'application/json'
        })
        
        # API ì„¤ì •
        self.api_base_url = "http://www.k-startup.go.kr/web/module/bizpbanc-list.do"
        
        logging.info("=== K-Startup ê°œì„ ëœ ìˆ˜ì§‘ ì‹œì‘ (BizInfo ë°©ì‹) ===")
    
    def fetch_list_data(self, page_num=1, page_size=100):
        """ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì¡°íšŒ"""
        try:
            params = {
                'page': page_num,
                'pageSize': page_size,
                'searchType': 'all',
                'searchPbancSttsCd': '01',  # ëª¨ì§‘ì¤‘
                'orderBy': 'recent'
            }
            
            response = self.session.post(
                self.api_base_url,
                json=params,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'resultList' in data:
                    logging.info(f"  í˜ì´ì§€ {page_num}: {len(data['resultList'])}ê°œ ì¡°íšŒ")
                    return data['resultList']
            
            return []
            
        except Exception as e:
            logging.error(f"í˜ì´ì§€ {page_num} ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def fetch_detail_page(self, announcement):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ (BizInfo ë°©ì‹)"""
        try:
            # ìƒì„¸ í˜ì´ì§€ URL ìƒì„±
            detail_url = announcement.get('detlPgUrl', '')
            if not detail_url:
                # URLì´ ì—†ìœ¼ë©´ IDë¡œ ìƒì„±
                pbancSn = announcement.get('bizPbancSn', '')
                detail_url = f"http://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do?schM=view&pbancSn={pbancSn}"
            elif not detail_url.startswith('http'):
                detail_url = f"http://www.k-startup.go.kr{detail_url}"
            
            response = self.session.get(detail_url, timeout=10)
            if response.status_code != 200:
                return announcement
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
            attachments = []
            file_idx = 1
            
            # ë°©ë²• 1: file_bg í´ë˜ìŠ¤ì—ì„œ title ì†ì„± í™•ì¸
            file_elements = soup.find_all('span', class_='file_bg')
            for elem in file_elements:
                title = elem.get('title', '')
                if title:
                    # ì‹¤ì œ íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¶œ
                    file_ext = self.extract_extension(title)
                    
                    # ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
                    parent = elem.find_parent(['li', 'div', 'td'])
                    if parent:
                        link = parent.find('a', href=True)
                        if link:
                            href = link['href']
                            # ë‹¤ìš´ë¡œë“œ URL ì •ë¦¬
                            if 'fileDownload' in href:
                                download_url = self.build_download_url(href)
                            else:
                                download_url = urljoin(detail_url, href)
                            
                            attachments.append({
                                'url': download_url,
                                'type': file_ext.upper(),
                                'safe_filename': f"KS_{announcement.get('bizPbancSn', '')}_{file_idx:02d}.{file_ext}",
                                'display_filename': title,
                                'original_filename': title
                            })
                            file_idx += 1
            
            # ë°©ë²• 2: ë‹¤ìš´ë¡œë“œ ë§í¬ ì§ì ‘ ì°¾ê¸°
            if not attachments:
                download_links = soup.find_all('a', href=re.compile(r'fileDownload|download|atchFile'))
                for link in download_links:
                    href = link['href']
                    link_text = link.get_text(strip=True)
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
                    filename = link_text if link_text and not link_text == 'ë‹¤ìš´ë¡œë“œ' else f"attachment_{file_idx}"
                    file_ext = self.extract_extension(filename)
                    
                    # ë‹¤ìš´ë¡œë“œ URL ìƒì„±
                    download_url = self.build_download_url(href)
                    
                    attachments.append({
                        'url': download_url,
                        'type': file_ext.upper() if file_ext != 'unknown' else 'FILE',
                        'safe_filename': f"KS_{announcement.get('bizPbancSn', '')}_{file_idx:02d}.{file_ext}",
                        'display_filename': filename,
                        'original_filename': filename
                    })
                    file_idx += 1
            
            # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸
            if attachments:
                announcement['attachment_urls'] = attachments
                announcement['attachment_count'] = len(attachments)
            
            # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
            content_area = soup.find(['div', 'section'], class_=['content', 'detail', 'view_cont'])
            if content_area:
                announcement['pbanc_ctnt'] = content_area.get_text(strip=True)[:3000]
            
            # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            info_table = soup.find('table', class_=['view_tbl', 'detail_tbl'])
            if info_table:
                details = {}
                rows = info_table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.get_text(strip=True)
                        value = td.get_text(strip=True)
                        details[key] = value
                
                # ì£¼ìš” ì •ë³´ ë§¤í•‘
                if 'ì§€ì›ëŒ€ìƒ' in details:
                    announcement['aply_trgt_ctnt'] = details['ì§€ì›ëŒ€ìƒ']
                if 'ì‹ ì²­ê¸°ê°„' in details:
                    dates = self.extract_dates_from_text(details['ì‹ ì²­ê¸°ê°„'])
                    if dates:
                        announcement['pbanc_rcpt_bgng_dt'] = dates[0]
                        if len(dates) > 1:
                            announcement['pbanc_rcpt_end_dt'] = dates[1]
            
            return announcement
            
        except Exception as e:
            logging.error(f"ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return announcement
    
    def extract_extension(self, filename):
        """íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¶œ"""
        if not filename:
            return 'pdf'  # ê¸°ë³¸ê°’
        
        # ì¼ë°˜ì ì¸ í™•ì¥ì íŒ¨í„´
        ext_match = re.search(r'\.([a-zA-Z0-9]+)$', filename)
        if ext_match:
            ext = ext_match.group(1).lower()
            # ìœ íš¨í•œ í™•ì¥ì ëª©ë¡
            valid_exts = ['pdf', 'hwp', 'hwpx', 'doc', 'docx', 'xls', 'xlsx', 
                         'ppt', 'pptx', 'zip', 'jpg', 'jpeg', 'png', 'gif']
            if ext in valid_exts:
                return ext
        
        # í…ìŠ¤íŠ¸ì—ì„œ í™•ì¥ì ì¶”ì¸¡
        filename_lower = filename.lower()
        if 'í•œê¸€' in filename or 'hwp' in filename_lower:
            return 'hwp'
        elif 'pdf' in filename_lower:
            return 'pdf'
        elif 'ì—‘ì…€' in filename or 'excel' in filename_lower:
            return 'xlsx'
        elif 'ì›Œë“œ' in filename or 'word' in filename_lower:
            return 'docx'
        elif 'íŒŒì›Œí¬ì¸íŠ¸' in filename or 'ppt' in filename_lower:
            return 'pptx'
        
        return 'pdf'  # ê¸°ë³¸ê°’
    
    def build_download_url(self, href):
        """ë‹¤ìš´ë¡œë“œ URL ìƒì„±"""
        if href.startswith('http'):
            return href
        elif href.startswith('//'):
            return 'http:' + href
        elif 'fileDownload' in href:
            # K-Startup íŠ¹ìˆ˜ ë‹¤ìš´ë¡œë“œ URL
            return f"http://www.k-startup.go.kr{href}"
        else:
            return f"http://www.k-startup.go.kr/web/module/{href}"
    
    def extract_dates_from_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        dates = []
        
        # ë‚ ì§œ íŒ¨í„´ë“¤
        patterns = [
            r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})',
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                year, month, day = match
                date_str = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                dates.append(date_str)
        
        return dates
    
    def generate_summary(self, announcement):
        """ìš”ì•½ ìƒì„± (BizInfo ìŠ¤íƒ€ì¼)"""
        parts = []
        
        # ì œëª©
        title = announcement.get('biz_pbanc_nm', '')
        if title and title not in ['ëª¨ì§‘ì¤‘', 'URLë³µì‚¬', 'í™ˆí˜ì´ì§€ ë°”ë¡œê°€ê¸°']:
            parts.append(f"ğŸ“‹ {title}")
        
        # ì£¼ê´€ê¸°ê´€
        org = announcement.get('pbanc_ntrp_nm', '')
        if org:
            parts.append(f"ğŸ¢ ì£¼ê´€: {org}")
        
        # ì§€ì›ëŒ€ìƒ
        target = announcement.get('aply_trgt_ctnt', '')
        if target:
            target_text = target[:80] + "..." if len(target) > 80 else target
            parts.append(f"ğŸ‘¥ ëŒ€ìƒ: {target_text}")
        
        # ì‹ ì²­ê¸°ê°„
        end_date = announcement.get('pbanc_rcpt_end_dt')
        if end_date:
            parts.append(f"ğŸ“… ë§ˆê°: {end_date}")
            
            # D-day ê³„ì‚°
            try:
                end_datetime = datetime.strptime(end_date, '%Y-%m-%d')
                today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                days_left = (end_datetime - today).days
                if days_left >= 0:
                    parts.append(f"â° D-{days_left}")
            except:
                pass
        
        # ì²¨ë¶€íŒŒì¼
        attach_count = announcement.get('attachment_count', 0)
        if attach_count > 0:
            file_types = []
            for att in announcement.get('attachment_urls', []):
                file_type = att.get('type', 'FILE')
                if file_type not in file_types:
                    file_types.append(file_type)
            parts.append(f"ğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({attach_count}ê°œ)")
        
        return '\n'.join(parts) if parts else f"ğŸ“‹ {title}"
    
    def generate_hashtags(self, announcement):
        """í•´ì‹œíƒœê·¸ ìƒì„±"""
        tags = []
        
        title = announcement.get('biz_pbanc_nm', '')
        content = announcement.get('pbanc_ctnt', '')
        org = announcement.get('pbanc_ntrp_nm', '')
        
        text = (title + ' ' + content + ' ' + org).lower()
        
        # í‚¤ì›Œë“œ ë§¤í•‘
        keyword_map = {
            'ì°½ì—…': '#ì°½ì—…',
            'ìŠ¤íƒ€íŠ¸ì—…': '#ìŠ¤íƒ€íŠ¸ì—…',
            'R&D': '#ì—°êµ¬ê°œë°œ',
            'ê¸°ìˆ ': '#ê¸°ìˆ ê°œë°œ',
            'íˆ¬ì': '#íˆ¬ììœ ì¹˜',
            'ìˆ˜ì¶œ': '#ìˆ˜ì¶œì§€ì›',
            'ë§ˆì¼€íŒ…': '#ë§ˆì¼€íŒ…',
            'êµìœ¡': '#êµìœ¡',
            'ë©˜í† ë§': '#ë©˜í† ë§',
            'ì»¨ì„¤íŒ…': '#ì»¨ì„¤íŒ…',
            'ì‚¬ì—…í™”': '#ì‚¬ì—…í™”',
            'AI': '#ì¸ê³µì§€ëŠ¥',
            'ë¹…ë°ì´í„°': '#ë¹…ë°ì´í„°'
        }
        
        for keyword, tag in keyword_map.items():
            if keyword.lower() in text:
                if tag not in tags:
                    tags.append(tag)
        
        # ê¸°ë³¸ íƒœê·¸
        if not tags:
            tags = ['#ì •ë¶€ì§€ì›ì‚¬ì—…', '#KìŠ¤íƒ€íŠ¸ì—…']
        
        return ' '.join(tags[:5])
    
    def process_announcements(self, announcements):
        """ê³µê³  ì²˜ë¦¬ (ìƒì„¸ ì •ë³´ í¬í•¨)"""
        processed = []
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(self.fetch_detail_page, ann): ann 
                      for ann in announcements}
            
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=15)
                    
                    # ìš”ì•½ ìƒì„±
                    result['bsns_sumry'] = self.generate_summary(result)
                    
                    # í•´ì‹œíƒœê·¸ ìƒì„±
                    result['hash_tag'] = self.generate_hashtags(result)
                    
                    processed.append(result)
                    
                except Exception as e:
                    logging.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return processed
    
    def save_to_database(self, announcements):
        """DB ì €ì¥"""
        if not announcements:
            return 0
        
        # ê¸°ì¡´ ID ì¡°íšŒ
        existing_result = self.supabase.table('kstartup_complete').select('announcement_id').execute()
        existing_ids = {item['announcement_id'] for item in existing_result.data} if existing_result.data else set()
        
        new_records = []
        for ann in announcements:
            announcement_id = f"KS_{ann.get('bizPbancSn', '')}"
            
            if announcement_id not in existing_ids:
                record = {
                    'announcement_id': announcement_id,
                    'biz_pbanc_nm': ann.get('bizPbancNm', ''),
                    'pbanc_ctnt': ann.get('pbanc_ctnt', ''),
                    'aply_trgt_ctnt': ann.get('aply_trgt_ctnt', ''),
                    'pbanc_rcpt_bgng_dt': ann.get('pbancRcptBgngDt'),
                    'pbanc_rcpt_end_dt': ann.get('pbancRcptEndDt'),
                    'pbanc_ntrp_nm': ann.get('pbancNtrpNm', ''),
                    'detl_pg_url': ann.get('detlPgUrl', ''),
                    'attachment_urls': ann.get('attachment_urls', []),
                    'attachment_count': ann.get('attachment_count', 0),
                    'bsns_sumry': ann.get('bsns_sumry', ''),
                    'hash_tag': ann.get('hash_tag', ''),
                    'created_at': datetime.now().isoformat()
                }
                new_records.append(record)
        
        # ë°°ì¹˜ ì €ì¥
        success_count = 0
        batch_size = 50
        for i in range(0, len(new_records), batch_size):
            batch = new_records[i:i+batch_size]
            try:
                result = self.supabase.table('kstartup_complete').insert(batch).execute()
                if result.data:
                    success_count += len(result.data)
                    logging.info(f"  ë°°ì¹˜ ì €ì¥: {len(result.data)}ê°œ")
            except Exception as e:
                logging.error(f"ì €ì¥ ì˜¤ë¥˜: {e}")
        
        return success_count
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # 1. ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ (5í˜ì´ì§€, 500ê°œ)
            all_announcements = []
            for page in range(1, 6):
                page_data = self.fetch_list_data(page, 100)
                if not page_data:
                    break
                all_announcements.extend(page_data)
            
            logging.info(f"ğŸ“‹ ì „ì²´ ì¡°íšŒ: {len(all_announcements)}ê°œ")
            
            if not all_announcements:
                logging.info("ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # 2. ìƒì„¸ ì •ë³´ ì²˜ë¦¬ (ì²¨ë¶€íŒŒì¼ í¬í•¨)
            logging.info("ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘...")
            processed = self.process_announcements(all_announcements)
            
            # 3. DB ì €ì¥
            saved_count = self.save_to_database(processed)
            
            # 4. ê²°ê³¼ ì¶œë ¥
            elapsed = time.time() - start_time
            logging.info("\n" + "="*50)
            logging.info("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")
            logging.info(f"âœ… ì‹ ê·œ ì €ì¥: {saved_count}ê°œ")
            logging.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
            logging.info("="*50)
            
            return True
            
        except Exception as e:
            logging.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False
        finally:
            self.session.close()

if __name__ == "__main__":
    collector = KStartupCollectorImproved()
    success = collector.run()
    sys.exit(0 if success else 1)
