#!/usr/bin/env python3
"""
ìƒì„¸ ì²¨ë¶€íŒŒì¼ ê²€ì¦ ë° ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ë¬¸ì„œí™”
"""

import os
import json
from pathlib import Path
from dotenv import load_dotenv
from supabase import create_client
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import hashlib

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class DetailedAttachmentVerifier:
    def __init__(self):
        self.project_root = Path("E:\\gov-support-automation")
        self.downloads_dir = self.project_root / "downloads"
        
        # Supabase ì—°ê²°
        url = os.getenv('SUPABASE_URL')
        key = os.getenv('SUPABASE_SERVICE_KEY')
        self.supabase = create_client(url, key) if url and key else None
        
        # í†µê³„ ì´ˆê¸°í™”
        self.results = {
            'kstartup': defaultdict(int),
            'bizinfo': defaultdict(int)
        }
        
        self.missing_downloads = {
            'kstartup': [],
            'bizinfo': []
        }
        
        self.local_files_map = {
            'kstartup': {},
            'bizinfo': {}
        }
    
    def get_server_attachments(self, table_name: str):
        """ì„œë²„ì—ì„œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¡°íšŒ"""
        if not self.supabase:
            print("[ERROR] Supabase ì—°ê²° ì—†ìŒ")
            return []
        
        print(f"\n[INFO] {table_name} í…Œì´ë¸” ì¡°íšŒ ì¤‘...")
        all_records = []
        
        try:
            offset = 0
            limit = 1000
            
            while True:
                result = self.supabase.table(table_name)\
                    .select('id, attachment_urls')\
                    .not_.is_('attachment_urls', 'null')\
                    .range(offset, offset + limit - 1)\
                    .execute()
                
                if not result.data:
                    break
                    
                all_records.extend(result.data)
                
                if len(result.data) < limit:
                    break
                offset += limit
            
            print(f"  [OK] {len(all_records)}ê°œ ë ˆì½”ë“œ ì¡°íšŒ ì™„ë£Œ")
            return all_records
            
        except Exception as e:
            print(f"  [ERROR] ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def scan_local_files(self, source: str):
        """ë¡œì»¬ íŒŒì¼ ìŠ¤ìº”"""
        folder = self.downloads_dir / source
        if not folder.exists():
            print(f"  [ERROR] {folder} í´ë” ì—†ìŒ")
            return {}
        
        files_map = {}
        for file_path in folder.glob("*"):
            if file_path.is_file():
                # íŒŒì¼ëª…ì—ì„œ ID ì¶”ì¶œ
                filename = file_path.name
                if source == 'kstartup' and 'KS_' in filename:
                    # KS_174310_xxx.pdf í˜•ì‹ì—ì„œ 174310 ì¶”ì¶œ
                    parts = filename.split('_')
                    if len(parts) >= 2 and parts[1].isdigit():
                        record_id = int(parts[1])
                        if record_id not in files_map:
                            files_map[record_id] = []
                        files_map[record_id].append({
                            'filename': filename,
                            'path': str(file_path),
                            'size': file_path.stat().st_size
                        })
                elif source == 'bizinfo' and 'PBLN_' in filename:
                    # PBLN_123456_xxx.pdf í˜•ì‹ì—ì„œ 123456 ì¶”ì¶œ
                    parts = filename.split('_')
                    if len(parts) >= 2 and parts[1].isdigit():
                        record_id = int(parts[1])
                        if record_id not in files_map:
                            files_map[record_id] = []
                        files_map[record_id].append({
                            'filename': filename,
                            'path': str(file_path),
                            'size': file_path.stat().st_size
                        })
        
        return files_map
    
    def verify_source(self, source: str, table_name: str):
        """íŠ¹ì • ì†ŒìŠ¤ ê²€ì¦"""
        print(f"\n{'='*80}")
        print(f"[VERIFY] {source.upper()} ì²¨ë¶€íŒŒì¼ ê²€ì¦")
        print('='*80)
        
        # ì„œë²„ ë°ì´í„° ì¡°íšŒ
        server_records = self.get_server_attachments(table_name)
        
        # ë¡œì»¬ íŒŒì¼ ìŠ¤ìº”
        local_files = self.scan_local_files(source)
        self.local_files_map[source] = local_files
        
        # í†µê³„ ì´ˆê¸°í™”
        total_server_urls = 0
        matched_urls = 0
        missing_urls = 0
        records_with_attachments = 0
        records_fully_downloaded = 0
        records_partially_downloaded = 0
        records_not_downloaded = 0
        
        # ê° ë ˆì½”ë“œë³„ ê²€ì¦
        for record in server_records:
            record_id = record['id']
            attachment_urls = record.get('attachment_urls', [])
            
            if not attachment_urls:
                continue
            
            records_with_attachments += 1
            record_matched = 0
            record_total = 0
            
            # attachment_urls ì²˜ë¦¬
            if isinstance(attachment_urls, list):
                for url_item in attachment_urls:
                    if isinstance(url_item, dict):
                        # URLì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                        url = url_item.get('url', '')
                        if url:
                            record_total += 1
                            total_server_urls += 1
                            
                            # ë¡œì»¬ íŒŒì¼ê³¼ ë§¤ì¹­ í™•ì¸
                            if record_id in local_files:
                                # í•´ë‹¹ IDì˜ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                                record_matched += 1
                                matched_urls += 1
                            else:
                                missing_urls += 1
                                self.missing_downloads[source].append({
                                    'id': record_id,
                                    'url': url,
                                    'filename': url_item.get('original_filename', 'unknown')
                                })
                    elif isinstance(url_item, str):
                        # URLì´ ë¬¸ìì—´ì¸ ê²½ìš°
                        record_total += 1
                        total_server_urls += 1
                        
                        if record_id in local_files:
                            record_matched += 1
                            matched_urls += 1
                        else:
                            missing_urls += 1
                            self.missing_downloads[source].append({
                                'id': record_id,
                                'url': url_item,
                                'filename': 'unknown'
                            })
            
            # ë ˆì½”ë“œë³„ ë‹¤ìš´ë¡œë“œ ìƒíƒœ ë¶„ë¥˜
            if record_total > 0:
                if record_matched == record_total:
                    records_fully_downloaded += 1
                elif record_matched > 0:
                    records_partially_downloaded += 1
                else:
                    records_not_downloaded += 1
        
        # ë¡œì»¬ì—ë§Œ ìˆëŠ” íŒŒì¼ í™•ì¸
        server_record_ids = {r['id'] for r in server_records}
        extra_local_ids = set(local_files.keys()) - server_record_ids
        
        # í†µê³„ ì €ì¥
        self.results[source] = {
            'total_server_records': len(server_records),
            'records_with_attachments': records_with_attachments,
            'total_server_urls': total_server_urls,
            'matched_urls': matched_urls,
            'missing_urls': missing_urls,
            'records_fully_downloaded': records_fully_downloaded,
            'records_partially_downloaded': records_partially_downloaded,
            'records_not_downloaded': records_not_downloaded,
            'total_local_files': sum(len(files) for files in local_files.values()),
            'local_record_ids': len(local_files),
            'extra_local_ids': len(extra_local_ids)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n[RESULT] ê²€ì¦ ê²°ê³¼:")
        print(f"  ì„œë²„ ë ˆì½”ë“œ (ì²¨ë¶€íŒŒì¼ ìˆìŒ): {records_with_attachments:,}ê°œ")
        print(f"  ì„œë²„ ì´ URL ìˆ˜: {total_server_urls:,}ê°œ")
        print(f"  ë¡œì»¬ íŒŒì¼ ìˆ˜: {self.results[source]['total_local_files']:,}ê°œ")
        print(f"  ë¡œì»¬ ë ˆì½”ë“œ ID ìˆ˜: {self.results[source]['local_record_ids']:,}ê°œ")
        
        print(f"\n[STATUS] ë‹¤ìš´ë¡œë“œ ìƒíƒœ:")
        print(f"  [OK] ì™„ì „ ë‹¤ìš´ë¡œë“œ: {records_fully_downloaded:,}ê°œ ë ˆì½”ë“œ")
        print(f"  [WARN] ë¶€ë¶„ ë‹¤ìš´ë¡œë“œ: {records_partially_downloaded:,}ê°œ ë ˆì½”ë“œ")
        print(f"  [FAIL] ë¯¸ë‹¤ìš´ë¡œë“œ: {records_not_downloaded:,}ê°œ ë ˆì½”ë“œ")
        
        if total_server_urls > 0:
            match_rate = (matched_urls / total_server_urls) * 100
            print(f"\n[MATCH] ë§¤ì¹­ë¥ : {match_rate:.1f}% ({matched_urls:,}/{total_server_urls:,})")
        
        # ë¬¸ì œ ìƒ˜í”Œ ì¶œë ¥
        if self.missing_downloads[source]:
            print(f"\n[MISSING] ë¯¸ë‹¤ìš´ë¡œë“œ ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
            for item in self.missing_downloads[source][:5]:
                print(f"    ID: {item['id']}, íŒŒì¼: {item['filename']}")
        
        if extra_local_ids:
            print(f"\n[EXTRA] ì„œë²„ì— ì—†ëŠ” ë¡œì»¬ íŒŒì¼ ID (ìµœëŒ€ 5ê°œ):")
            for local_id in list(extra_local_ids)[:5]:
                files = local_files[local_id]
                print(f"    ID: {local_id}, íŒŒì¼ ìˆ˜: {len(files)}ê°œ")
    
    def generate_final_report(self):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*80)
        print("[REPORT] ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ")
        print("="*80)
        
        # ì „ì²´ í†µê³„
        total_server_urls = sum(r['total_server_urls'] for r in self.results.values())
        total_matched = sum(r['matched_urls'] for r in self.results.values())
        total_missing = sum(r['missing_urls'] for r in self.results.values())
        total_local_files = sum(r['total_local_files'] for r in self.results.values())
        
        print(f"\n[TOTAL] ì „ì²´ í†µê³„:")
        print(f"  ì„œë²„ ì´ URL: {total_server_urls:,}ê°œ")
        print(f"  ë¡œì»¬ ì´ íŒŒì¼: {total_local_files:,}ê°œ")
        print(f"  ë§¤ì¹­ ì„±ê³µ: {total_matched:,}ê°œ")
        print(f"  ë¯¸ë‹¤ìš´ë¡œë“œ: {total_missing:,}ê°œ")
        
        if total_server_urls > 0:
            overall_match_rate = (total_matched / total_server_urls) * 100
            print(f"  ì „ì²´ ë§¤ì¹­ë¥ : {overall_match_rate:.1f}%")
        
        # ê° ì†ŒìŠ¤ë³„ ìš”ì•½
        for source in ['kstartup', 'bizinfo']:
            if source in self.results:
                r = self.results[source]
                print(f"\n[{source.upper()}]:")
                print(f"  ì„œë²„ URL: {r['total_server_urls']:,}ê°œ")
                print(f"  ë¡œì»¬ íŒŒì¼: {r['total_local_files']:,}ê°œ")
                print(f"  ì™„ì „ ë‹¤ìš´ë¡œë“œ: {r['records_fully_downloaded']:,}ê°œ ë ˆì½”ë“œ")
                print(f"  ë¶€ë¶„ ë‹¤ìš´ë¡œë“œ: {r['records_partially_downloaded']:,}ê°œ ë ˆì½”ë“œ")
                print(f"  ë¯¸ë‹¤ìš´ë¡œë“œ: {r['records_not_downloaded']:,}ê°œ ë ˆì½”ë“œ")
        
        # JSON ë³´ê³ ì„œ ì €ì¥
        report_file = self.project_root / "attachment_verification_detailed.json"
        report_data = {
            'summary': {
                'total_server_urls': total_server_urls,
                'total_matched': total_matched,
                'total_missing': total_missing,
                'total_local_files': total_local_files,
                'overall_match_rate': overall_match_rate if total_server_urls > 0 else 0
            },
            'details': self.results,
            'missing_samples': {
                'kstartup': self.missing_downloads['kstartup'][:10],
                'bizinfo': self.missing_downloads['bizinfo'][:10]
            }
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n[SAVE] ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")
        
        return report_data
    
    def document_automation_scripts(self):
        """GitHub Actions ìë™í™”ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ë¬¸ì„œí™”"""
        print("\n" + "="*80)
        print("[DOC] ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ë¬¸ì„œí™”")
        print("="*80)
        
        automation_doc = """# ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡

## ğŸ¯ í•µì‹¬ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

### 1. ë©”ì¸ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
- **`scripts/complete_automation.py`** - ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìë™í™”
- **`scripts/perfect_automation.py`** - ì™„ë²½í•œ ë‹¤ìš´ë¡œë“œ ìë™í™”
- **`scripts/all_in_one_automation.py`** - ì˜¬ì¸ì› ìë™í™”

### 2. ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
- **`scripts/download_100_percent.py`** - 100% ë‹¤ìš´ë¡œë“œ ë³´ì¥
- **`perfect_attachment_downloader.py`** - ì™„ë²½í•œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë”
- **`complete_attachment_manager.py`** - ì²¨ë¶€íŒŒì¼ ê´€ë¦¬ì

### 3. íŒŒì¼ëª… ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- **`fix_broken_filenames_final.py`** - ê¹¨ì§„ íŒŒì¼ëª… ìˆ˜ì •
- **`complete_filename_fix.py`** - íŒŒì¼ëª… ì™„ì „ ìˆ˜ì •
- **`fix_html_entities.py`** - HTML ì—”í‹°í‹° ë””ì½”ë”©

### 4. ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- **`scripts/verify_attachments_complete.py`** - ì²¨ë¶€íŒŒì¼ ì™„ì „ì„± ê²€ì¦
- **`scripts/detailed_attachment_verification.py`** - ìƒì„¸ ê²€ì¦
- **`scripts/compare_server_local_attachments.py`** - ì„œë²„/ë¡œì»¬ ë¹„êµ

## ğŸ”„ GitHub Actions ì›Œí¬í”Œë¡œìš° êµ¬ì„±

```yaml
name: Attachment Processing Automation

on:
  schedule:
    - cron: '0 2 * * *'  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ ì‹¤í–‰
  workflow_dispatch:  # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥

jobs:
  process-attachments:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Download attachments
      run: |
        python scripts/download_100_percent.py
    
    - name: Fix filenames
      run: |
        python complete_filename_fix.py
    
    - name: Verify downloads
      run: |
        python scripts/verify_attachments_complete.py
    
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: attachment-reports
        path: |
          *.json
          downloads/
```

## ğŸ“‹ ì‹¤í–‰ ìˆœì„œ

1. **ë°ì´í„° ìˆ˜ì§‘**: `collect_kstartup_batch.py` / `collect_bizinfo_batch.py`
2. **ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ**: `download_100_percent.py`
3. **íŒŒì¼ëª… ìˆ˜ì •**: `complete_filename_fix.py`
4. **ì••ì¶• í•´ì œ**: (í•„ìš”ì‹œ êµ¬í˜„)
5. **í…ìŠ¤íŠ¸ ì¶”ì¶œ**: (ë‹¤ìŒ ë‹¨ê³„)
6. **ê²€ì¦**: `verify_attachments_complete.py`

## ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_KEY=your_service_key
COLLECTION_MODE=full
```

## ğŸ“Š ì¶œë ¥ íŒŒì¼

- `download_complete_record.json` - ë‹¤ìš´ë¡œë“œ ê¸°ë¡
- `attachment_verification_detailed.json` - ê²€ì¦ ë³´ê³ ì„œ
- `downloads/kstartup/` - K-Startup ì²¨ë¶€íŒŒì¼
- `downloads/bizinfo/` - BizInfo ì²¨ë¶€íŒŒì¼
"""
        
        # ë¬¸ì„œ ì €ì¥
        doc_file = self.project_root / "AUTOMATION_SCRIPTS_GUIDE.md"
        with open(doc_file, 'w', encoding='utf-8') as f:
            f.write(automation_doc)
        
        print(f"[OK] ìë™í™” ê°€ì´ë“œ ìƒì„±: {doc_file}")
        
        return doc_file


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    verifier = DetailedAttachmentVerifier()
    
    # K-Startup ê²€ì¦
    verifier.verify_source('kstartup', 'kstartup_complete')
    
    # BizInfo ê²€ì¦
    verifier.verify_source('bizinfo', 'bizinfo_complete')
    
    # ìµœì¢… ë³´ê³ ì„œ
    verifier.generate_final_report()
    
    # ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ë¬¸ì„œí™”
    verifier.document_automation_scripts()


if __name__ == "__main__":
    main()