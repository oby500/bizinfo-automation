#!/usr/bin/env python3
"""
BizInfo ì²¨ë¶€íŒŒì¼ URL ìˆ˜ì§‘ - URLë§Œ ìˆ˜ì§‘
- ì²¨ë¶€íŒŒì¼ URLë§Œ ìˆ˜ì§‘
- íŒŒì¼ëª…ê³¼ íƒ€ì… ì •ë³´ëŠ” ë‹¤ìš´ë¡œë“œ ì‹œ HTTP í—¤ë”ì—ì„œ ì¶”ì¶œ
"""
import sys
sys.stdout.reconfigure(encoding='utf-8')
import os
import requests
from bs4 import BeautifulSoup
import re
from supabase import create_client
from dotenv import load_dotenv
from urllib.parse import urljoin, unquote, urlparse, parse_qs
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

load_dotenv()

# Supabase ì„¤ì • (SERVICE_KEY ìš°ì„  ì‚¬ìš©)
url = os.environ.get('SUPABASE_URL', 'https://csuziaogycciwgxxmahm.supabase.co')
key = os.environ.get('SUPABASE_SERVICE_KEY') or os.environ.get('SUPABASE_KEY')

# í‚¤ê°€ ì—†ìœ¼ë©´ í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš©
if not key:
    key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImNzdXppYW9neWNjaXdneHhtYWhtIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MzYxNTc4MCwiZXhwIjoyMDY5MTkxNzgwfQ.HnhM7zSLzi7lHVPd2IVQKIACDq_YA05mBMgZbSN1c9Q'

supabase = create_client(url, key)

lock = threading.Lock()
progress = {
    'success': 0, 
    'error': 0, 
    'total': 0, 
    'new_files': 0
}


session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': '*/*',
    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
    'Referer': 'https://www.bizinfo.go.kr/'
})


def extract_bizinfo_attachments(detail_url, pblanc_id, announcement_title=None):
    """BizInfo ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ"""
    all_attachments = []
    
    try:
        response = session.get(detail_url, timeout=15)
        if response.status_code != 200:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # BizInfo ì²¨ë¶€íŒŒì¼ íŒ¨í„´ ì°¾ê¸°
        # 1. .fileDown í´ë˜ìŠ¤ ë§í¬ë“¤
        file_links = soup.find_all('a', class_='fileDown')
        
        # 2. onclick="javascript:fnFileDown" íŒ¨í„´
        if not file_links:
            file_links = soup.find_all('a', onclick=re.compile(r'fnFileDown'))
        
        # 3. fileLoad, fileBlank íŒ¨í„´ (ìƒˆë¡œìš´ íŒ¨í„´)
        if not file_links:
            file_links = soup.find_all('a', onclick=re.compile(r'(fileLoad|fileBlank)'))
        
        # 4. ì¼ë°˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ (href="/jsp/down.jsp" ë“±)
        if not file_links:
            file_links = soup.find_all('a', href=re.compile(r'(down\.jsp|download|file|getImageFile)'))
        
        attachments = []
        
        for idx, link in enumerate(file_links, 1):
            href = link.get('href', '')
            text = link.get_text(strip=True) or ''
            onclick = link.get('onclick', '')
            
            # URL ê²°ì •
            full_url = None
            if href and href.startswith('/'):
                full_url = urljoin('https://www.bizinfo.go.kr', href)
            elif href and href.startswith('http'):
                full_url = href
            elif onclick:
                # onclickì—ì„œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                if 'fnFileDown' in onclick:
                    match = re.search(r"fnFileDown\('([^']+)'", onclick)
                    if match:
                        file_param = match.group(1)
                        full_url = f"https://www.bizinfo.go.kr/jsp/down.jsp?file={file_param}"
                elif 'fileLoad' in onclick or 'fileBlank' in onclick:
                    # fileLoad('/webapp/upload/bizinfo/file/2025/09' + '/' + '202509101035440192.pdf', ...
                    match = re.search(r"(fileLoad|fileBlank)\(([^)]+)\)", onclick)
                    if match:
                        params = match.group(2)
                        # ê²½ë¡œ ì¡°í•© - ì²« 3ê°œ ë¬¸ìì—´ ì¡°í•©
                        path_parts = re.findall(r"'([^']+)'", params)
                        if path_parts:
                            file_path = ''.join(path_parts[:3] if len(path_parts) >= 3 else path_parts)
                            full_url = f"https://www.bizinfo.go.kr{file_path}"
            
            if not full_url:
                continue
            
            # URLë§Œ ì €ì¥ (íŒŒì¼ëª…ê³¼ íƒ€ì… ì •ë³´ëŠ” ë‹¤ìš´ë¡œë“œ ì‹œ HTTP í—¤ë”ì—ì„œ ì¶”ì¶œ)
            attachment = {
                'url': full_url
            }
            
            attachments.append(attachment)
        
        all_attachments.extend(attachments)
        
    except Exception as e:
        pass
    
    # ì¤‘ë³µ ì œê±°
    unique_attachments = []
    seen_urls = set()
    for att in all_attachments:
        if att['url'] not in seen_urls:
            seen_urls.add(att['url'])
            unique_attachments.append(att)
    
    return unique_attachments

def process_bizinfo_record(record):
    """BizInfo ë ˆì½”ë“œ ì²˜ë¦¬"""
    pblanc_id = record['pblanc_id']
    detail_url = record.get('detail_url') or record.get('dtl_url')
    title = record.get('pblanc_nm') or record.get('bsns_title', '')
    
    if not detail_url:
        with lock:
            progress['error'] += 1
        return False
    
    try:
        attachments = extract_bizinfo_attachments(detail_url, pblanc_id, title)
        
        if attachments:
            # ì²¨ë¶€íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì €ì¥
            result = supabase.table('bizinfo_complete')\
                .update({'attachment_urls': attachments})\
                .eq('pblanc_id', pblanc_id)\
                .execute()
            
            if result.data:
                with lock:
                    progress['success'] += 1
                    progress['new_files'] += len(attachments)
                print(f"  âœ… {len(attachments)}ê°œ URL ìˆ˜ì§‘ ì™„ë£Œ")
                return True
        else:
            # ì²¨ë¶€íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°°ì—´ë¡œ ì €ì¥
            result = supabase.table('bizinfo_complete')\
                .update({'attachment_urls': []})\
                .eq('pblanc_id', pblanc_id)\
                .execute()
            
            if result.data:
                with lock:
                    progress['success'] += 1
                print(f"  ğŸ“ ì²¨ë¶€íŒŒì¼ ì—†ìŒ (ë¹ˆ ë°°ì—´ ì €ì¥)")
                return True
        
        with lock:
            progress['error'] += 1
        return False
        
    except Exception:
        with lock:
            progress['error'] += 1
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*70)
    print("ğŸ“ BizInfo ì²¨ë¶€íŒŒì¼ URL ìˆ˜ì§‘ (URLë§Œ)")
    print("="*70)
    
    # ì²˜ë¦¬ ì œí•œ í™•ì¸ - ê¸°ë³¸ê°’ 200ê°œë¡œ ì¦ê°€ (100% ìˆ˜ì§‘ ë³´ì¥)
    processing_limit = int(os.environ.get('PROCESSING_LIMIT', '200'))
    
    # ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ - í•­ìƒ ì „ì²´ ì²˜ë¦¬ (í˜ì´ì§€ë„¤ì´ì…˜)
    all_data = []
    page_size = 1000
    offset = 0
    
    while True:
        batch = supabase.table('bizinfo_complete')\
            .select('pblanc_id, pblanc_nm, bsns_title, detail_url, dtl_url, attachment_urls')\
            .range(offset, offset + page_size - 1)\
            .execute()
        
        if not batch.data:
            break
            
        all_data.extend(batch.data)
        print(f"  ë¡œë”©: {len(all_data)}ê°œ...")
        
        if len(batch.data) < page_size:
            break
        offset += page_size
    
    all_records = type('obj', (object,), {'data': all_data})()
    print(f"ğŸ“Œ Full ëª¨ë“œ: ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ({len(all_records.data)}ê°œ)")
    
    needs_processing = []
    
    for record in all_records.data:
        # ì²¨ë¶€íŒŒì¼ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬ (NULLë§Œ ì²˜ë¦¬, ë¹ˆ ë°°ì—´ì€ ì´ë¯¸ ì²˜ë¦¬ë¨)
        attachment_urls = record.get('attachment_urls')
        detail_url = record.get('detail_url') or record.get('dtl_url')
        
        if not detail_url:
            continue  # URLì´ ì—†ìœ¼ë©´ ì²˜ë¦¬ ë¶ˆê°€
            
        # NULLì´ê±°ë‚˜ ë¹ˆ ë°°ì—´ì¸ ê²½ìš° ì²˜ë¦¬ (ìµœê·¼ ë°ì´í„° ì²¨ë¶€íŒŒì¼ ì¬ìˆ˜ì§‘)
        # ë˜ëŠ” ì˜ëª»ëœ í˜•ì‹ì˜ attachment_urlsë„ ì¬ì²˜ë¦¬ (getImageFile, DOC, UNKNOWN íƒ€ì…)
        if attachment_urls is None or attachment_urls == [] or \
           (isinstance(attachment_urls, list) and len(attachment_urls) > 0 and \
            any(att.get('type') in ['getImageFile', 'DOC', 'UNKNOWN', 'HTML'] for att in attachment_urls if isinstance(att, dict))):
            needs_processing.append(record)
    
    # ì²˜ë¦¬ ì œí•œ ì ìš© (ê¸°ë³¸ 200ê°œ)
    if processing_limit > 0 and len(needs_processing) > processing_limit:
        # ìµœì‹  ë°ì´í„°ë¶€í„° ì²˜ë¦¬í•˜ë„ë¡ ì •ë ¬
        needs_processing.sort(key=lambda x: x.get('created_at', ''), reverse=True)
        needs_processing = needs_processing[:processing_limit]
        print(f"ğŸ“Œ ì œí•œ ëª¨ë“œ: ìµœëŒ€ {processing_limit}ê°œë§Œ ì²˜ë¦¬ (ìµœì‹  ë°ì´í„° ìš°ì„ )")
    
    progress['total'] = len(needs_processing)
    
    print(f"âœ… ê²€í†  ëŒ€ìƒ: {len(all_records.data)}ê°œ")
    print(f"ğŸ“ ì²˜ë¦¬ í•„ìš”: {progress['total']}ê°œ")
    
    if progress['total'] == 0:
        print("ğŸ‰ ëª¨ë“  ë ˆì½”ë“œê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return
    
    print(f"ğŸ”¥ {progress['total']}ê°œ ì²˜ë¦¬ ì‹œì‘ (15 workers)...\n")
    
    # ë³‘ë ¬ ì²˜ë¦¬ (ì•ˆì •ì„±ê³¼ ì†ë„ ê· í˜•ì„ ìœ„í•´ worker ìˆ˜ ìµœì í™”)
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(process_bizinfo_record, record): record for record in needs_processing}
        
        for i, future in enumerate(as_completed(futures), 1):
            try:
                future.result()
                if i % 100 == 0:
                    print(f"ì§„í–‰: {i}/{progress['total']} | ì„±ê³µ: {progress['success']} | íŒŒì¼: {progress['new_files']}ê°œ")
            except:
                pass
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“Š BizInfo ì²¨ë¶€íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ")
    print("="*70)
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {progress['success']}/{progress['total']}")
    print(f"ğŸ“ ìˆ˜ì§‘ëœ URL: {progress['new_files']}ê°œ")
    print(f"ğŸ“ ì²¨ë¶€íŒŒì¼ ì—†ìŒ: ë¹ˆ ë°°ì—´ []ë¡œ ì €ì¥ë¨")
    print("\nğŸ”§ ê°œì„ ì‚¬í•­:")
    print("  - NULL vs ë¹ˆ ë°°ì—´ ëª…í™•íˆ êµ¬ë¶„")
    print("  - ì²¨ë¶€íŒŒì¼ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°°ì—´ []ë¡œ ì €ì¥")
    print("  - ìˆœìˆ˜ ë‹¤ìš´ë¡œë“œ URLë§Œ ì €ì¥")
    print("="*70)

if __name__ == "__main__":
    main()