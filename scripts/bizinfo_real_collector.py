#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Bizinfo ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ê¸° - êµ¬ê¸€ ì‹œíŠ¸ ë¡œì§ ê¸°ë°˜
GitHub Actionsìš©
"""

import sys
sys.stdout.reconfigure(encoding='utf-8')

import os
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client
import time
import re
import json

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def format_date_time(dt):
    """ë‚ ì§œ+ì‹œê°„ í¬ë§· YYYY-MM-DD HH:MM:SS"""
    return dt.strftime('%Y-%m-%d %H:%M:%S')

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text:
        return ""
    # HTML íƒœê·¸ ì œê±°
    clean = re.sub(r'<[^>]*>', '', str(text))
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    clean = re.sub(r'\s+', ' ', clean)
    return clean.strip()

def extract_announcement_id(url):
    """ê³µê³  URLì—ì„œ ID ì¶”ì¶œ"""
    if not url:
        return ""
    
    # pblancId íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    match = re.search(r'pblancId=([A-Z0-9_]+)', url)
    if match:
        return match.group(1)
    return ""

def fetch_bizinfo_excel_data():
    """ê¸°ì—…ë§ˆë‹¹ Excel ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±"""
    print("ğŸ“Š ê¸°ì—…ë§ˆë‹¹ Excel ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    try:
        # Excel ë‹¤ìš´ë¡œë“œ URL (ì‹¤ì œ ê¸°ì—…ë§ˆë‹¹ì—ì„œ ì œê³µí•˜ëŠ” URLë¡œ êµì²´ í•„ìš”)
        excel_url = "https://www.bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/excel.do"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(excel_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            print("âœ… Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
            # ì‹¤ì œ Excel íŒŒì‹± ë¡œì§ êµ¬í˜„ í•„ìš”
            # pandasë¥¼ ì‚¬ìš©í•˜ì—¬ Excel íŒŒì‹±
            import pandas as pd
            import io
            
            df = pd.read_excel(io.BytesIO(response.content))
            print(f"ğŸ“‹ Excel ë°ì´í„°: {len(df)}ê°œ í–‰")
            
            return df.to_dict('records')
        else:
            print(f"âŒ Excel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
            return []
    
    except Exception as e:
        print(f"âŒ Excel ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def fetch_bizinfo_rss_data():
    """ê¸°ì—…ë§ˆë‹¹ RSS í”¼ë“œ ë°ì´í„° ìˆ˜ì§‘"""
    print("ğŸ“¡ ê¸°ì—…ë§ˆë‹¹ RSS ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    try:
        rss_url = "https://www.bizinfo.go.kr/uss/rss/bizinfo.do"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(rss_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            # RSS XML íŒŒì‹±
            root = ET.fromstring(response.text)
            
            items = []
            for item in root.findall('.//item'):
                item_data = {}
                
                # ê¸°ë³¸ RSS í•„ë“œ
                title_elem = item.find('title')
                link_elem = item.find('link')
                desc_elem = item.find('description')
                pubdate_elem = item.find('pubDate')
                
                if title_elem is not None:
                    item_data['title'] = clean_text(title_elem.text)
                if link_elem is not None:
                    item_data['link'] = link_elem.text
                if desc_elem is not None:
                    item_data['description'] = clean_text(desc_elem.text)
                if pubdate_elem is not None:
                    item_data['pubDate'] = pubdate_elem.text
                
                # ê³µê³ ID ì¶”ì¶œ
                if 'link' in item_data:
                    item_data['announcement_id'] = extract_announcement_id(item_data['link'])
                
                items.append(item_data)
            
            print(f"âœ… RSS ë°ì´í„°: {len(items)}ê°œ ìˆ˜ì§‘")
            return items
        else:
            print(f"âŒ RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {response.status_code}")
            return []
    
    except Exception as e:
        print(f"âŒ RSS ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def fetch_detail_info(detail_url):
    """ìƒì„¸í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(detail_url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            detail_info = {
                'supervisor': '',
                'executor': '',
                'support_target': '',
                'support_content': '',
                'application_start_date': '',
                'application_end_date': '',
                'attachment_urls': []
            }
            
            # ê°ì¢… ì •ë³´ ì¶”ì¶œ ë¡œì§ êµ¬í˜„
            # (ì‹¤ì œ ê¸°ì—…ë§ˆë‹¹ í˜ì´ì§€ êµ¬ì¡°ì— ë§ì¶° ì¡°ì • í•„ìš”)
            
            return detail_info
        
        return {}
    
    except Exception as e:
        print(f"âš ï¸ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {detail_url} - {e}")
        return {}

def collect_bizinfo_data():
    """ê¸°ì—…ë§ˆë‹¹ ë°ì´í„° ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜"""
    print("="*60)
    print("ğŸ¢ ê¸°ì—…ë§ˆë‹¹ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("="*60)
    
    # ìˆ˜ì§‘ ëª¨ë“œ í™•ì¸
    collection_mode = os.getenv('COLLECTION_MODE', 'daily')
    print(f"ğŸ“‹ ìˆ˜ì§‘ ëª¨ë“œ: {collection_mode}")
    
    if collection_mode == 'daily':
        print("ğŸ“… Daily ëª¨ë“œ: ìµœê·¼ RSS í”¼ë“œë§Œ ìˆ˜ì§‘")
        max_items = 20  # ìµœëŒ€ 20ê°œ í•­ëª©
    else:
        print("ğŸ”„ Full ëª¨ë“œ: ì „ì²´ RSS í”¼ë“œ + ì¶”ê°€ ì†ŒìŠ¤ ìˆ˜ì§‘")
        max_items = 100  # ìµœëŒ€ 100ê°œ í•­ëª©
    
    # Supabase ì—°ê²°
    SUPABASE_URL = os.getenv('SUPABASE_URL')
    SUPABASE_KEY = os.getenv('SUPABASE_KEY')
    
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    print("âœ… Supabase ì—°ê²° ì„±ê³µ")
    
    # ê¸°ì¡´ ê³µê³ ID ì¡°íšŒ
    try:
        existing_result = supabase.table('bizinfo_complete').select('announcement_id').execute()
        existing_ids = set()
        if existing_result.data:
            existing_ids = {str(item['announcement_id']).strip() for item in existing_result.data if item.get('announcement_id')}
        print(f"ğŸ“‹ ê¸°ì¡´ ê³µê³  ìˆ˜: {len(existing_ids)}ê°œ")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")
        existing_ids = set()
    
    # ë°ì´í„° ìˆ˜ì§‘
    new_items = []
    
    # 1. RSS ë°ì´í„° ìˆ˜ì§‘
    rss_items = fetch_bizinfo_rss_data()
    
    # 2. ê° í•­ëª© ì²˜ë¦¬ (ëª¨ë“œë³„ ì œí•œ)
    processed_count = 0
    for item in rss_items:
        if processed_count >= max_items:
            print(f"ğŸ“Š ìµœëŒ€ ì²˜ë¦¬ í•­ëª© ìˆ˜ ({max_items}) ë„ë‹¬ - ìˆ˜ì§‘ ì¢…ë£Œ")
            break
        
        processed_count += 1
        announcement_id = item.get('announcement_id', '')
        
        if not announcement_id:
            print("âš ï¸ ê³µê³ ID ì—†ìŒ - ê±´ë„ˆëœ€")
            continue
        
        if announcement_id in existing_ids:
            print(f"âš ï¸ ì¤‘ë³µ: {announcement_id}")
            continue
        
        print(f"ğŸ” ìƒˆ ê³µê³  ì²˜ë¦¬: {announcement_id}")
        
        # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        detail_url = item.get('link', '')
        detail_info = fetch_detail_info(detail_url) if detail_url else {}
        
        # ìˆ˜ì§‘ ì‹œê°„
        collected_time = format_date_time(datetime.now())
        
        # ë°ì´í„° êµ¬ì„±
        new_item = {
            'announcement_id': announcement_id,
            'title': item.get('title', ''),
            'description': item.get('description', ''),
            'announcement_url': item.get('link', ''),
            'supervisor': detail_info.get('supervisor', ''),
            'executor': detail_info.get('executor', ''),
            'support_target': detail_info.get('support_target', ''),
            'support_content': detail_info.get('support_content', ''),
            'application_start_date': detail_info.get('application_start_date', ''),
            'application_end_date': detail_info.get('application_end_date', ''),
            'attachment_urls': json.dumps(detail_info.get('attachment_urls', []), ensure_ascii=False),
            'source': 'ê¸°ì—…ë§ˆë‹¹',
            'status': 'ìˆ˜ì§‘ì™„ë£Œ',
            'created_at': collected_time,
            'updated_at': collected_time
        }
        
        new_items.append(new_item)
        existing_ids.add(announcement_id)
        
        # ìš”ì²­ ê°„ ë”œë ˆì´
        time.sleep(0.2)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    if new_items:
        print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— {len(new_items)}ê°œ ì €ì¥ ì¤‘...")
        try:
            # ë°°ì¹˜ë¡œ ì‚½ì…
            batch_size = 10
            for i in range(0, len(new_items), batch_size):
                batch = new_items[i:i+batch_size]
                result = supabase.table('bizinfo_complete').insert(batch).execute()
                print(f"ğŸ“ ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ì €ì¥ ì™„ë£Œ")
                time.sleep(0.5)  # ë°°ì¹˜ ê°„ ë”œë ˆì´
            
            print(f"âœ… ì´ {len(new_items)}ê°œ ìƒˆë¡œìš´ ê³µê³  ì €ì¥ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
    else:
        print("â„¹ï¸ ì €ì¥í•  ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n" + "="*60)
    print("ğŸ‰ ê¸°ì—…ë§ˆë‹¹ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"ğŸ“Š ìƒˆë¡œìš´ ê³µê³ : {len(new_items)}ê°œ")
    print("="*60)

if __name__ == "__main__":
    collect_bizinfo_data()