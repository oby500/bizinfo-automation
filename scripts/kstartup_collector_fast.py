#!/usr/bin/env python3
"""
K-Startup ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (ê³ ì† ë²„ì „)
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„ 
- ì„¸ì…˜ ì¬ì‚¬ìš©ìœ¼ë¡œ ì—°ê²° ìµœì í™”
"""
import os
import sys
import requests
import json
from datetime import datetime
from urllib.parse import urljoin
from supabase import create_client, Client
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

class KStartupCollectorFast:
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # Supabase ì—°ê²°
        url = os.environ.get('SUPABASE_URL')
        key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
        
        if not url or not key:
            logging.error("í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        self.supabase: Client = create_client(url, key)
        
        # ì„¸ì…˜ ì¬ì‚¬ìš© (ì—°ê²° í’€ë§)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Content-Type': 'application/json'
        })
        
        # API ì„¤ì •
        self.api_base_url = "http://www.k-startup.go.kr/web/module/bizpbanc-list.do"
        
        logging.info("=== K-Startup ê³ ì† ìˆ˜ì§‘ ì‹œì‘ ===")
    
    def fetch_page(self, page_num):
        """ë‹¨ì¼ í˜ì´ì§€ ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            params = {
                'page': page_num,
                'pageSize': 100,  # í•œ ë²ˆì— 100ê°œì”©
                'searchType': 'all',
                'searchPbancSttsCd': '01',  # ëª¨ì§‘ì¤‘
                'orderBy': 'recent'
            }
            
            response = self.session.post(
                self.api_base_url,
                json=params,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'resultList' in data:
                    logging.info(f"  í˜ì´ì§€ {page_num}: {len(data['resultList'])}ê°œ ì¡°íšŒ")
                    return data['resultList']
            
            return []
            
        except Exception as e:
            logging.error(f"í˜ì´ì§€ {page_num} ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def fetch_all_announcements_fast(self):
        """ëª¨ë“  ê³µê³  ë³‘ë ¬ ì¡°íšŒ"""
        try:
            start_time = time.time()
            
            # ë¨¼ì € ì²« í˜ì´ì§€ë¡œ ì „ì²´ ê°œìˆ˜ í™•ì¸
            first_page = self.fetch_page(1)
            if not first_page:
                logging.warning("API ì‘ë‹µ ì—†ìŒ, ìŠ¤í¬ë˜í•‘ ëª¨ë“œë¡œ ì „í™˜")
                return self.scrape_announcements_fast()
            
            all_announcements = first_page
            
            # ì¶”ê°€ í˜ì´ì§€ê°€ ìˆì„ ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬
            # K-Startupì€ ë³´í†µ 500ê°œ ì´í•˜ì´ë¯€ë¡œ 5í˜ì´ì§€ë©´ ì¶©ë¶„
            pages_to_fetch = [2, 3, 4, 5]
            
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {executor.submit(self.fetch_page, page): page 
                          for page in pages_to_fetch}
                
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=10)
                        if result:
                            all_announcements.extend(result)
                    except Exception as e:
                        logging.error(f"í˜ì´ì§€ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            
            elapsed = time.time() - start_time
            logging.info(f"API ì¡°íšŒ ì™„ë£Œ: {len(all_announcements)}ê°œ ({elapsed:.1f}ì´ˆ)")
            
            return all_announcements
            
        except Exception as e:
            logging.error(f"ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return self.scrape_announcements_fast()
    
    def scrape_announcements_fast(self):
        """ë¹ ë¥¸ ì›¹ ìŠ¤í¬ë˜í•‘ (ëŒ€ì²´ ë°©ë²•)"""
        try:
            from bs4 import BeautifulSoup
            
            logging.info("ê³ ì† ìŠ¤í¬ë˜í•‘ ëª¨ë“œ...")
            
            # ì—¬ëŸ¬ í˜ì´ì§€ ë³‘ë ¬ ìŠ¤í¬ë˜í•‘
            base_url = "https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do"
            announcements = []
            
            def scrape_page(page_num):
                try:
                    url = f"{base_url}?page={page_num}"
                    response = self.session.get(url, timeout=8)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        page_items = []
                        
                        # ê³µê³  ëª©ë¡ ì¶”ì¶œ (ê°„ì†Œí™”)
                        items = soup.find_all(['div', 'li', 'tr'], class_=re.compile(r'item|list|row'))[:20]
                        
                        for idx, item in enumerate(items, 1):
                            title_elem = item.find('a')
                            if title_elem:
                                page_items.append({
                                    'bizPbancSn': f"{datetime.now().strftime('%Y%m%d')}_{page_num}_{idx}",
                                    'bizPbancNm': title_elem.get_text(strip=True),
                                    'pbancNtrpNm': '',
                                    'pbancRcptBgngDt': None,
                                    'pbancRcptEndDt': None,
                                    'detlPgUrl': urljoin(base_url, title_elem.get('href', ''))
                                })
                        
                        return page_items
                    return []
                except:
                    return []
            
            # ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ (3í˜ì´ì§€)
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = [executor.submit(scrape_page, i) for i in range(1, 4)]
                
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=10)
                        announcements.extend(result)
                    except:
                        pass
            
            logging.info(f"ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(announcements)}ê°œ")
            return announcements
            
        except Exception as e:
            logging.error(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return []
    
    def save_to_database_batch(self, announcements):
        """ë°°ì¹˜ DB ì €ì¥ (ê³ ì†)"""
        if not announcements:
            logging.info("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        start_time = time.time()
        
        # 1. ê¸°ì¡´ ID ì¡°íšŒ (í•œ ë²ˆì—)
        existing_result = self.supabase.table('kstartup_complete').select('announcement_id').execute()
        existing_ids = {item['announcement_id'] for item in existing_result.data} if existing_result.data else set()
        
        # 2. ì‹ ê·œ ë°ì´í„° í•„í„°ë§ (ë©”ëª¨ë¦¬ì—ì„œ)
        new_records = []
        duplicate_count = 0
        
        for ann in announcements:
            announcement_id = f"KS_{ann.get('bizPbancSn', '')}"
            
            if announcement_id in existing_ids:
                duplicate_count += 1
                continue
            
            # ë ˆì½”ë“œ ìƒì„± (ê°„ì†Œí™”)
            record = {
                'announcement_id': announcement_id,
                'biz_pbanc_nm': ann.get('bizPbancNm', ''),
                'pbanc_ctnt': ann.get('pbancCtnt', ''),
                'supt_biz_clsfc': ann.get('suptBizClsfc', ''),
                'aply_trgt_ctnt': ann.get('aplyTrgtCtnt', ''),
                'supt_regin': ann.get('suptRegin', ''),
                'pbanc_rcpt_bgng_dt': self.parse_date_fast(ann.get('pbancRcptBgngDt')),
                'pbanc_rcpt_end_dt': self.parse_date_fast(ann.get('pbancRcptEndDt')),
                'pbanc_ntrp_nm': ann.get('pbancNtrpNm', ''),
                'biz_gdnc_url': ann.get('bizGdncUrl', ''),
                'biz_aply_url': ann.get('bizAplyUrl', ''),
                'detl_pg_url': ann.get('detlPgUrl', ''),
                'attachment_urls': [],
                'attachment_count': 0,
                'attachment_processing_status': 'pending',  # ì´ˆê¸°ê°’
                'created_at': datetime.now().isoformat()
            }
            
            new_records.append(record)
        
        # 3. ë°°ì¹˜ ì €ì¥ (í•œ ë²ˆì—)
        success_count = 0
        
        if new_records:
            logging.info(f"ë°°ì¹˜ ì €ì¥: {len(new_records)}ê°œ")
            
            # 100ê°œì”© ë‚˜ëˆ ì„œ ì €ì¥ (Supabase ì œí•œ)
            batch_size = 100
            for i in range(0, len(new_records), batch_size):
                batch = new_records[i:i+batch_size]
                try:
                    result = self.supabase.table('kstartup_complete').insert(batch).execute()
                    if result.data:
                        success_count += len(result.data)
                        logging.info(f"  ë°°ì¹˜ {i//batch_size + 1} ì €ì¥: {len(result.data)}ê°œ")
                except Exception as e:
                    logging.error(f"ë°°ì¹˜ ì €ì¥ ì˜¤ë¥˜: {e}")
        
        elapsed = time.time() - start_time
        
        # ê²°ê³¼ ìš”ì•½
        logging.info("\n" + "="*50)
        logging.info("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")
        logging.info(f"âœ… ì‹ ê·œ ì €ì¥: {success_count}ê°œ")
        logging.info(f"â­ï¸ ì¤‘ë³µ ì œì™¸: {duplicate_count}ê°œ")
        logging.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
        logging.info(f"âš¡ í‰ê·  ì†ë„: {len(announcements)/elapsed:.1f}ê°œ/ì´ˆ")
        logging.info("="*50)
        
        return success_count
    
    def parse_date_fast(self, date_str):
        """ë¹ ë¥¸ ë‚ ì§œ íŒŒì‹±"""
        if not date_str:
            return None
        
        try:
            date_str = date_str.strip()[:10]  # ë‚ ì§œ ë¶€ë¶„ë§Œ
            
            if '-' in date_str:
                return date_str
            elif '.' in date_str:
                return date_str.replace('.', '-')
            elif len(date_str) == 8 and date_str.isdigit():
                return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
            
            return None
        except:
            return None
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # 1. ë³‘ë ¬ ì¡°íšŒ
            announcements = self.fetch_all_announcements_fast()
            
            if not announcements:
                logging.info("ìˆ˜ì§‘í•  ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # 2. ë°°ì¹˜ ì €ì¥
            saved_count = self.save_to_database_batch(announcements)
            
            # 3. ì „ì²´ ì‹œê°„
            total_elapsed = time.time() - start_time
            logging.info(f"\nğŸš€ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed:.1f}ì´ˆ")
            
            return True
            
        except Exception as e:
            logging.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
        finally:
            # ì„¸ì…˜ ì¢…ë£Œ
            self.session.close()

if __name__ == "__main__":
    import re  # BeautifulSoupì—ì„œ ì‚¬ìš©
    
    collector = KStartupCollectorFast()
    success = collector.run()
    sys.exit(0 if success else 1)
