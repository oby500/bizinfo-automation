#!/usr/bin/env python3
"""
K-Startup ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (ê°œì„ ëœ ê³ ì† ë²„ì „)
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„ 
- ì„¸ì…˜ ìž¬ì‚¬ìš©ìœ¼ë¡œ ì—°ê²° ìµœì í™”
- ìƒˆë¡œìš´ ê³µê³  ê°ì§€ ë¡œì§ ì¶”ê°€
- ë” ë§Žì€ íŽ˜ì´ì§€ ìˆ˜ì§‘
"""
import os
import sys
import requests
import json
from datetime import datetime, timedelta
from urllib.parse import urljoin
from supabase import create_client, Client
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

class KStartupCollectorFast:
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # Supabase ì—°ê²°
        url = os.environ.get('SUPABASE_URL')
        key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
        
        if not url or not key:
            logging.error("í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        self.supabase: Client = create_client(url, key)
        
        # ì„¸ì…˜ ìž¬ì‚¬ìš© (ì—°ê²° í’€ë§)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Content-Type': 'application/json'
        })
        
        # API ì„¤ì •
        self.api_base_url = "http://www.k-startup.go.kr/web/module/bizpbanc-list.do"
        
        # ìµœê·¼ ì²´í¬ ê¸°ì¤€ (7ì¼)
        self.recent_days = 7
        
        logging.info("=== K-Startup ê³ ì† ìˆ˜ì§‘ ì‹œìž‘ (ê°œì„  ë²„ì „) ===")
    
    def get_last_collected_info(self):
        """ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì •ë³´ ì¡°íšŒ"""
        try:
            # ê°€ìž¥ ìµœê·¼ ë°ì´í„° ì¡°íšŒ
            result = self.supabase.table('kstartup_complete')\
                .select('announcement_id,created_at')\
                .order('created_at', desc=True)\
                .limit(1)\
                .execute()
            
            if result.data:
                last_record = result.data[0]
                last_time = datetime.fromisoformat(last_record['created_at'].replace('Z', '+00:00'))
                logging.info(f"ë§ˆì§€ë§‰ ìˆ˜ì§‘: {last_time.strftime('%Y-%m-%d %H:%M')} - {last_record['announcement_id']}")
                return last_time
            else:
                logging.info("ì²« ìˆ˜ì§‘ìž…ë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            logging.error(f"ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def fetch_page(self, page_num):
        """ë‹¨ì¼ íŽ˜ì´ì§€ ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            params = {
                'page': page_num,
                'pageSize': 100,  # í•œ ë²ˆì— 100ê°œì”©
                'searchType': 'all',
                'searchPbancSttsCd': '01',  # ëª¨ì§‘ì¤‘
                'orderBy': 'recent'
            }
            
            response = self.session.post(
                self.api_base_url,
                json=params,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'resultList' in data:
                    logging.info(f"  íŽ˜ì´ì§€ {page_num}: {len(data['resultList'])}ê°œ ì¡°íšŒ")
                    return data['resultList']
            
            return []
            
        except Exception as e:
            logging.error(f"íŽ˜ì´ì§€ {page_num} ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def fetch_all_announcements_fast(self):
        """ëª¨ë“  ê³µê³  ë³‘ë ¬ ì¡°íšŒ (ê°œì„ )"""
        try:
            start_time = time.time()
            
            # ë¨¼ì € ì²« íŽ˜ì´ì§€ë¡œ ì „ì²´ ê°œìˆ˜ í™•ì¸
            first_page = self.fetch_page(1)
            if not first_page:
                logging.warning("API ì‘ë‹µ ì—†ìŒ, ìŠ¤í¬ëž˜í•‘ ëª¨ë“œë¡œ ì „í™˜")
                return self.scrape_announcements_fast()
            
            all_announcements = first_page
            
            # ì¶”ê°€ íŽ˜ì´ì§€ê°€ ìžˆì„ ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬
            # 10íŽ˜ì´ì§€ê¹Œì§€ í™•ìž¥ (1000ê°œ)
            pages_to_fetch = list(range(2, 11))  # 2~10 íŽ˜ì´ì§€
            
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = {executor.submit(self.fetch_page, page): page 
                          for page in pages_to_fetch}
                
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=10)
                        if result:
                            all_announcements.extend(result)
                        else:
                            # ë¹ˆ íŽ˜ì´ì§€ë©´ ë” ì´ìƒ ì§„í–‰ ì•ˆí•¨
                            break
                    except Exception as e:
                        logging.error(f"íŽ˜ì´ì§€ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            
            elapsed = time.time() - start_time
            logging.info(f"API ì¡°íšŒ ì™„ë£Œ: {len(all_announcements)}ê°œ ({elapsed:.1f}ì´ˆ)")
            
            return all_announcements
            
        except Exception as e:
            logging.error(f"ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return self.scrape_announcements_fast()
    
    def scrape_announcements_fast(self):
        """ë¹ ë¥¸ ì›¹ ìŠ¤í¬ëž˜í•‘ (ê°œì„ ëœ ë²„ì „)"""
        try:
            from bs4 import BeautifulSoup
            import re
            
            logging.info("ê³ ì† ìŠ¤í¬ëž˜í•‘ ëª¨ë“œ (í™•ìž¥)...")
            
            # ì—¬ëŸ¬ íŽ˜ì´ì§€ ë³‘ë ¬ ìŠ¤í¬ëž˜í•‘
            base_url = "https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do"
            announcements = []
            
            def scrape_page(page_num):
                try:
                    url = f"{base_url}?page={page_num}"
                    response = self.session.get(url, timeout=8)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        page_items = []
                        
                        # ê³µê³  ëª©ë¡ ì¶”ì¶œ (ë” ë§Žì´)
                        items = soup.find_all(['div', 'li', 'tr'], class_=re.compile(r'item|list|row'))[:50]
                        
                        for idx, item in enumerate(items, 1):
                            title_elem = item.find('a')
                            if title_elem:
                                # ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                                date_elem = item.find(['span', 'td'], class_=re.compile(r'date|time'))
                                date_str = date_elem.get_text(strip=True) if date_elem else None
                                
                                page_items.append({
                                    'bizPbancSn': f"{datetime.now().strftime('%Y%m%d')}_{page_num}_{idx}",
                                    'bizPbancNm': title_elem.get_text(strip=True),
                                    'pbancNtrpNm': '',
                                    'pbancRcptBgngDt': self.parse_date_from_text(date_str),
                                    'pbancRcptEndDt': None,
                                    'detlPgUrl': urljoin(base_url, title_elem.get('href', ''))
                                })
                        
                        return page_items
                    return []
                except Exception as e:
                    logging.error(f"ìŠ¤í¬ëž˜í•‘ íŽ˜ì´ì§€ {page_num} ì˜¤ë¥˜: {e}")
                    return []
            
            # ë³‘ë ¬ ìŠ¤í¬ëž˜í•‘ (10íŽ˜ì´ì§€ë¡œ í™•ìž¥)
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(scrape_page, i) for i in range(1, 11)]
                
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=10)
                        announcements.extend(result)
                    except:
                        pass
            
            logging.info(f"ìŠ¤í¬ëž˜í•‘ ì™„ë£Œ: {len(announcements)}ê°œ")
            return announcements
            
        except Exception as e:
            logging.error(f"ìŠ¤í¬ëž˜í•‘ ì˜¤ë¥˜: {e}")
            return []
    
    def check_new_announcements(self, announcements):
        """ìƒˆë¡œìš´ ê³µê³  ê°ì§€"""
        if not announcements:
            return [], []
        
        # 1. ê¸°ì¡´ ID ì¡°íšŒ (í•œ ë²ˆì—)
        existing_result = self.supabase.table('kstartup_complete').select('announcement_id').execute()
        existing_ids = {item['announcement_id'] for item in existing_result.data} if existing_result.data else set()
        
        new_announcements = []
        updated_announcements = []
        
        # 2. ìƒˆë¡œìš´ ê³µê³ ì™€ ì—…ë°ì´íŠ¸ëœ ê³µê³  ë¶„ë¥˜
        for ann in announcements:
            announcement_id = f"KS_{ann.get('bizPbancSn', '')}"
            
            if announcement_id not in existing_ids:
                new_announcements.append(ann)
            else:
                # ë§ˆê°ì¼ ë³€ê²½ ë“± ì²´í¬ í•„ìš”ì‹œ
                updated_announcements.append(ann)
        
        logging.info(f"ðŸ“Š ê°ì§€ ê²°ê³¼: ì‹ ê·œ {len(new_announcements)}ê°œ, ì—…ë°ì´íŠ¸ {len(updated_announcements)}ê°œ")
        
        return new_announcements, updated_announcements
    
    def save_to_database_batch(self, announcements):
        """ë°°ì¹˜ DB ì €ìž¥ (ê°œì„ )"""
        if not announcements:
            logging.info("ì €ìž¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        start_time = time.time()
        
        # ìƒˆë¡œìš´ ê³µê³  ê°ì§€
        new_announcements, updated_announcements = self.check_new_announcements(announcements)
        
        if not new_announcements and not updated_announcements:
            logging.info("âœ… ëª¨ë“  ê³µê³ ê°€ ìµœì‹  ìƒíƒœìž…ë‹ˆë‹¤.")
            return 0
        
        # ì‹ ê·œ ê³µê³  ì €ìž¥
        success_count = 0
        
        if new_announcements:
            logging.info(f"ðŸ†• ì‹ ê·œ ê³µê³  ì €ìž¥: {len(new_announcements)}ê°œ")
            
            new_records = []
            for ann in new_announcements:
                announcement_id = f"KS_{ann.get('bizPbancSn', '')}"
                
                # ë ˆì½”ë“œ ìƒì„±
                record = {
                    'announcement_id': announcement_id,
                    'biz_pbanc_nm': ann.get('bizPbancNm', ''),
                    'pbanc_ctnt': ann.get('pbancCtnt', ''),
                    'supt_biz_clsfc': ann.get('suptBizClsfc', ''),
                    'aply_trgt_ctnt': ann.get('aplyTrgtCtnt', ''),
                    'supt_regin': ann.get('suptRegin', ''),
                    'pbanc_rcpt_bgng_dt': self.parse_date_fast(ann.get('pbancRcptBgngDt')),
                    'pbanc_rcpt_end_dt': self.parse_date_fast(ann.get('pbancRcptEndDt')),
                    'pbanc_ntrp_nm': ann.get('pbancNtrpNm', ''),
                    'biz_gdnc_url': ann.get('bizGdncUrl', ''),
                    'biz_aply_url': ann.get('bizAplyUrl', ''),
                    'detl_pg_url': ann.get('detlPgUrl', ''),
                    'attachment_urls': [],
                    'attachment_count': 0,
                    'attachment_processing_status': 'pending',
                    'created_at': datetime.now().isoformat()
                }
                
                new_records.append(record)
            
            # 100ê°œì”© ë‚˜ëˆ ì„œ ì €ìž¥ (Supabase ì œí•œ)
            batch_size = 100
            for i in range(0, len(new_records), batch_size):
                batch = new_records[i:i+batch_size]
                try:
                    result = self.supabase.table('kstartup_complete').insert(batch).execute()
                    if result.data:
                        success_count += len(result.data)
                        logging.info(f"  ë°°ì¹˜ {i//batch_size + 1} ì €ìž¥: {len(result.data)}ê°œ")
                except Exception as e:
                    logging.error(f"ë°°ì¹˜ ì €ìž¥ ì˜¤ë¥˜: {e}")
        
        # ì—…ë°ì´íŠ¸ëœ ê³µê³  ì²˜ë¦¬ (í•„ìš”ì‹œ)
        if updated_announcements:
            logging.info(f"ðŸ”„ ì—…ë°ì´íŠ¸ í•„ìš” ê³µê³ : {len(updated_announcements)}ê°œ (í˜„ìž¬ ìŠ¤í‚µ)")
        
        elapsed = time.time() - start_time
        
        # ê²°ê³¼ ìš”ì•½
        logging.info("\n" + "="*50)
        logging.info("ðŸ“Š ìˆ˜ì§‘ ê²°ê³¼")
        logging.info(f"âœ… ì‹ ê·œ ì €ìž¥: {success_count}ê°œ")
        logging.info(f"â­ï¸ ì¤‘ë³µ ì œì™¸: {len(announcements) - len(new_announcements)}ê°œ")
        logging.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
        if len(announcements) > 0:
            logging.info(f"âš¡ í‰ê·  ì†ë„: {len(announcements)/elapsed:.1f}ê°œ/ì´ˆ")
        
        # ì•Œë¦¼ìš© ë©”ì‹œì§€ (ì‹ ê·œ ê³µê³ ê°€ ë§Žìœ¼ë©´)
        if success_count > 10:
            logging.info(f"\nðŸŽ‰ ì˜¤ëŠ˜ ì‹ ê·œ ê³µê³ ê°€ {success_count}ê°œë‚˜ ìžˆìŠµë‹ˆë‹¤!")
        
        logging.info("="*50)
        
        return success_count
    
    def parse_date_fast(self, date_str):
        """ë¹ ë¥¸ ë‚ ì§œ íŒŒì‹±"""
        if not date_str:
            return None
        
        try:
            date_str = str(date_str).strip()[:10]  # ë‚ ì§œ ë¶€ë¶„ë§Œ
            
            if '-' in date_str:
                return date_str
            elif '.' in date_str:
                return date_str.replace('.', '-')
            elif len(date_str) == 8 and date_str.isdigit():
                return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
            
            return None
        except:
            return None
    
    def parse_date_from_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        if not text:
            return None
        
        import re
        
        # 2025-01-01, 2025.01.01, 2025/01/01 í˜•ì‹
        patterns = [
            r'(\d{4}[-./]\d{1,2}[-./]\d{1,2})',
            r'(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)',
            r'(\d{2}[-./]\d{1,2}[-./]\d{1,2})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return self.parse_date_fast(match.group(1))
        
        return None
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # 0. ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì •ë³´ í™•ì¸
            last_collected = self.get_last_collected_info()
            
            # 1. ë³‘ë ¬ ì¡°íšŒ (ìµœëŒ€ 1000ê°œ)
            announcements = self.fetch_all_announcements_fast()
            
            if not announcements:
                logging.info("ìˆ˜ì§‘í•  ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            logging.info(f"ðŸ“‹ ì „ì²´ ì¡°íšŒ: {len(announcements)}ê°œ")
            
            # 2. ë°°ì¹˜ ì €ìž¥ (ì¤‘ë³µ ì²´í¬ í¬í•¨)
            saved_count = self.save_to_database_batch(announcements)
            
            # 3. ì „ì²´ ì‹œê°„
            total_elapsed = time.time() - start_time
            logging.info(f"\nðŸš€ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed:.1f}ì´ˆ")
            
            # 4. ì²˜ë¦¬ í†µê³„
            if saved_count > 0:
                logging.info(f"âœ¨ ìƒˆë¡œìš´ ê³µê³  {saved_count}ê°œ ì¶”ê°€ ì™„ë£Œ!")
            
            return True
            
        except Exception as e:
            logging.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
        finally:
            # ì„¸ì…˜ ì¢…ë£Œ
            self.session.close()

if __name__ == "__main__":
    collector = KStartupCollectorFast()
    success = collector.run()
    sys.exit(0 if success else 1)
