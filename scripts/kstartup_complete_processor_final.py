#!/usr/bin/env python3
"""
K-Startup í†µí•© ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ë²„ì „)
- ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ ë¡œì§ ê°•í™”
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„  (ìµœì í™”)
"""
import os
import sys
import time
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, parse_qs, urlparse
import re
from supabase import create_client, Client
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

class KStartupCompleteProcessorFinal:
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # Supabase ì—°ê²°
        url = os.environ.get('SUPABASE_URL')
        key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
        
        if not url or not key:
            logging.error("í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        self.supabase: Client = create_client(url, key)
        
        # í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # ì„¸ì…˜ ì¬ì‚¬ìš© (ì—°ê²° ì¬ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ)
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        logging.info("=== K-Startup ìµœì¢… ë²„ì „ (ì†ë„+íŒŒì¼ëª… ê°œì„ ) ===")
    
    def extract_real_filename(self, text):
        """ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ (ìµœì í™”)"""
        if not text:
            return None
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'^\[.*?\]\s*', '', text)  # [ì²¨ë¶€íŒŒì¼] ë“± ì œê±°
        text = text.strip()
        
        # íŒŒì¼ í™•ì¥ì íŒ¨í„´ (ì»´íŒŒì¼ëœ ì •ê·œì‹ - ì†ë„ í–¥ìƒ)
        if not hasattr(self, '_file_pattern'):
            self._file_pattern = re.compile(
                r'([^\/\\:*?"<>|\n\r\t]+\.(?:hwp|hwpx|pdf|doc|docx|xls|xlsx|ppt|pptx|zip|jpg|jpeg|png|gif|txt|rtf))\b',
                re.IGNORECASE
            )
        
        match = self._file_pattern.search(text)
        if match:
            return match.group(1).strip()
        
        # í™•ì¥ìê°€ ì—†ì–´ë„ íŒŒì¼ëª…ì²˜ëŸ¼ ë³´ì´ë©´ ë°˜í™˜
        if 5 < len(text) < 200:
            return text
        
        return None
    
    def get_file_extension_fast(self, filename):
        """íŒŒì¼ í™•ì¥ì ì¶”ì¶œ (ìµœì í™”)"""
        if not filename:
            return 'unknown'
        
        filename_lower = filename.lower()
        
        # ì§ì ‘ í™•ì¥ì ë§¤ì¹­ (ì •ê·œì‹ë³´ë‹¤ ë¹ ë¦„)
        for ext in ['hwp', 'hwpx', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx', 'zip', 'jpg', 'jpeg', 'png', 'gif', 'txt', 'rtf']:
            if filename_lower.endswith('.' + ext):
                return ext
        
        # íŒŒì¼ëª… íŒíŠ¸ë¡œ ì¶”ë¡ 
        if 'í•œê¸€' in filename or 'hwp' in filename_lower:
            return 'hwp'
        elif 'pdf' in filename_lower:
            return 'pdf'
        elif 'excel' in filename_lower or 'xls' in filename_lower:
            return 'xlsx'
        elif 'word' in filename_lower or 'doc' in filename_lower:
            return 'docx'
        elif 'ppt' in filename_lower or 'ë°œí‘œ' in filename:
            return 'pptx'
        elif 'zip' in filename_lower or 'ì••ì¶•' in filename:
            return 'zip'
        
        return 'unknown'
    
    def create_safe_filename(self, announcement_id, index, original_filename):
        """ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±"""
        ext = self.get_file_extension_fast(original_filename)
        return f"{announcement_id}_{index:02d}.{ext}"
    
    def extract_attachments_fast(self, announcement_id, detail_url):
        """ë¹ ë¥¸ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ (íŒŒì¼ëª… ê°œì„  í¬í•¨)"""
        if not detail_url:
            return [], []
        
        try:
            # K-Startupì€ HTTP ì‚¬ìš©
            if detail_url.startswith('https://'):
                detail_url = detail_url.replace('https://', 'http://')
            
            response = self.session.get(detail_url, timeout=8)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            attachments = []
            page_hashtags = []
            
            # í•´ì‹œíƒœê·¸ ì¶”ì¶œ (ê°„ë‹¨í•˜ê²Œ)
            for tag_elem in soup.find_all(class_=re.compile(r'keyword|tag', re.I))[:5]:
                text = tag_elem.get_text(strip=True)
                if text and len(text) < 20:
                    page_hashtags.append(text)
            
            # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
            attachment_index = 0
            processed_urls = set()
            
            # content_wrap ë‚´ì˜ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
            content_area = soup.find('div', class_='content_wrap') or soup
            download_links = content_area.find_all('a', href=re.compile(r'/afile/fileDownload/'))
            
            for link in download_links[:20]:  # ìµœëŒ€ 20ê°œë§Œ ì²˜ë¦¬ (ì†ë„)
                href = link.get('href', '')
                if not href:
                    continue
                
                full_url = urljoin(detail_url, href)
                if full_url in processed_urls:
                    continue
                processed_urls.add(full_url)
                attachment_index += 1
                
                # íŒŒì¼ëª… ì°¾ê¸° (ë¹ ë¥¸ ë°©ë²•ë§Œ)
                display_filename = None
                
                # 1. ë¶€ëª¨ li ìš”ì†Œì—ì„œ ì°¾ê¸°
                parent_li = link.find_parent('li')
                if parent_li:
                    # file_bgì˜ title ì†ì„±
                    file_bg = parent_li.find(class_='file_bg')
                    if file_bg and file_bg.get('title'):
                        display_filename = self.extract_real_filename(file_bg.get('title'))
                    
                    # li í…ìŠ¤íŠ¸ì—ì„œ
                    if not display_filename:
                        li_text = parent_li.get_text(strip=True).replace('ë‹¤ìš´ë¡œë“œ', '').replace('â€” ğŸ“', '')
                        display_filename = self.extract_real_filename(li_text)
                
                # 2. ë§í¬ ì£¼ë³€ í…ìŠ¤íŠ¸
                if not display_filename:
                    parent = link.find_parent(['div', 'td'])
                    if parent:
                        parent_text = parent.get_text(strip=True).replace('ë‹¤ìš´ë¡œë“œ', '')
                        display_filename = self.extract_real_filename(parent_text)
                
                # 3. ê¸°ë³¸ê°’
                if not display_filename:
                    display_filename = f"ì²¨ë¶€íŒŒì¼_{attachment_index}"
                
                # safe_filename ìƒì„±
                safe_filename = self.create_safe_filename(announcement_id, attachment_index, display_filename)
                
                # íŒŒì¼ íƒ€ì… ê²°ì •
                file_type = self.get_file_type_fast(display_filename, href)
                
                attachment = {
                    'url': full_url,
                    'text': 'ë‹¤ìš´ë¡œë“œ',
                    'type': file_type,
                    'params': {},  # íŒŒë¼ë¯¸í„° íŒŒì‹± ìƒëµ (ì†ë„)
                    'safe_filename': safe_filename,
                    'display_filename': display_filename,
                    'original_filename': display_filename
                }
                
                attachments.append(attachment)
            
            return attachments, page_hashtags
            
        except Exception as e:
            logging.debug(f"ì²¨ë¶€íŒŒì¼ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return [], []
    
    def get_file_type_fast(self, filename, url):
        """íŒŒì¼ íƒ€ì… ì¶”ì¶œ (ìµœì í™”)"""
        text_lower = (filename or '').lower()
        url_lower = url.lower()
        
        # í™•ì¥ì ê¸°ë°˜ ë¹ ë¥¸ ë§¤ì¹­
        if '.hwp' in text_lower or '.hwp' in url_lower:
            return 'HWP'
        elif '.pdf' in text_lower or '.pdf' in url_lower:
            return 'PDF'
        elif '.doc' in text_lower or '.doc' in url_lower:
            return 'DOC'
        elif '.xls' in text_lower or '.xls' in url_lower:
            return 'EXCEL'
        elif '.ppt' in text_lower or '.ppt' in url_lower:
            return 'PPT'
        elif '.zip' in text_lower or '.zip' in url_lower:
            return 'ZIP'
        elif any(ext in text_lower or ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.gif']):
            return 'IMAGE'
        
        return 'FILE'
    
    def process_single_item(self, item: Dict) -> Dict:
        """ë‹¨ì¼ í•­ëª© ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            result = {
                'id': item['id'],
                'announcement_id': item['announcement_id'],
                'biz_pbanc_nm': item['biz_pbanc_nm'],
                'success': False,
                'attachments': [],
                'hashtags': '',
                'summary': '',
                'error': None
            }
            
            # ì²¨ë¶€íŒŒì¼ í¬ë¡¤ë§
            if item.get('detl_pg_url'):
                attachments, page_hashtags = self.extract_attachments_fast(
                    item['announcement_id'], 
                    item['detl_pg_url']
                )
                result['attachments'] = attachments
                
                # í•´ì‹œíƒœê·¸ ìƒì„±
                hashtags = self.generate_hashtags_fast(item, page_hashtags)
                result['hashtags'] = hashtags
                
                # ìš”ì•½ ìƒì„±
                summary = self.create_summary_fast(item, attachments, hashtags)
                result['summary'] = summary
                
                result['success'] = True
            
            return result
            
        except Exception as e:
            return {
                'id': item['id'],
                'announcement_id': item.get('announcement_id', 'unknown'),
                'biz_pbanc_nm': item.get('biz_pbanc_nm', ''),
                'success': False,
                'attachments': [],
                'hashtags': '',
                'summary': '',
                'error': str(e)
            }
    
    def generate_hashtags_fast(self, item, page_hashtags=None):
        """ë¹ ë¥¸ í•´ì‹œíƒœê·¸ ìƒì„±"""
        tags = []
        
        if page_hashtags:
            tags.extend(page_hashtags[:5])
        
        # ì£¼ìš” í•„ë“œë§Œ ì²˜ë¦¬
        if item.get('supt_biz_clsfc'):
            tags.extend(item['supt_biz_clsfc'].split(',')[:3])
        
        if item.get('pbanc_ntrp_nm'):
            org = item['pbanc_ntrp_nm'].replace('(ì£¼)', '').strip()
            if len(org) <= 10:
                tags.append(org)
        
        # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•˜ê²Œ)
        title = item.get('biz_pbanc_nm', '')
        for keyword in ['ìŠ¤íƒ€íŠ¸ì—…', 'ì°½ì—…', 'AI', 'ë””ì§€í„¸', 'ê¸€ë¡œë²Œ']:
            if keyword in title:
                tags.append(keyword)
        
        # ì¤‘ë³µ ì œê±°
        unique_tags = list(dict.fromkeys(tags))[:10]
        return ' '.join([f'#{tag.strip()}' for tag in unique_tags if tag])
    
    def create_summary_fast(self, item, attachments, hashtags):
        """ë¹ ë¥¸ ìš”ì•½ ìƒì„±"""
        parts = []
        
        # ì œëª©
        if item.get('biz_pbanc_nm'):
            parts.append(f"ğŸ“‹ {item['biz_pbanc_nm']}")
        
        # ì£¼ê´€ê¸°ê´€
        if item.get('pbanc_ntrp_nm'):
            parts.append(f"ğŸ¢ ì£¼ê´€: {item['pbanc_ntrp_nm']}")
        
        # ê¸°ê°„
        if item.get('pbanc_rcpt_end_dt'):
            end_date = str(item['pbanc_rcpt_end_dt'])
            if len(end_date) == 8:
                end_date = f"{end_date[:4]}-{end_date[4:6]}-{end_date[6:8]}"
            parts.append(f"ğŸ“… ë§ˆê°: {end_date}")
        
        # ì²¨ë¶€íŒŒì¼
        if attachments:
            file_types = set(a['type'] for a in attachments)
            parts.append(f"ğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({len(attachments)}ê°œ)")
        
        # í•´ì‹œíƒœê·¸
        if hashtags:
            parts.append(f"ğŸ·ï¸ {hashtags}")
        
        return '\n'.join(parts)
    
    def batch_update_database(self, results: List[Dict]) -> Tuple[int, int]:
        """ë°°ì¹˜ DB ì—…ë°ì´íŠ¸"""
        success_count = 0
        error_count = 0
        
        for result in results:
            if not result['success']:
                error_count += 1
                continue
            
            try:
                update_data = {
                    'attachment_urls': result['attachments'] or [],
                    'attachment_count': len(result['attachments']),
                    'hash_tag': result['hashtags'],
                    'bsns_sumry': result['summary'],
                    'attachment_processing_status': {
                        'status': 'completed',
                        'processed_at': datetime.now().isoformat(),
                        'has_safe_filename': True,
                        'version': 'final_fast'
                    }
                }
                
                self.supabase.table('kstartup_complete').update(
                    update_data
                ).eq('id', result['id']).execute()
                
                success_count += 1
                
            except Exception as e:
                error_count += 1
                logging.error(f"DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        
        return success_count, error_count
    
    def get_unprocessed_announcements(self, limit=None):
        """ì²˜ë¦¬ ì•ˆ ëœ ê³µê³  ì¡°íšŒ"""
        try:
            query = self.supabase.table('kstartup_complete').select(
                'id', 'announcement_id', 'biz_pbanc_nm', 'detl_pg_url',
                'pbanc_ntrp_nm', 'supt_biz_clsfc', 'aply_trgt_ctnt',
                'pbanc_rcpt_bgng_dt', 'pbanc_rcpt_end_dt', 'attachment_urls'
            ).order('created_at', desc=True).limit(500)
            
            result = query.execute()
            
            unprocessed = []
            for item in result.data:
                needs_processing = False
                
                # attachment_urlsê°€ ì—†ê±°ë‚˜ unknownì´ ìˆëŠ” ê²½ìš°
                if not item.get('attachment_urls'):
                    needs_processing = True
                else:
                    urls_str = json.dumps(item['attachment_urls'])
                    if '.unknown' in urls_str or 'ì²¨ë¶€íŒŒì¼_' in urls_str:
                        needs_processing = True
                
                if needs_processing:
                    item.pop('attachment_urls', None)
                    unprocessed.append(item)
                
                if limit and len(unprocessed) >= limit:
                    break
            
            return unprocessed[:limit] if limit else unprocessed[:200]
            
        except Exception as e:
            logging.error(f"ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ê³ ì† ë³‘ë ¬ ì²˜ë¦¬)"""
        try:
            start_time = time.time()
            
            # Step 1: ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ
            unprocessed = self.get_unprocessed_announcements(limit=200)
            logging.info(f"ì²˜ë¦¬ ëŒ€ìƒ: {len(unprocessed)}ê°œ")
            
            if not unprocessed:
                logging.info("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # Step 2: ê³ ì† ë³‘ë ¬ ì²˜ë¦¬
            batch_size = 20  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
            all_results = []
            
            with ThreadPoolExecutor(max_workers=5) as executor:  # ì›Œì»¤ ì¦ê°€
                for i in range(0, len(unprocessed), batch_size):
                    batch = unprocessed[i:i+batch_size]
                    logging.info(f"\në°°ì¹˜ {i//batch_size + 1}/{(len(unprocessed)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...")
                    
                    # ë³‘ë ¬ ì‘ì—… ì‹œì‘
                    futures = {
                        executor.submit(self.process_single_item, item): item 
                        for item in batch
                    }
                    
                    # ê²°ê³¼ ìˆ˜ì§‘
                    batch_results = []
                    for future in as_completed(futures):
                        try:
                            result = future.result(timeout=20)
                            batch_results.append(result)
                            
                            if result['success']:
                                att_count = len(result['attachments'])
                                if result['attachments']:
                                    # íŒŒì¼ëª… ê°œì„  í™•ì¸
                                    improved = any(not a['safe_filename'].endswith('.unknown') 
                                                 for a in result['attachments'])
                                    status = "âœ“âœ“" if improved else "âœ“"
                                    logging.info(f"  {status} {result['biz_pbanc_nm'][:30]}... ({att_count}ê°œ)")
                            else:
                                logging.warning(f"  âœ— {result['biz_pbanc_nm'][:30]}...")
                                
                        except Exception as e:
                            logging.error(f"  âœ— ì²˜ë¦¬ ì‹¤íŒ¨")
                    
                    all_results.extend(batch_results)
                    
                    # ë°°ì¹˜ DB ì—…ë°ì´íŠ¸
                    if batch_results:
                        success, error = self.batch_update_database(batch_results)
                        logging.info(f"  ë°°ì¹˜ ê²°ê³¼: ì„±ê³µ {success}ê°œ, ì‹¤íŒ¨ {error}ê°œ")
                    
                    # ë‹¤ìŒ ë°°ì¹˜ ì „ ì§§ì€ ëŒ€ê¸°
                    if i + batch_size < len(unprocessed):
                        time.sleep(0.5)  # ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
            
            # ê²°ê³¼ ìš”ì•½
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            total_success = sum(1 for r in all_results if r['success'])
            total_error = len(all_results) - total_success
            total_attachments = sum(len(r['attachments']) for r in all_results if r['success'])
            
            # íŒŒì¼ëª… í’ˆì§ˆ í†µê³„
            improved_count = 0
            unknown_count = 0
            for r in all_results:
                if r['success'] and r['attachments']:
                    for att in r['attachments']:
                        if att.get('safe_filename'):
                            if att['safe_filename'].endswith('.unknown'):
                                unknown_count += 1
                            else:
                                improved_count += 1
            
            logging.info("\n" + "="*50)
            logging.info("ğŸ“Š K-STARTUP ì²˜ë¦¬ ê²°ê³¼")
            logging.info("="*50)
            logging.info(f"âœ… ì „ì²´: {len(all_results)}ê°œ")
            logging.info(f"âœ… ì„±ê³µ: {total_success}ê°œ")
            logging.info(f"âŒ ì‹¤íŒ¨: {total_error}ê°œ")
            logging.info(f"ğŸ“ ì²¨ë¶€íŒŒì¼: {total_attachments}ê°œ")
            logging.info(f"   - ì •ìƒ íŒŒì¼ëª…: {improved_count}ê°œ")
            logging.info(f"   - Unknown: {unknown_count}ê°œ")
            logging.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.1f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)")
            logging.info(f"âš¡ í‰ê·  ì†ë„: {len(all_results)/elapsed_time:.1f}ê°œ/ì´ˆ")
            logging.info("="*50)
            
        except Exception as e:
            logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        finally:
            # ì„¸ì…˜ ì¢…ë£Œ
            self.session.close()

if __name__ == "__main__":
    processor = KStartupCompleteProcessorFinal()
    processor.run()
