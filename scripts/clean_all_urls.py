#!/usr/bin/env python3
"""
K-Startupê³¼ BizInfo ëª¨ë‘ ì •ë¦¬ - ìˆœìˆ˜ URLë§Œ ë‚¨ê¸°ê¸°
"""
import sys
sys.stdout.reconfigure(encoding='utf-8')
import os
import json
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()

url = os.environ.get('SUPABASE_URL')
key = os.environ.get('SUPABASE_SERVICE_KEY')
supabase = create_client(url, key)

def clean_kstartup():
    """K-Startup attachment_urls ì •ë¦¬"""
    print("\n" + "="*70)
    print("ğŸ§¹ K-Startup attachment_urls ì •ë¦¬")
    print("="*70)
    
    # ëª¨ë“  attachment_urlsê°€ ìˆëŠ” ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸°
    all_data = []
    offset = 0
    page_size = 1000
    
    while True:
        batch = supabase.table('kstartup_complete')\
            .select('announcement_id, attachment_urls')\
            .not_.is_('attachment_urls', 'null')\
            .not_.eq('attachment_urls', '[]')\
            .range(offset, offset + page_size - 1)\
            .execute()
        
        if not batch.data:
            break
            
        all_data.extend(batch.data)
        
        if len(batch.data) < page_size:
            break
        offset += page_size
    
    print(f"âœ… K-Startup {len(all_data)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
    
    fixed_count = 0
    
    for record in all_data:
        announcement_id = record['announcement_id']
        attachments = record.get('attachment_urls')
        
        if isinstance(attachments, str):
            try:
                attachments = json.loads(attachments)
            except:
                continue
        
        if not isinstance(attachments, list):
            continue
        
        needs_cleaning = False
        cleaned_urls = []
        
        for att in attachments:
            if isinstance(att, dict):
                # ì´ë¯¸ ê¹¨ë—í•œ í˜•ì‹
                if list(att.keys()) == ['url']:
                    cleaned_urls.append(att)
                # ì˜ëª»ëœ í˜•ì‹
                else:
                    needs_cleaning = True
                    # download_url ìš°ì„ 
                    if 'download_url' in att:
                        cleaned_urls.append({'url': att['download_url']})
                    # url í•„ë“œ
                    elif 'url' in att:
                        cleaned_urls.append({'url': att['url']})
            elif isinstance(att, str):
                # ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ URLë¡œ ì‚¬ìš©
                needs_cleaning = True
                cleaned_urls.append({'url': att})
        
        if needs_cleaning and cleaned_urls:
            try:
                # ì¤‘ë³µ ì œê±°
                seen_urls = set()
                unique_urls = []
                for item in cleaned_urls:
                    if item['url'] not in seen_urls:
                        seen_urls.add(item['url'])
                        unique_urls.append(item)
                
                # ì—…ë°ì´íŠ¸
                result = supabase.table('kstartup_complete')\
                    .update({'attachment_urls': unique_urls})\
                    .eq('announcement_id', announcement_id)\
                    .execute()
                
                if result.data:
                    fixed_count += 1
                    if fixed_count % 50 == 0:
                        print(f"  ì²˜ë¦¬ ì¤‘... {fixed_count}ê°œ ì™„ë£Œ")
            except:
                pass
    
    print(f"âœ… K-Startup ì •ë¦¬ ì™„ë£Œ: {fixed_count}ê°œ")
    return fixed_count

def clean_bizinfo():
    """BizInfo attachment_urls ì •ë¦¬"""
    print("\n" + "="*70)
    print("ğŸ§¹ BizInfo attachment_urls ì •ë¦¬")
    print("="*70)
    
    # ëª¨ë“  attachment_urlsê°€ ìˆëŠ” ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸°
    all_data = []
    offset = 0
    page_size = 1000
    
    while True:
        batch = supabase.table('bizinfo_complete')\
            .select('pblanc_id, attachment_urls')\
            .not_.is_('attachment_urls', 'null')\
            .not_.eq('attachment_urls', '[]')\
            .range(offset, offset + page_size - 1)\
            .execute()
        
        if not batch.data:
            break
            
        all_data.extend(batch.data)
        
        if len(batch.data) < page_size:
            break
        offset += page_size
    
    print(f"âœ… BizInfo {len(all_data)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
    
    fixed_count = 0
    
    for record in all_data:
        pblanc_id = record['pblanc_id']
        attachments = record.get('attachment_urls')
        
        if isinstance(attachments, str):
            try:
                attachments = json.loads(attachments)
            except:
                continue
        
        if not isinstance(attachments, list):
            continue
        
        needs_cleaning = False
        cleaned_urls = []
        
        for att in attachments:
            if isinstance(att, dict):
                # ì´ë¯¸ ê¹¨ë—í•œ í˜•ì‹
                if list(att.keys()) == ['url']:
                    cleaned_urls.append(att)
                # ì˜ëª»ëœ í˜•ì‹
                else:
                    needs_cleaning = True
                    # download_url ìš°ì„ 
                    if 'download_url' in att:
                        cleaned_urls.append({'url': att['download_url']})
                    # url í•„ë“œ
                    elif 'url' in att:
                        cleaned_urls.append({'url': att['url']})
                    # file_idë¡œ URL ìƒì„±
                    elif 'file_id' in att:
                        file_id = att['file_id']
                        if file_id.startswith('getImageFile.do'):
                            url_str = f"https://www.bizinfo.go.kr/cmm/fms/{file_id}"
                        elif file_id.startswith('FILE_'):
                            url_str = f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={file_id}"
                        else:
                            url_str = f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?{file_id}"
                        cleaned_urls.append({'url': url_str})
            elif isinstance(att, str):
                # ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ URLë¡œ ì‚¬ìš©
                needs_cleaning = True
                cleaned_urls.append({'url': att})
        
        if needs_cleaning and cleaned_urls:
            try:
                # ì¤‘ë³µ ì œê±°
                seen_urls = set()
                unique_urls = []
                for item in cleaned_urls:
                    if item['url'] not in seen_urls:
                        seen_urls.add(item['url'])
                        unique_urls.append(item)
                
                # ì—…ë°ì´íŠ¸
                result = supabase.table('bizinfo_complete')\
                    .update({'attachment_urls': unique_urls})\
                    .eq('pblanc_id', pblanc_id)\
                    .execute()
                
                if result.data:
                    fixed_count += 1
                    if fixed_count % 50 == 0:
                        print(f"  ì²˜ë¦¬ ì¤‘... {fixed_count}ê°œ ì™„ë£Œ")
            except:
                pass
    
    print(f"âœ… BizInfo ì •ë¦¬ ì™„ë£Œ: {fixed_count}ê°œ")
    return fixed_count

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*70)
    print("ğŸ§¹ ì „ì²´ attachment_urls ì •ë¦¬ - URLë§Œ ë‚¨ê¸°ê¸°")
    print("="*70)
    
    # K-Startup ì •ë¦¬
    kstartup_fixed = clean_kstartup()
    
    # BizInfo ì •ë¦¬
    bizinfo_fixed = clean_bizinfo()
    
    print("\n" + "="*70)
    print("ğŸ“Š ì „ì²´ ì •ë¦¬ ì™„ë£Œ")
    print("="*70)
    print(f"âœ… K-Startup: {kstartup_fixed}ê°œ ì •ë¦¬")
    print(f"âœ… BizInfo: {bizinfo_fixed}ê°œ ì •ë¦¬")
    print(f"âœ… ì´í•©: {kstartup_fixed + bizinfo_fixed}ê°œ ì •ë¦¬")
    print("\nğŸ¯ ê²°ê³¼:")
    print("  - ëª¨ë“  attachment_urlsê°€ {'url': '...'} í˜•ì‹ìœ¼ë¡œ í†µì¼")
    print("  - ë‹¤ìš´ë¡œë“œ URLë§Œ ì €ì¥")
    print("  - ë¶ˆí•„ìš”í•œ í•„ë“œ ëª¨ë‘ ì œê±°")
    print("="*70)

if __name__ == "__main__":
    main()