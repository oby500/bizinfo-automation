#!/usr/bin/env python3
"""
ê¸°ì—…ë§ˆë‹¹ ì²¨ë¶€íŒŒì¼ í¬ë¡¤ëŸ¬ - ë¯¸ì²˜ë¦¬ ë°ì´í„°ë§Œ ì²˜ë¦¬
ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” ì œì™¸í•˜ê³  ì‹¤íŒ¨í•œ ê²ƒë§Œ ì¬ì²˜ë¦¬
"""
import os
import sys
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import parse_qs, urlparse
from supabase import create_client
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ì „ì—­ ë³€ìˆ˜
lock = threading.Lock()
success_count = 0
error_count = 0
attachment_total = 0
skip_count = 0

def extract_file_type(text):
    """íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¸¡"""
    text_lower = text.lower()
    if '.hwp' in text_lower or 'í•œê¸€' in text_lower:
        return 'HWP'
    elif '.pdf' in text_lower:
        return 'PDF'
    elif '.doc' in text_lower or 'word' in text_lower:
        return 'DOCX'
    elif '.xls' in text_lower or 'excel' in text_lower:
        return 'XLSX'
    elif '.zip' in text_lower or '.rar' in text_lower:
        return 'ZIP'
    elif '.png' in text_lower or '.jpg' in text_lower or '.gif' in text_lower:
        return 'IMAGE'
    elif '.ppt' in text_lower:
        return 'PPT'
    else:
        return 'UNKNOWN'

def process_item(data, idx, total, supabase):
    """ê°œë³„ í•­ëª© ì²˜ë¦¬"""
    global success_count, error_count, attachment_total, skip_count
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ì²´í¬
    current_summary = data.get('bsns_sumry', '')
    current_attachments = data.get('attachment_urls')
    
    # ì´ë¯¸ ì¶©ë¶„íˆ ì²˜ë¦¬ëœ ê²½ìš° ìŠ¤í‚µ
    if current_summary and len(current_summary) >= 150 and current_attachments:
        with lock:
            skip_count += 1
        print(f"[{idx}/{total}] â­ï¸ ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œ")
        return False
    
    # ì„¸ì…˜ ìƒì„± (ìŠ¤ë ˆë“œë³„ë¡œ ë…ë¦½ì ì¸ ì„¸ì…˜)
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    
    try:
        pblanc_id = data['pblanc_id']
        pblanc_nm = data['pblanc_nm'][:50] + "..." if len(data['pblanc_nm']) > 50 else data['pblanc_nm']
        dtl_url = data.get('dtl_url')
        
        print(f"[{idx}/{total}] {pblanc_nm}")
        
        if not dtl_url:
            print(f"  [{idx}] âš ï¸ ìƒì„¸ URL ì—†ìŒ")
            return False
        
        # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
        max_retries = 3
        for retry in range(max_retries):
            try:
                # ìƒì„¸í˜ì´ì§€ ì ‘ì†
                response = session.get(dtl_url, timeout=15)
                response.encoding = 'utf-8'
                
                if response.status_code != 200:
                    print(f"  [{idx}] âš ï¸ HTTP {response.status_code}")
                    if retry < max_retries - 1:
                        time.sleep(2)
                        continue
                    with lock:
                        error_count += 1
                    return False
                
                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
            except requests.exceptions.RequestException as e:
                if retry < max_retries - 1:
                    print(f"  [{idx}] ì¬ì‹œë„ {retry+1}/{max_retries-1}")
                    time.sleep(3)
                    continue
                print(f"  [{idx}] âŒ ì—°ê²° ì‹¤íŒ¨: {str(e)[:30]}")
                with lock:
                    error_count += 1
                return False
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        attachments = []
        
        # ë°©ë²• 1: atchFileIdê°€ ìˆëŠ” ëª¨ë“  ë§í¬ ì°¾ê¸°
        file_links = soup.find_all('a', href=lambda x: x and 'atchFileId=' in x)
        
        for link in file_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            if 'atchFileId=' in href:
                # atchFileId ì¶”ì¶œ
                atch_file_id = href.split('atchFileId=')[1].split('&')[0]
                
                # fileSn ì¶”ì¶œ (ì—†ìœ¼ë©´ 0)
                file_sn = '0'
                if 'fileSn=' in href:
                    file_sn = href.split('fileSn=')[1].split('&')[0]
                
                # íŒŒì¼ íƒ€ì… ì¶”ì¸¡
                file_type = extract_file_type(text)
                
                # ì§ì ‘ ë‹¤ìš´ë¡œë“œ URL êµ¬ì„± (ì„¸ì…˜ ì—†ì´ë„ ì ‘ê·¼ ê°€ëŠ¥)
                direct_url = f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={atch_file_id}&fileSn={file_sn}"
                
                attachment = {
                    'url': direct_url,
                    'type': file_type,
                    'safe_filename': f"{pblanc_id}_{len(attachments)+1:02d}.{file_type.lower()}",
                    'display_filename': text or f"ì²¨ë¶€íŒŒì¼_{len(attachments)+1}",
                    'original_filename': text,
                    'text': text,
                    'params': {
                        'atchFileId': atch_file_id,
                        'fileSn': file_sn
                    }
                }
                
                # ì¤‘ë³µ ì²´í¬
                is_duplicate = any(
                    a['params']['atchFileId'] == atch_file_id and 
                    a['params']['fileSn'] == file_sn 
                    for a in attachments
                )
                
                if not is_duplicate:
                    attachments.append(attachment)
        
        # ë°©ë²• 2: ì²¨ë¶€íŒŒì¼ ì˜ì—­ì—ì„œ ì¶”ê°€ ì°¾ê¸°
        if not attachments:
            file_areas = soup.find_all(['div', 'ul', 'dl'], class_=['file', 'attach', 'download'])
            for area in file_areas:
                links = area.find_all('a', href=True)
                for link in links:
                    href = link.get('href', '')
                    if 'atchFileId=' in href:
                        atch_file_id = href.split('atchFileId=')[1].split('&')[0]
                        file_sn = href.split('fileSn=')[1].split('&')[0] if 'fileSn=' in href else '0'
                        
                        attachments.append({
                            'url': f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={atch_file_id}&fileSn={file_sn}",
                            'type': 'UNKNOWN',
                            'safe_filename': f"{pblanc_id}_{len(attachments)+1:02d}.unknown",
                            'display_filename': link.get_text(strip=True) or f"ì²¨ë¶€íŒŒì¼_{len(attachments)+1}",
                            'params': {'atchFileId': atch_file_id, 'fileSn': file_sn}
                        })
        
        # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ìš”ì•½ ê°œì„ ìš©)
        content_parts = []
        
        # ë³¸ë¬¸ ë‚´ìš© ì°¾ê¸° - ë” ë§ì€ ì„ íƒì ì¶”ê°€
        content_selectors = [
            'div.view_cont', 'div.content', 'div.board_view',
            'td.content', 'td.view_cont',
            'div.bbs_cont', 'div.board_content',
            'div#content', 'div.con_view'
        ]
        
        for selector in content_selectors:
            content_area = soup.select_one(selector)
            if content_area:
                text = content_area.get_text(separator=' ', strip=True)
                if text and len(text) > 50:
                    content_parts.append(text[:1000])  # ë” ê¸´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    break
        
        # ìš”ì•½ ìƒì„±/ê°œì„  - í˜„ì¬ ìš”ì•½ì´ ë¶€ì¡±í•œ ê²½ìš°ë§Œ
        if not current_summary or len(current_summary) < 150:
            summary_parts = []
            summary_parts.append(f"ğŸ“‹ {data['pblanc_nm']}")
            
            # ë³¸ë¬¸ ë‚´ìš© ë” ìì„¸íˆ í¬í•¨
            if content_parts:
                # ê³µë°± ì •ë¦¬ ë° ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
                content_text = ' '.join(content_parts[0].split())[:400]
                summary_parts.append(f"ğŸ“ {content_text}...")
            
            # ê¸°ê°„ ì •ë³´ ì¶”ì¶œ ì‹œë„
            date_info = soup.find(text=lambda t: t and ('ì ‘ìˆ˜ê¸°ê°„' in t or 'ì‹ ì²­ê¸°ê°„' in t))
            if date_info:
                summary_parts.append(f"ğŸ“… {date_info.strip()}")
            
            if attachments:
                file_types = list(set([a['type'] for a in attachments]))
                summary_parts.append(f"ğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({len(attachments)}ê°œ)")
            
            new_summary = "\n".join(summary_parts)
        else:
            new_summary = current_summary
            # ì²¨ë¶€íŒŒì¼ ì •ë³´ë§Œ ì¶”ê°€
            if attachments and 'ğŸ“' not in current_summary:
                file_types = list(set([a['type'] for a in attachments]))
                new_summary += f"\nğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({len(attachments)}ê°œ)"
        
        # DB ì—…ë°ì´íŠ¸ - ì‹¤ì œ ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ
        update_data = {}
        
        if attachments and not current_attachments:
            update_data['attachment_urls'] = attachments
            with lock:
                attachment_total += len(attachments)
        
        if len(new_summary) > len(current_summary):
            update_data['bsns_sumry'] = new_summary
        
        if update_data:
            result = supabase.table('bizinfo_complete').update(
                update_data
            ).eq('id', data['id']).execute()
            
            with lock:
                success_count += 1
            print(f"  [{idx}] âœ… ì„±ê³µ (ì²¨ë¶€: {len(attachments)}ê°œ, ìš”ì•½: {len(new_summary)}ì)")
            return True
        else:
            with lock:
                skip_count += 1
            print(f"  [{idx}] â­ï¸ ë³€ê²½ì‚¬í•­ ì—†ìŒ")
            return False
        
    except Exception as e:
        with lock:
            error_count += 1
        print(f"  [{idx}] âŒ ì˜¤ë¥˜: {str(e)[:50]}")
        return False

def main():
    global success_count, error_count, attachment_total, skip_count
    
    print("=" * 60)
    print(" ê¸°ì—…ë§ˆë‹¹ ë¯¸ì²˜ë¦¬ ë°ì´í„° í¬ë¡¤ë§")
    print("=" * 60)
    
    # Supabase ì—°ê²°
    supabase_url = os.environ.get('SUPABASE_URL')
    supabase_key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
    
    if not supabase_url or not supabase_key:
        print("âŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    supabase = create_client(supabase_url, supabase_key)
    
    # ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ - ë¯¸ì²˜ë¦¬ ë°ì´í„°ë§Œ
    print("1. ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ ì¤‘...")
    try:
        # ì „ì²´ ë°ì´í„° ì¡°íšŒ (ìµœëŒ€ 5000ê°œ)
        all_targets = []
        offset = 0
        limit = 1000
        
        while True:
            response = supabase.table('bizinfo_complete').select(
                'id', 'pblanc_id', 'pblanc_nm', 'dtl_url', 'bsns_sumry', 'attachment_urls'
            ).range(offset, offset + limit - 1).execute()
            
            if not response.data:
                break
                
            all_targets.extend(response.data)
            offset += limit
            
            if len(all_targets) >= 5000:  # ìµœëŒ€ 5000ê°œ
                break
        
        # ë¯¸ì²˜ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
        targets = []
        already_done = 0
        for item in all_targets:
            bsns_sumry = item.get('bsns_sumry', '')
            attachment_urls = item.get('attachment_urls')
            
            # ìš”ì•½ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ì²¨ë¶€íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ë§Œ
            if (not bsns_sumry or len(bsns_sumry) < 150) or (not attachment_urls):
                targets.append(item)
            else:
                already_done += 1
        
        print(f"âœ… ì „ì²´: {len(all_targets)}ê°œ")
        print(f"âœ… ì´ë¯¸ ì²˜ë¦¬: {already_done}ê°œ")
        print(f"âœ… ì²˜ë¦¬ í•„ìš”: {len(targets)}ê°œ")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    if not targets:
        print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\n2. ì•ˆì •ì ì¸ í¬ë¡¤ë§ ì‹œì‘...")
    print(f"   - ìŠ¤ë ˆë“œ ìˆ˜: 5ê°œ (ì•ˆì •ì„± ìš°ì„ )")
    print(f"   - ì¬ì‹œë„: 3íšŒ")
    print(f"   - ì˜ˆìƒ ì‹œê°„: {len(targets) // 5 // 2}ë¶„")
    print("-" * 60)
    
    start_time = time.time()
    
    # ë°°ì¹˜ ì²˜ë¦¬ (50ê°œì”©, ë” ì‘ì€ ë°°ì¹˜)
    batch_size = 50
    for batch_start in range(0, len(targets), batch_size):
        batch_end = min(batch_start + batch_size, len(targets))
        batch = targets[batch_start:batch_end]
        
        print(f"\në°°ì¹˜ ì²˜ë¦¬: {batch_start+1}-{batch_end}/{len(targets)}")
        
        # ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ì²˜ë¦¬ (ìŠ¤ë ˆë“œ ìˆ˜ ì¤„ì„)
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for i, data in enumerate(batch, batch_start + 1):
                future = executor.submit(process_item, data, i, len(targets), supabase)
                futures.append(future)
                time.sleep(0.2)  # ìš”ì²­ ê°„ê²© ëŠ˜ë¦¼
            
            # ê²°ê³¼ ëŒ€ê¸°
            for future in as_completed(futures):
                future.result()
        
        # ë°°ì¹˜ ê°„ íœ´ì‹
        if batch_end < len(targets):
            print(f"ë°°ì¹˜ ì™„ë£Œ. 3ì´ˆ ëŒ€ê¸°...")
            time.sleep(3)
    
    elapsed_time = time.time() - start_time
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(" í¬ë¡¤ë§ ì™„ë£Œ")
    print("=" * 60)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"â­ï¸ ìŠ¤í‚µ: {skip_count}ê°œ (ì´ë¯¸ ì²˜ë¦¬ë¨)")
    print(f"âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"ğŸ“ ì²¨ë¶€íŒŒì¼: {attachment_total}ê°œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)")
    if success_count > 0:
        print(f"ğŸ“Š ì²˜ë¦¬ ì†ë„: {success_count/elapsed_time:.1f}ê°œ/ì´ˆ")
    print("=" * 60)

if __name__ == "__main__":
    main()
