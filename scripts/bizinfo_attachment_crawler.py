#!/usr/bin/env python3
"""
ê¸°ì—…ë§ˆë‹¹ ì²¨ë¶€íŒŒì¼ í¬ë¡¤ëŸ¬ - ì‹¤ì œ íŒŒì¼ íƒ€ì… ê°ì§€ ë²„ì „
HEAD ìš”ì²­ìœ¼ë¡œ Content-Typeì„ í™•ì¸í•˜ì—¬ ì •í™•í•œ íŒŒì¼ íƒ€ì… ì €ì¥
"""
import os
import sys
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import parse_qs, urlparse
from supabase import create_client
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import mimetypes

# ì „ì—­ ë³€ìˆ˜
lock = threading.Lock()
success_count = 0
error_count = 0
attachment_total = 0
skip_count = 0

def get_file_type_from_url(url, session=None):
    """HEAD ìš”ì²­ìœ¼ë¡œ ì‹¤ì œ íŒŒì¼ íƒ€ì… ê°ì§€"""
    if session is None:
        session = requests.Session()
    
    try:
        # HEAD ìš”ì²­ìœ¼ë¡œ Content-Type í™•ì¸
        response = session.head(url, timeout=5, allow_redirects=True)
        content_type = response.headers.get('Content-Type', '').lower()
        
        # Content-Typeìœ¼ë¡œ í™•ì¥ì ê²°ì •
        if 'pdf' in content_type:
            return 'PDF'
        elif 'hwp' in content_type or 'haansoft' in content_type or 'x-hwp' in content_type:
            return 'HWP'
        elif 'word' in content_type or 'msword' in content_type or 'document' in content_type:
            return 'DOCX'
        elif 'excel' in content_type or 'spreadsheet' in content_type or 'ms-excel' in content_type:
            return 'XLSX'
        elif 'powerpoint' in content_type or 'presentation' in content_type:
            return 'PPT'
        elif 'zip' in content_type or 'x-zip' in content_type or 'compressed' in content_type:
            return 'ZIP'
        elif 'image' in content_type:
            if 'jpeg' in content_type or 'jpg' in content_type:
                return 'JPG'
            elif 'png' in content_type:
                return 'PNG'
            elif 'gif' in content_type:
                return 'GIF'
            else:
                return 'IMAGE'
        elif 'text' in content_type:
            if 'plain' in content_type:
                return 'TXT'
            elif 'html' in content_type:
                return 'HTML'
            else:
                return 'TEXT'
        elif 'octet-stream' in content_type:
            # octet-streamì¸ ê²½ìš° URLì˜ í™•ì¥ìë¡œ ì¶”ì¸¡
            return guess_type_from_url(url)
        else:
            return 'UNKNOWN'
            
    except Exception as e:
        # HEAD ìš”ì²­ ì‹¤íŒ¨ ì‹œ URLì—ì„œ ì¶”ì¸¡
        return guess_type_from_url(url)

def guess_type_from_url(url):
    """URL ë˜ëŠ” íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¸¡ (í´ë°±ìš©)"""
    url_lower = url.lower()
    
    # URLì—ì„œ í™•ì¥ì ì¶”ì¶œ ì‹œë„
    if '.hwp' in url_lower or '.hwpx' in url_lower:
        return 'HWP'
    elif '.pdf' in url_lower:
        return 'PDF'
    elif '.doc' in url_lower or '.docx' in url_lower:
        return 'DOCX'
    elif '.xls' in url_lower or '.xlsx' in url_lower:
        return 'XLSX'
    elif '.ppt' in url_lower or '.pptx' in url_lower:
        return 'PPT'
    elif '.zip' in url_lower or '.rar' in url_lower or '.7z' in url_lower:
        return 'ZIP'
    elif '.jpg' in url_lower or '.jpeg' in url_lower:
        return 'JPG'
    elif '.png' in url_lower:
        return 'PNG'
    elif '.gif' in url_lower:
        return 'GIF'
    elif '.txt' in url_lower:
        return 'TXT'
    elif '.rtf' in url_lower:
        return 'RTF'
    else:
        return 'UNKNOWN'

def extract_file_type_from_text(text):
    """ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ íƒ€ì… íŒíŠ¸ ì¶”ì¶œ"""
    text_lower = text.lower()
    if 'í•œê¸€' in text_lower or 'hwp' in text_lower:
        return 'HWP'
    elif 'pdf' in text_lower:
        return 'PDF'
    elif 'word' in text_lower or 'doc' in text_lower:
        return 'DOCX'
    elif 'excel' in text_lower or 'xls' in text_lower or 'ì—‘ì…€' in text_lower:
        return 'XLSX'
    elif 'ppt' in text_lower or 'powerpoint' in text_lower or 'íŒŒì›Œí¬ì¸íŠ¸' in text_lower:
        return 'PPT'
    elif 'zip' in text_lower or 'ì••ì¶•' in text_lower:
        return 'ZIP'
    elif 'ì´ë¯¸ì§€' in text_lower or 'image' in text_lower or 'ì‚¬ì§„' in text_lower:
        return 'IMAGE'
    return None

def process_item(data, idx, total, supabase):
    """ê°œë³„ í•­ëª© ì²˜ë¦¬"""
    global success_count, error_count, attachment_total, skip_count
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ì²´í¬
    current_summary = data.get('bsns_sumry', '')
    current_attachments = data.get('attachment_urls')
    
    # ì²¨ë¶€íŒŒì¼ì´ ìˆê³  ëª¨ë“  íŒŒì¼ì´ UNKNOWNì´ ì•„ë‹Œ ê²½ìš°ë§Œ ìŠ¤í‚µ
    has_valid_types = False
    if current_attachments:
        for att in current_attachments:
            if isinstance(att, dict) and att.get('type') != 'UNKNOWN':
                has_valid_types = True
                break
    
    # ì´ë¯¸ ì¶©ë¶„íˆ ì²˜ë¦¬ëœ ê²½ìš° ìŠ¤í‚µ (ìš”ì•½ë„ ì¶©ë¶„í•˜ê³  íŒŒì¼ íƒ€ì…ë„ ì •ìƒ)
    if current_summary and len(current_summary) >= 150 and has_valid_types:
        with lock:
            skip_count += 1
        print(f"[{idx}/{total}] â­ï¸ ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œ")
        return False
    
    # ì„¸ì…˜ ìƒì„± (ìŠ¤ë ˆë“œë³„ë¡œ ë…ë¦½ì ì¸ ì„¸ì…˜)
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    
    try:
        pblanc_id = data['pblanc_id']
        pblanc_nm = data['pblanc_nm'][:50] + "..." if len(data['pblanc_nm']) > 50 else data['pblanc_nm']
        dtl_url = data.get('dtl_url')
        
        print(f"[{idx}/{total}] {pblanc_nm}")
        
        if not dtl_url:
            print(f"  [{idx}] âš ï¸ ìƒì„¸ URL ì—†ìŒ")
            return False
        
        # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
        max_retries = 3
        for retry in range(max_retries):
            try:
                # ìƒì„¸í˜ì´ì§€ ì ‘ì†
                response = session.get(dtl_url, timeout=15)
                response.encoding = 'utf-8'
                
                if response.status_code != 200:
                    print(f"  [{idx}] âš ï¸ HTTP {response.status_code}")
                    if retry < max_retries - 1:
                        time.sleep(2)
                        continue
                    with lock:
                        error_count += 1
                    return False
                
                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
            except requests.exceptions.RequestException as e:
                if retry < max_retries - 1:
                    print(f"  [{idx}] ì¬ì‹œë„ {retry+1}/{max_retries-1}")
                    time.sleep(3)
                    continue
                print(f"  [{idx}] âŒ ì—°ê²° ì‹¤íŒ¨: {str(e)[:30]}")
                with lock:
                    error_count += 1
                return False
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        attachments = []
        processed_urls = set()  # ì¤‘ë³µ ì²´í¬ìš©
        
        # ë°©ë²• 1: atchFileIdê°€ ìˆëŠ” ëª¨ë“  ë§í¬ ì°¾ê¸°
        file_links = soup.find_all('a', href=lambda x: x and 'atchFileId=' in x)
        
        for link in file_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            if 'atchFileId=' in href:
                # atchFileId ì¶”ì¶œ
                atch_file_id = href.split('atchFileId=')[1].split('&')[0]
                
                # fileSn ì¶”ì¶œ (ì—†ìœ¼ë©´ 0)
                file_sn = '0'
                if 'fileSn=' in href:
                    file_sn = href.split('fileSn=')[1].split('&')[0]
                
                # ì§ì ‘ ë‹¤ìš´ë¡œë“œ URL êµ¬ì„±
                direct_url = f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={atch_file_id}&fileSn={file_sn}"
                
                # ì¤‘ë³µ ì²´í¬
                if direct_url in processed_urls:
                    continue
                processed_urls.add(direct_url)
                
                # ì‹¤ì œ íŒŒì¼ íƒ€ì… ê°ì§€ (HEAD ìš”ì²­)
                file_type = get_file_type_from_url(direct_url, session)
                
                # ì—¬ì „íˆ UNKNOWNì´ë©´ í…ìŠ¤íŠ¸ì—ì„œ íŒíŠ¸ ì°¾ê¸°
                if file_type == 'UNKNOWN':
                    text_hint = extract_file_type_from_text(text)
                    if text_hint:
                        file_type = text_hint
                
                # íŒŒì¼ëª… ì •ë¦¬
                display_filename = text or f"ì²¨ë¶€íŒŒì¼_{len(attachments)+1}"
                safe_filename = f"{pblanc_id}_{len(attachments)+1:02d}.{file_type.lower()}"
                
                attachment = {
                    'url': direct_url,
                    'type': file_type,
                    'safe_filename': safe_filename,
                    'display_filename': display_filename,
                    'original_filename': text,
                    'text': text,
                    'params': {
                        'atchFileId': atch_file_id,
                        'fileSn': file_sn
                    }
                }
                
                attachments.append(attachment)
        
        # ë°©ë²• 2: ì²¨ë¶€íŒŒì¼ ì˜ì—­ì—ì„œ ì¶”ê°€ ì°¾ê¸°
        if not attachments:
            file_areas = soup.find_all(['div', 'ul', 'dl'], class_=['file', 'attach', 'download'])
            for area in file_areas:
                links = area.find_all('a', href=True)
                for link in links:
                    href = link.get('href', '')
                    if 'atchFileId=' in href:
                        atch_file_id = href.split('atchFileId=')[1].split('&')[0]
                        file_sn = href.split('fileSn=')[1].split('&')[0] if 'fileSn=' in href else '0'
                        
                        direct_url = f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={atch_file_id}&fileSn={file_sn}"
                        
                        if direct_url not in processed_urls:
                            processed_urls.add(direct_url)
                            file_type = get_file_type_from_url(direct_url, session)
                            
                            attachments.append({
                                'url': direct_url,
                                'type': file_type,
                                'safe_filename': f"{pblanc_id}_{len(attachments)+1:02d}.{file_type.lower()}",
                                'display_filename': link.get_text(strip=True) or f"ì²¨ë¶€íŒŒì¼_{len(attachments)+1}",
                                'params': {'atchFileId': atch_file_id, 'fileSn': file_sn}
                            })
        
        # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ìš”ì•½ ê°œì„ ìš©)
        content_parts = []
        
        # ë³¸ë¬¸ ë‚´ìš© ì°¾ê¸° - ë” ë§ì€ ì„ íƒì ì¶”ê°€
        content_selectors = [
            'div.view_cont', 'div.content', 'div.board_view',
            'td.content', 'td.view_cont',
            'div.bbs_cont', 'div.board_content',
            'div#content', 'div.con_view'
        ]
        
        for selector in content_selectors:
            content_area = soup.select_one(selector)
            if content_area:
                text = content_area.get_text(separator=' ', strip=True)
                if text and len(text) > 50:
                    content_parts.append(text[:1000])  # ë” ê¸´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    break
        
        # ìš”ì•½ ìƒì„±/ê°œì„  - í˜„ì¬ ìš”ì•½ì´ ë¶€ì¡±í•œ ê²½ìš°ë§Œ
        if not current_summary or len(current_summary) < 150:
            summary_parts = []
            summary_parts.append(f"ğŸ“‹ {data['pblanc_nm']}")
            
            # ë³¸ë¬¸ ë‚´ìš© ë” ìì„¸íˆ í¬í•¨
            if content_parts:
                # ê³µë°± ì •ë¦¬ ë° ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
                content_text = ' '.join(content_parts[0].split())[:400]
                summary_parts.append(f"ğŸ“ {content_text}...")
            
            # ê¸°ê°„ ì •ë³´ ì¶”ì¶œ ì‹œë„
            date_info = soup.find(text=lambda t: t and ('ì ‘ìˆ˜ê¸°ê°„' in t or 'ì‹ ì²­ê¸°ê°„' in t))
            if date_info:
                summary_parts.append(f"ğŸ“… {date_info.strip()}")
            
            if attachments:
                file_types = list(set([a['type'] for a in attachments]))
                summary_parts.append(f"ğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({len(attachments)}ê°œ)")
            
            new_summary = "\n".join(summary_parts)
        else:
            new_summary = current_summary
            # ì²¨ë¶€íŒŒì¼ ì •ë³´ë§Œ ì¶”ê°€
            if attachments and 'ğŸ“' not in current_summary:
                file_types = list(set([a['type'] for a in attachments]))
                new_summary += f"\nğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({len(attachments)}ê°œ)"
        
        # DB ì—…ë°ì´íŠ¸ - ì‹¤ì œ ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ
        update_data = {}
        
        # ì²¨ë¶€íŒŒì¼ì´ ì—†ê±°ë‚˜ UNKNOWNë§Œ ìˆëŠ” ê²½ìš° ì—…ë°ì´íŠ¸
        if attachments and (not current_attachments or not has_valid_types):
            update_data['attachment_urls'] = attachments
            with lock:
                attachment_total += len(attachments)
        
        if len(new_summary) > len(current_summary):
            update_data['bsns_sumry'] = new_summary
        
        if update_data:
            result = supabase.table('bizinfo_complete').update(
                update_data
            ).eq('id', data['id']).execute()
            
            with lock:
                success_count += 1
            
            # íŒŒì¼ íƒ€ì… í†µê³„ ì¶œë ¥
            if attachments:
                type_counts = {}
                for att in attachments:
                    file_type = att.get('type', 'UNKNOWN')
                    type_counts[file_type] = type_counts.get(file_type, 0) + 1
                type_info = ', '.join([f"{t}:{c}" for t, c in type_counts.items()])
                print(f"  [{idx}] âœ… ì„±ê³µ (ì²¨ë¶€: {len(attachments)}ê°œ [{type_info}], ìš”ì•½: {len(new_summary)}ì)")
            else:
                print(f"  [{idx}] âœ… ì„±ê³µ (ìš”ì•½: {len(new_summary)}ì)")
            return True
        else:
            with lock:
                skip_count += 1
            print(f"  [{idx}] â­ï¸ ë³€ê²½ì‚¬í•­ ì—†ìŒ")
            return False
        
    except Exception as e:
        with lock:
            error_count += 1
        print(f"  [{idx}] âŒ ì˜¤ë¥˜: {str(e)[:50]}")
        return False

def main():
    global success_count, error_count, attachment_total, skip_count
    
    print("=" * 60)
    print(" ê¸°ì—…ë§ˆë‹¹ ì²¨ë¶€íŒŒì¼ íƒ€ì… ë³µêµ¬ í¬ë¡¤ë§")
    print(" - HEAD ìš”ì²­ìœ¼ë¡œ ì‹¤ì œ íŒŒì¼ íƒ€ì… ê°ì§€")
    print("=" * 60)
    
    # Supabase ì—°ê²°
    supabase_url = os.environ.get('SUPABASE_URL')
    supabase_key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
    
    if not supabase_url or not supabase_key:
        print("âŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    supabase = create_client(supabase_url, supabase_key)
    
    # ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ
    print("1. ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ ì¤‘...")
    try:
        # ì „ì²´ ë°ì´í„° ì¡°íšŒ (ìµœëŒ€ 5000ê°œ)
        all_targets = []
        offset = 0
        limit = 1000
        
        while True:
            response = supabase.table('bizinfo_complete').select(
                'id', 'pblanc_id', 'pblanc_nm', 'dtl_url', 'bsns_sumry', 'attachment_urls'
            ).range(offset, offset + limit - 1).execute()
            
            if not response.data:
                break
                
            all_targets.extend(response.data)
            offset += limit
            
            if len(all_targets) >= 5000:  # ìµœëŒ€ 5000ê°œ
                break
        
        # ì²˜ë¦¬ ëŒ€ìƒ ë¶„ë¥˜
        targets = []
        unknown_count = 0
        already_done = 0
        
        for item in all_targets:
            bsns_sumry = item.get('bsns_sumry', '')
            attachment_urls = item.get('attachment_urls')
            
            # ì²¨ë¶€íŒŒì¼ì´ ìˆëŠ” ê²½ìš° UNKNOWN ì²´í¬
            has_unknown = False
            if attachment_urls:
                for att in attachment_urls:
                    if isinstance(att, dict) and att.get('type') == 'UNKNOWN':
                        has_unknown = True
                        unknown_count += 1
                        break
            
            # UNKNOWNì´ ìˆê±°ë‚˜, ìš”ì•½ì´ ë¶€ì¡±í•˜ê±°ë‚˜, ì²¨ë¶€íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
            if has_unknown or (not bsns_sumry or len(bsns_sumry) < 150) or (not attachment_urls):
                targets.append(item)
            else:
                already_done += 1
        
        print(f"âœ… ì „ì²´: {len(all_targets)}ê°œ")
        print(f"âœ… UNKNOWN íŒŒì¼ íƒ€ì…: {unknown_count}ê°œ")
        print(f"âœ… ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œ: {already_done}ê°œ")
        print(f"âœ… ì²˜ë¦¬ í•„ìš”: {len(targets)}ê°œ")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    if not targets:
        print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\n2. íŒŒì¼ íƒ€ì… ë³µêµ¬ í¬ë¡¤ë§ ì‹œì‘...")
    print(f"   - ìŠ¤ë ˆë“œ ìˆ˜: 5ê°œ (ì•ˆì •ì„± ìš°ì„ )")
    print(f"   - HEAD ìš”ì²­ìœ¼ë¡œ ì‹¤ì œ íŒŒì¼ íƒ€ì… í™•ì¸")
    print(f"   - ì˜ˆìƒ ì‹œê°„: {len(targets) // 5 // 2}ë¶„")
    print("-" * 60)
    
    start_time = time.time()
    
    # ë°°ì¹˜ ì²˜ë¦¬ (50ê°œì”©, ë” ì‘ì€ ë°°ì¹˜)
    batch_size = 50
    for batch_start in range(0, len(targets), batch_size):
        batch_end = min(batch_start + batch_size, len(targets))
        batch = targets[batch_start:batch_end]
        
        print(f"\në°°ì¹˜ ì²˜ë¦¬: {batch_start+1}-{batch_end}/{len(targets)}")
        
        # ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ì²˜ë¦¬ (ìŠ¤ë ˆë“œ ìˆ˜ ì¤„ì„)
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for i, data in enumerate(batch, batch_start + 1):
                future = executor.submit(process_item, data, i, len(targets), supabase)
                futures.append(future)
                time.sleep(0.2)  # ìš”ì²­ ê°„ê²©
            
            # ê²°ê³¼ ëŒ€ê¸°
            for future in as_completed(futures):
                future.result()
        
        # ë°°ì¹˜ ê°„ íœ´ì‹
        if batch_end < len(targets):
            print(f"ë°°ì¹˜ ì™„ë£Œ. 3ì´ˆ ëŒ€ê¸°...")
            time.sleep(3)
    
    elapsed_time = time.time() - start_time
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(" íŒŒì¼ íƒ€ì… ë³µêµ¬ ì™„ë£Œ")
    print("=" * 60)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"â­ï¸ ìŠ¤í‚µ: {skip_count}ê°œ (ì´ë¯¸ ì²˜ë¦¬ë¨)")
    print(f"âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"ğŸ“ ì²¨ë¶€íŒŒì¼: {attachment_total}ê°œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)")
    if success_count > 0:
        print(f"ğŸ“Š ì²˜ë¦¬ ì†ë„: {success_count/elapsed_time:.1f}ê°œ/ì´ˆ")
    print("=" * 60)

if __name__ == "__main__":
    main()
