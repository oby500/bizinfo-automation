#!/usr/bin/env python3
"""
ê¸°ì—…ë§ˆë‹¹ ì²¨ë¶€íŒŒì¼ í¬ë¡¤ëŸ¬ - ì›ë˜ ì‘ë™í•˜ë˜ ë°©ì‹ ë³µêµ¬
8ì›” 8ì¼ê¹Œì§€ ì •ìƒ ì‘ë™í–ˆë˜ HTTP í¬ë¡¤ë§ ë°©ì‹
"""
import os
import sys
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import parse_qs, urlparse
from supabase import create_client
from datetime import datetime

def extract_file_type(text):
    """íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¸¡"""
    text_lower = text.lower()
    if '.hwp' in text_lower or 'í•œê¸€' in text_lower:
        return 'HWP'
    elif '.pdf' in text_lower:
        return 'PDF'
    elif '.doc' in text_lower or 'word' in text_lower:
        return 'DOCX'
    elif '.xls' in text_lower or 'excel' in text_lower:
        return 'XLSX'
    elif '.zip' in text_lower or '.rar' in text_lower:
        return 'ZIP'
    elif '.png' in text_lower or '.jpg' in text_lower or '.gif' in text_lower:
        return 'IMAGE'
    elif '.ppt' in text_lower:
        return 'PPT'
    else:
        return 'UNKNOWN'

def main():
    print("=" * 60)
    print(" ê¸°ì—…ë§ˆë‹¹ ì²¨ë¶€íŒŒì¼ í¬ë¡¤ë§ (8ì›” 8ì¼ ë²„ì „ ë³µêµ¬)")
    print("=" * 60)
    
    # Supabase ì—°ê²°
    supabase_url = os.environ.get('SUPABASE_URL')
    supabase_key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
    
    if not supabase_url or not supabase_key:
        print("âŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    supabase = create_client(supabase_url, supabase_key)
    
    # ì„¸ì…˜ ìƒì„± (ì¿ í‚¤ ìœ ì§€)
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    
    # ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ - attachment_urlsê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ë°ì´í„°
    print("1. ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ ì¤‘...")
    try:
        # attachment_urlsê°€ nullì´ê±°ë‚˜ ë¹ˆ ë°°ì—´ì¸ ë°ì´í„°
        response = supabase.table('bizinfo_complete').select(
            'id', 'pblanc_id', 'pblanc_nm', 'dtl_url', 'bsns_sumry', 'attachment_urls'
        ).or_(
            'attachment_urls.is.null',
            'attachment_urls.eq.[]'
        ).limit(100).execute()
        
        targets = response.data
        
        # ì¶”ê°€ë¡œ bsns_sumryê°€ ì§§ì€ ê²ƒë„ í¬í•¨
        if len(targets) < 100:
            response2 = supabase.table('bizinfo_complete').select(
                'id', 'pblanc_id', 'pblanc_nm', 'dtl_url', 'bsns_sumry', 'attachment_urls'
            ).limit(500).execute()
            
            for item in response2.data:
                # attachment_urlsê°€ ì—†ê±°ë‚˜ bsns_sumryê°€ 150ì ë¯¸ë§Œ
                if (not item.get('attachment_urls') or item.get('attachment_urls') == []) or \
                   (item.get('bsns_sumry') and len(item.get('bsns_sumry', '')) < 150):
                    if item['id'] not in [t['id'] for t in targets]:
                        targets.append(item)
                        if len(targets) >= 100:
                            break
        
        print(f"âœ… ì²˜ë¦¬ ëŒ€ìƒ: {len(targets)}ê°œ")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        sys.exit(1)
    
    if not targets:
        print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ (ì„¸ì…˜ ì¿ í‚¤ íšë“)
    print("\n2. ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
    try:
        main_page = session.get('https://www.bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/list.do')
        print(f"âœ… ì„¸ì…˜ ì¿ í‚¤ íšë“: {len(session.cookies)}ê°œ")
    except:
        print("âš ï¸ ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")
    
    success_count = 0
    error_count = 0
    attachment_total = 0
    
    print("\n3. í¬ë¡¤ë§ ì‹œì‘...")
    print("-" * 60)
    
    for idx, data in enumerate(targets, 1):
        try:
            pblanc_id = data['pblanc_id']
            pblanc_nm = data['pblanc_nm'][:50] + "..." if len(data['pblanc_nm']) > 50 else data['pblanc_nm']
            dtl_url = data.get('dtl_url')
            
            print(f"\n[{idx}/{len(targets)}] {pblanc_nm}")
            
            if not dtl_url:
                print("  âš ï¸ ìƒì„¸ URL ì—†ìŒ")
                continue
            
            # ìƒì„¸í˜ì´ì§€ ì ‘ì†
            try:
                response = session.get(dtl_url, timeout=15)
                response.encoding = 'utf-8'
                
                if response.status_code != 200:
                    print(f"  âš ï¸ HTTP {response.status_code}")
                    error_count += 1
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                attachments = []
                
                # ë°©ë²• 1: atchFileIdê°€ ìˆëŠ” ëª¨ë“  ë§í¬ ì°¾ê¸°
                file_links = soup.find_all('a', href=lambda x: x and 'atchFileId=' in x)
                
                for link in file_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    # URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                    if 'atchFileId=' in href:
                        # atchFileId ì¶”ì¶œ
                        atch_file_id = href.split('atchFileId=')[1].split('&')[0]
                        
                        # fileSn ì¶”ì¶œ (ì—†ìœ¼ë©´ 0)
                        file_sn = '0'
                        if 'fileSn=' in href:
                            file_sn = href.split('fileSn=')[1].split('&')[0]
                        
                        # íŒŒì¼ íƒ€ì… ì¶”ì¸¡
                        file_type = extract_file_type(text)
                        
                        # ì§ì ‘ ë‹¤ìš´ë¡œë“œ URL êµ¬ì„± (ì„¸ì…˜ ì—†ì´ë„ ì ‘ê·¼ ê°€ëŠ¥)
                        direct_url = f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={atch_file_id}&fileSn={file_sn}"
                        
                        attachment = {
                            'url': direct_url,
                            'type': file_type,
                            'safe_filename': f"{pblanc_id}_{len(attachments)+1:02d}.{file_type.lower()}",
                            'display_filename': text or f"ì²¨ë¶€íŒŒì¼_{len(attachments)+1}",
                            'original_filename': text,
                            'text': text,
                            'params': {
                                'atchFileId': atch_file_id,
                                'fileSn': file_sn
                            }
                        }
                        
                        # ì¤‘ë³µ ì²´í¬
                        is_duplicate = any(
                            a['params']['atchFileId'] == atch_file_id and 
                            a['params']['fileSn'] == file_sn 
                            for a in attachments
                        )
                        
                        if not is_duplicate:
                            attachments.append(attachment)
                
                # ë°©ë²• 2: ì²¨ë¶€íŒŒì¼ ì˜ì—­ì—ì„œ ì¶”ê°€ ì°¾ê¸°
                if not attachments:
                    file_areas = soup.find_all(['div', 'ul', 'dl'], class_=['file', 'attach', 'download'])
                    for area in file_areas:
                        links = area.find_all('a', href=True)
                        for link in links:
                            href = link.get('href', '')
                            if 'atchFileId=' in href:
                                atch_file_id = href.split('atchFileId=')[1].split('&')[0]
                                file_sn = href.split('fileSn=')[1].split('&')[0] if 'fileSn=' in href else '0'
                                
                                attachments.append({
                                    'url': f"https://www.bizinfo.go.kr/cmm/fms/getImageFile.do?atchFileId={atch_file_id}&fileSn={file_sn}",
                                    'type': 'UNKNOWN',
                                    'safe_filename': f"{pblanc_id}_{len(attachments)+1:02d}.unknown",
                                    'display_filename': link.get_text(strip=True) or f"ì²¨ë¶€íŒŒì¼_{len(attachments)+1}",
                                    'params': {'atchFileId': atch_file_id, 'fileSn': file_sn}
                                })
                
                # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ìš”ì•½ ê°œì„ ìš©)
                content_parts = []
                
                # ë³¸ë¬¸ ë‚´ìš© ì°¾ê¸°
                content_areas = soup.find_all(['div', 'td'], class_=['view_cont', 'content', 'board_view'])
                for area in content_areas:
                    text = area.get_text(strip=True)
                    if text and len(text) > 50:
                        content_parts.append(text[:500])
                        break
                
                # ìš”ì•½ ìƒì„±/ê°œì„ 
                current_summary = data.get('bsns_sumry', '')
                
                if not current_summary or len(current_summary) < 150:
                    summary_parts = []
                    summary_parts.append(f"ğŸ“‹ {data['pblanc_nm']}")
                    
                    if content_parts:
                        summary_parts.append(f"ğŸ“ {content_parts[0][:200]}...")
                    
                    if attachments:
                        file_types = list(set([a['type'] for a in attachments]))
                        summary_parts.append(f"ğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({len(attachments)}ê°œ)")
                    
                    new_summary = "\n".join(summary_parts)
                else:
                    new_summary = current_summary
                    # ì²¨ë¶€íŒŒì¼ ì •ë³´ë§Œ ì¶”ê°€
                    if attachments and 'ğŸ“' not in current_summary:
                        file_types = list(set([a['type'] for a in attachments]))
                        new_summary += f"\nğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({len(attachments)}ê°œ)"
                
                # DB ì—…ë°ì´íŠ¸
                update_data = {}
                
                if attachments:
                    update_data['attachment_urls'] = attachments
                    attachment_total += len(attachments)
                
                if len(new_summary) > len(current_summary):
                    update_data['bsns_sumry'] = new_summary
                
                if update_data:
                    result = supabase.table('bizinfo_complete').update(
                        update_data
                    ).eq('id', data['id']).execute()
                    
                    success_count += 1
                    print(f"  âœ… ì—…ë°ì´íŠ¸ ì„±ê³µ (ì²¨ë¶€: {len(attachments)}ê°œ, ìš”ì•½: {len(new_summary)}ì)")
                else:
                    print(f"  â­ï¸ ì´ë¯¸ ì²˜ë¦¬ë¨")
                
            except requests.exceptions.RequestException as e:
                print(f"  âŒ HTTP ìš”ì²­ ì‹¤íŒ¨: {e}")
                error_count += 1
            
            # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)
            
        except Exception as e:
            error_count += 1
            print(f"  âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(" í¬ë¡¤ë§ ì™„ë£Œ")
    print("=" * 60)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"ğŸ“ ì²¨ë¶€íŒŒì¼: {attachment_total}ê°œ")
    print("=" * 60)

if __name__ == "__main__":
    main()