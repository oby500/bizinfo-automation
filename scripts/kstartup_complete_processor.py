#!/usr/bin/env python3
"""
K-Startup í†µí•© ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (GitHub Actionsìš©)
- ì²¨ë¶€íŒŒì¼ ë§í¬ í¬ë¡¤ë§ (safe_filename, display_filename í¬í•¨)
- í•´ì‹œíƒœê·¸ ë° ìš”ì•½ ìƒì„±
"""
import os
import sys
import time
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, parse_qs, urlparse
import re
from supabase import create_client, Client
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

class KStartupCompleteProcessor:
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # Supabase ì—°ê²°
        url = os.environ.get('SUPABASE_URL')
        key = os.environ.get('SUPABASE_KEY') or os.environ.get('SUPABASE_SERVICE_KEY')
        
        if not url or not key:
            logging.error("í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
        self.supabase: Client = create_client(url, key)
        
        # í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        logging.info("=== K-Startup í†µí•© ì²˜ë¦¬ ì‹œì‘ ===")
    
    def clean_filename(self, text):
        """íŒŒì¼ëª… ì •ë¦¬ - ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°"""
        if not text:
            return None
        
        # íŒŒì¼ëª… íŒ¨í„´: í™•ì¥ìë¥¼ í¬í•¨í•œ íŒŒì¼ëª… ì°¾ê¸°
        patterns = [
            r'([^\/\\:*?"<>|\n\r\t]+\.(?:hwp|hwpx|pdf|doc|docx|xls|xlsx|ppt|pptx|zip|jpg|jpeg|png|gif|txt|rtf))\b',
            r'([^\s]+\.(?:hwp|hwpx|pdf|doc|docx|xls|xlsx|ppt|pptx|zip|jpg|jpeg|png|gif|txt|rtf))\b'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                filename = match.group(1).strip()
                # ì²¨ë¶€íŒŒì¼, ë‹¤ìš´ë¡œë“œ ë“±ì˜ ë‹¨ì–´ ì œê±°
                filename = re.sub(r'^(ì²¨ë¶€íŒŒì¼\s*|ë‹¤ìš´ë¡œë“œ\s*)', '', filename)
                filename = re.sub(r'\s*(ë‹¤ìš´ë¡œë“œ|ì²¨ë¶€íŒŒì¼)\s*$', '', filename)
                return filename
        
        return None
    
    def create_safe_filename(self, announcement_id, index, original_filename):
        """ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„± (K-Startupìš©)"""
        if original_filename:
            # í™•ì¥ì ì¶”ì¶œ
            ext = ''
            if '.' in original_filename:
                ext = original_filename.split('.')[-1].lower()
                # í™•ì¥ìê°€ ë„ˆë¬´ ê¸¸ë©´ unknown
                if len(ext) > 10:
                    ext = 'unknown'
            else:
                ext = 'unknown'
            
            # ì•ˆì „í•œ íŒŒì¼ëª…: announcement_id_ìˆœë²ˆ.í™•ì¥ì
            safe_name = f"{announcement_id}_{index:02d}.{ext}"
            return safe_name
        
        # íŒŒì¼ëª…ì„ ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš°
        return f"{announcement_id}_{index:02d}.unknown"
    
    def get_filename_from_head_request(self, url):
        """HEAD ìš”ì²­ìœ¼ë¡œ ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ"""
        try:
            response = requests.head(url, headers=self.headers, allow_redirects=True, timeout=5)
            content_disposition = response.headers.get('Content-Disposition', '')
            
            if content_disposition:
                # filename*=UTF-8'' íŒ¨í„´
                match = re.search(r"filename\*=UTF-8''([^;]+)", content_disposition)
                if match:
                    filename = requests.utils.unquote(match.group(1))
                    return filename
                
                # filename= íŒ¨í„´
                match = re.search(r'filename="?([^"\;]+)"?', content_disposition)
                if match:
                    filename = match.group(1)
                    try:
                        filename = filename.encode('iso-8859-1').decode('utf-8')
                    except:
                        pass
                    return filename
        except:
            pass
        
        return None
    
    def extract_hashtags_from_page(self, soup):
        """í˜ì´ì§€ì—ì„œ í•´ì‹œíƒœê·¸ ì¶”ì¶œ"""
        hashtags = []
        
        try:
            # K-Startup í˜ì´ì§€ì˜ íƒœê·¸ êµ¬ì¡° ì°¾ê¸°
            # í‚¤ì›Œë“œ, ë¶„ì•¼, íƒœê·¸ ë“±
            keyword_areas = soup.find_all(['div', 'span', 'p'], class_=re.compile(r'keyword|tag|field', re.I))
            for area in keyword_areas:
                text = area.get_text(strip=True)
                if text and len(text) < 20:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                    hashtags.append(text)
            
            # í…Œì´ë¸”ì—ì„œ ë¶„ì•¼ ì •ë³´ ì°¾ê¸°
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        header = th.get_text(strip=True)
                        if 'ë¶„ì•¼' in header or 'ì—…ì¢…' in header or 'í‚¤ì›Œë“œ' in header:
                            value = td.get_text(strip=True)
                            if value and len(value) < 30:
                                tags = [t.strip() for t in value.split(',')]
                                hashtags.extend(tags[:3])
        except:
            pass
        
        return hashtags
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            # Step 1: ì²˜ë¦¬ ëŒ€ìƒ ì¡°íšŒ
            unprocessed = self.get_unprocessed_announcements()
            logging.info(f"ì²˜ë¦¬ ëŒ€ìƒ: {len(unprocessed)}ê°œ")
            
            if not unprocessed:
                logging.info("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # Step 2: ë°°ì¹˜ ì²˜ë¦¬
            success_count = 0
            attachment_count = 0
            error_count = 0
            
            for idx, item in enumerate(unprocessed, 1):
                try:
                    logging.info(f"\n[{idx}/{len(unprocessed)}] {item['biz_pbanc_nm'][:50]}...")
                    
                    # ì²¨ë¶€íŒŒì¼ í¬ë¡¤ë§
                    attachments = []
                    page_hashtags = []
                    
                    if item.get('detl_pg_url'):
                        attachments, page_hashtags = self.extract_attachments(item['announcement_id'], item['detl_pg_url'])
                        if attachments:
                            attachment_count += len(attachments)
                            logging.info(f"  â”œâ”€ ì²¨ë¶€íŒŒì¼: {len(attachments)}ê°œ")
                            for att_idx, att in enumerate(attachments, 1):
                                logging.info(f"    â””â”€ {att.get('safe_filename', '')} => {att.get('display_filename', '')}")
                    
                    # í•´ì‹œíƒœê·¸ ìƒì„± (í˜ì´ì§€ í•´ì‹œíƒœê·¸ + ìë™ ìƒì„±)
                    hashtags = self.generate_hashtags(item, page_hashtags)
                    if hashtags:
                        logging.info(f"  â”œâ”€ í•´ì‹œíƒœê·¸: {len(hashtags.split())}ê°œ")
                    
                    # ìš”ì•½ ìƒì„±
                    summary = self.create_summary(item, attachments, hashtags)
                    logging.info(f"  â”œâ”€ ìš”ì•½: {len(summary)}ì")
                    
                    # DB ì—…ë°ì´íŠ¸
                    if self.update_database(item['id'], attachments, hashtags, summary):
                        success_count += 1
                        logging.info(f"  â””â”€ âœ… ì²˜ë¦¬ ì™„ë£Œ")
                    else:
                        error_count += 1
                        logging.error(f"  â””â”€ âŒ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                    
                    # API ë¶€í•˜ ë°©ì§€
                    if idx % 10 == 0:
                        time.sleep(2)
                    else:
                        time.sleep(0.5)
                        
                except Exception as e:
                    error_count += 1
                    logging.error(f"  â””â”€ âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ê²°ê³¼ ìš”ì•½
            logging.info("\n" + "="*50)
            logging.info("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
            logging.info(f"  ì „ì²´: {len(unprocessed)}ê°œ")
            logging.info(f"  ì„±ê³µ: {success_count}ê°œ")
            logging.info(f"  ì‹¤íŒ¨: {error_count}ê°œ")
            logging.info(f"  ì²¨ë¶€íŒŒì¼: {attachment_count}ê°œ")
            logging.info("="*50)
            
        except Exception as e:
            logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def get_unprocessed_announcements(self, limit=None):
        """ì²˜ë¦¬ ì•ˆ ëœ ê³µê³  ì¡°íšŒ"""
        try:
            query = self.supabase.table('kstartup_complete').select(
                'id', 'announcement_id', 'biz_pbanc_nm', 'detl_pg_url',
                'pbanc_ntrp_nm', 'supt_biz_clsfc', 'aply_trgt_ctnt',
                'pbanc_rcpt_bgng_dt', 'pbanc_rcpt_end_dt', 'attachment_urls'
            ).order('created_at', desc=True).limit(500)
            
            result = query.execute()
            
            # attachment_urlsê°€ ì—†ê±°ë‚˜ safe_filenameì´ ì—†ëŠ” ê²ƒë§Œ í•„í„°
            unprocessed = []
            for item in result.data:
                if not item.get('attachment_urls'):
                    # attachment_urlsê°€ ë¹„ì–´ìˆìŒ
                    item.pop('attachment_urls', None)
                    unprocessed.append(item)
                else:
                    # attachment_urlsëŠ” ìˆëŠ”ë° safe_filenameì´ ì—†ëŠ” ê²½ìš°
                    urls_str = json.dumps(item['attachment_urls'])
                    if 'safe_filename' not in urls_str:
                        item.pop('attachment_urls', None)
                        unprocessed.append(item)
                
                if limit and len(unprocessed) >= limit:
                    break
            
            return unprocessed[:100]  # ìµœëŒ€ 100ê°œ
            
        except Exception as e:
            logging.error(f"ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def extract_attachments(self, announcement_id, detail_url):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ (safe_filename í¬í•¨)"""
        if not detail_url:
            return [], []
        
        try:
            # K-Startupì€ HTTP ì‚¬ìš©
            if detail_url.startswith('https://'):
                detail_url = detail_url.replace('https://', 'http://')
            
            response = requests.get(detail_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            attachments = []
            
            # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
            page_hashtags = self.extract_hashtags_from_page(soup)
            
            # ì²¨ë¶€íŒŒì¼ íŒ¨í„´ (K-Startup íŠ¹í™”)
            patterns = [
                {'regex': r'download', 'type': 'download'},
                {'regex': r'file', 'type': 'file'},
                {'regex': r'attach', 'type': 'attach'},
                {'regex': r'atch', 'type': 'atch'},
                {'regex': r'\.pdf|\.hwp|\.docx|\.xlsx|\.pptx', 'type': 'direct_file'}
            ]
            
            # ëª¨ë“  ë§í¬ ê²€ì‚¬
            all_links = soup.find_all('a', href=True)
            attachment_index = 0
            
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                onclick = link.get('onclick', '')
                title = link.get('title', '')
                
                # ì²¨ë¶€íŒŒì¼ ê´€ë ¨ ë§í¬ ì°¾ê¸°
                for pattern in patterns:
                    if re.search(pattern['regex'], href.lower() + text.lower() + onclick.lower()):
                        # onclickì—ì„œ URL ì¶”ì¶œ
                        if onclick and not href:
                            # JavaScript í•¨ìˆ˜ì—ì„œ URL ì¶”ì¶œ
                            url_match = re.search(r"['\"]([^'\"]+)['\"]", onclick)
                            if url_match:
                                href = url_match.group(1)
                        
                        if href and href != '#' and 'javascript:' not in href.lower():
                            # ì „ì²´ URL ìƒì„±
                            if not href.startswith('http'):
                                # K-Startupì€ HTTP ì‚¬ìš©
                                base_url = detail_url.replace('https://', 'http://')
                                full_url = urljoin(base_url, href)
                            else:
                                full_url = href
                            
                            # URL íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                            parsed = urlparse(full_url)
                            params = parse_qs(parsed.query)
                            
                            # íŒŒì¼ëª… ì°¾ê¸°
                            display_filename = None
                            original_filename = text or 'ì²¨ë¶€íŒŒì¼'
                            
                            # 1. ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ëª… ì°¾ê¸°
                            if text and text != 'ë‹¤ìš´ë¡œë“œ' and text != 'ì²¨ë¶€íŒŒì¼':
                                display_filename = self.clean_filename(text)
                                if display_filename:
                                    original_filename = display_filename
                            
                            # 2. title ì†ì„±ì—ì„œ ì°¾ê¸°
                            if not display_filename and title:
                                display_filename = self.clean_filename(title)
                                if display_filename:
                                    original_filename = display_filename
                            
                            # 3. hrefì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                            if not display_filename:
                                # URL ê²½ë¡œì—ì„œ íŒŒì¼ëª… ë¶€ë¶„ ì¶”ì¶œ
                                path_parts = parsed.path.split('/')
                                for part in reversed(path_parts):
                                    if '.' in part:
                                        display_filename = part
                                        original_filename = part
                                        break
                            
                            # 4. HEAD ìš”ì²­ìœ¼ë¡œ ì‹¤ì œ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
                            if not display_filename or display_filename == 'ì²¨ë¶€íŒŒì¼':
                                real_filename = self.get_filename_from_head_request(full_url)
                                if real_filename:
                                    display_filename = real_filename
                                    original_filename = real_filename
                            
                            # display_filenameì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
                            if not display_filename:
                                display_filename = f"ì²¨ë¶€íŒŒì¼_{attachment_index + 1}"
                            
                            # ì¤‘ë³µ ì²´í¬
                            if full_url not in [a['url'] for a in attachments]:
                                attachment_index += 1
                                
                                # safe_filename ìƒì„±
                                safe_filename = self.create_safe_filename(announcement_id, attachment_index, display_filename)
                                
                                # íŒŒì¼ íƒ€ì… ê²°ì •
                                file_type = self.get_file_type(display_filename, href)
                                
                                attachment = {
                                    'url': full_url,
                                    'text': 'ë‹¤ìš´ë¡œë“œ',
                                    'type': file_type,
                                    'params': {k: v[0] if len(v) == 1 else v for k, v in params.items()},
                                    'safe_filename': safe_filename,
                                    'display_filename': display_filename,
                                    'original_filename': original_filename
                                }
                                
                                attachments.append(attachment)
                        break
            
            # ì²¨ë¶€íŒŒì¼ ì˜ì—­ íŠ¹ë³„ ì²˜ë¦¬
            file_areas = soup.find_all(['div', 'td', 'ul'], class_=re.compile(r'attach|file|down', re.I))
            for area in file_areas:
                area_links = area.find_all('a', href=True)
                for link in area_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    if href and href != '#' and 'javascript:' not in href.lower():
                        if not href.startswith('http'):
                            base_url = detail_url.replace('https://', 'http://')
                            full_url = urljoin(base_url, href)
                        else:
                            full_url = href
                        
                        if full_url not in [a['url'] for a in attachments]:
                            attachment_index += 1
                            
                            display_filename = self.clean_filename(text) or f"ì²¨ë¶€íŒŒì¼_{attachment_index}"
                            safe_filename = self.create_safe_filename(announcement_id, attachment_index, display_filename)
                            
                            parsed = urlparse(full_url)
                            params = parse_qs(parsed.query)
                            
                            attachment = {
                                'url': full_url,
                                'text': 'ë‹¤ìš´ë¡œë“œ',
                                'type': self.get_file_type(display_filename, href),
                                'params': {k: v[0] if len(v) == 1 else v for k, v in params.items()},
                                'safe_filename': safe_filename,
                                'display_filename': display_filename,
                                'original_filename': text or display_filename
                            }
                            
                            attachments.append(attachment)
            
            return attachments, page_hashtags
            
        except Exception as e:
            logging.error(f"ì²¨ë¶€íŒŒì¼ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return [], []
    
    def get_file_type(self, filename, url):
        """íŒŒì¼ íƒ€ì… ì¶”ì¶œ"""
        text_lower = filename.lower() if filename else ''
        url_lower = url.lower()
        
        if any(ext in text_lower + url_lower for ext in ['.hwp', 'hwp']):
            return 'HWP'
        elif any(ext in text_lower + url_lower for ext in ['.pdf', 'pdf']):
            return 'PDF'
        elif any(ext in text_lower + url_lower for ext in ['.doc', '.docx', 'word']):
            return 'DOC'
        elif any(ext in text_lower + url_lower for ext in ['.xls', '.xlsx', 'excel']):
            return 'EXCEL'
        elif any(ext in text_lower + url_lower for ext in ['.ppt', '.pptx']):
            return 'PPT'
        elif any(ext in text_lower + url_lower for ext in ['.zip', '.rar']):
            return 'ZIP'
        elif any(ext in text_lower + url_lower for ext in ['.jpg', '.jpeg', '.png', '.gif']):
            return 'IMAGE'
        else:
            return 'FILE'
    
    def generate_hashtags(self, item, page_hashtags=None):
        """í•´ì‹œíƒœê·¸ ìƒì„± (í˜ì´ì§€ í•´ì‹œíƒœê·¸ + ìë™ ìƒì„±)"""
        tags = []
        
        # í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ í•´ì‹œíƒœê·¸ ì¶”ê°€
        if page_hashtags:
            tags.extend(page_hashtags[:5])  # ìµœëŒ€ 5ê°œ
        
        # ì§€ì›ë¶„ë¥˜ì—ì„œ ì¶”ì¶œ
        if item.get('supt_biz_clsfc'):
            field = item['supt_biz_clsfc']
            field_tags = [t.strip() for t in field.split(',') if t.strip()]
            tags.extend(field_tags[:3])  # ìµœëŒ€ 3ê°œ
        
        # ì‹ ì²­ëŒ€ìƒì—ì„œ ì¶”ì¶œ
        if item.get('aply_trgt_ctnt'):
            target = item['aply_trgt_ctnt']
            if 'ìŠ¤íƒ€íŠ¸ì—…' in target:
                tags.append('ìŠ¤íƒ€íŠ¸ì—…')
            if 'ì¤‘ì†Œê¸°ì—…' in target:
                tags.append('ì¤‘ì†Œê¸°ì—…')
            if 'ì°½ì—…' in target:
                tags.append('ì°½ì—…')
        
        # ì£¼ê´€ê¸°ê´€ (ì§§ì€ ê²ƒë§Œ)
        if item.get('pbanc_ntrp_nm'):
            org = item['pbanc_ntrp_nm'].replace('(ì£¼)', '').strip()
            if len(org) <= 10:
                tags.append(org)
        
        # ê³µê³ ëª…ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        if item.get('biz_pbanc_nm'):
            title = item['biz_pbanc_nm']
            title_keywords = ['R&D', 'AI', 'ì¸ê³µì§€ëŠ¥', 'ë¹…ë°ì´í„°', 'ë°”ì´ì˜¤', 'í™˜ê²½', 'ê·¸ë¦°',
                            'ë””ì§€í„¸', 'í˜ì‹ ', 'ê¸€ë¡œë²Œ', 'ìˆ˜ì¶œ', 'ê¸°ìˆ ê°œë°œ', 'ì‚¬ì—…í™”', 'íˆ¬ì',
                            'ì•¡ì…€ëŸ¬ë ˆì´íŒ…', 'ë©˜í† ë§', 'IR', 'ë°ëª¨ë°ì´', 'ì—‘ì…€ëŸ¬ë ˆì´í„°']
            for keyword in title_keywords:
                if keyword.lower() in title.lower():
                    tags.append(keyword)
        
        # ì¤‘ë³µ ì œê±° ë° í•´ì‹œíƒœê·¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        unique_tags = list(dict.fromkeys(tags))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
        hashtags = ' '.join([f'#{tag.strip()}' for tag in unique_tags[:10]])  # ìµœëŒ€ 10ê°œ
        
        return hashtags
    
    def create_summary(self, item, attachments, hashtags):
        """ìš”ì•½ ìƒì„±"""
        summary_parts = []
        
        # ê³µê³ ëª…
        if item.get('biz_pbanc_nm'):
            summary_parts.append(f"ğŸ“‹ {item['biz_pbanc_nm']}")
        
        # ì£¼ê´€ê¸°ê´€
        if item.get('pbanc_ntrp_nm'):
            summary_parts.append(f"ğŸ¢ ì£¼ê´€: {item['pbanc_ntrp_nm']}")
        
        # ì‹ ì²­ê¸°ê°„ ë° D-Day
        if item.get('pbanc_rcpt_bgng_dt') and item.get('pbanc_rcpt_end_dt'):
            start_date = item['pbanc_rcpt_bgng_dt']
            end_date = item['pbanc_rcpt_end_dt']
            
            # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYYMMDD -> YYYY-MM-DD)
            if start_date and len(start_date) == 8:
                start_date = f"{start_date[:4]}-{start_date[4:6]}-{start_date[6:8]}"
            if end_date and len(end_date) == 8:
                end_date = f"{end_date[:4]}-{end_date[4:6]}-{end_date[6:8]}"
            
            summary_parts.append(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
            
            # D-Day ê³„ì‚°
            try:
                if end_date:
                    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                    days_left = (end_dt - datetime.now()).days
                    
                    if 0 <= days_left <= 3:
                        summary_parts.append(f"ğŸš¨ ë§ˆê°ì„ë°• D-{days_left}")
                    elif 4 <= days_left <= 7:
                        summary_parts.append(f"â° D-{days_left}")
                    elif days_left > 0:
                        summary_parts.append(f"ğŸ“† D-{days_left}")
            except:
                pass
        
        # ì§€ì›ë¶„ì•¼
        if item.get('supt_biz_clsfc'):
            summary_parts.append(f"ğŸ¯ ë¶„ì•¼: {item['supt_biz_clsfc']}")
        
        # ì‹ ì²­ëŒ€ìƒ
        if item.get('aply_trgt_ctnt'):
            target = item['aply_trgt_ctnt'][:100]
            summary_parts.append(f"ğŸ‘¥ ëŒ€ìƒ: {target}...")
        
        # ì²¨ë¶€íŒŒì¼
        if attachments:
            file_types = list(set([a['type'] for a in attachments]))
            summary_parts.append(f"ğŸ“ ì²¨ë¶€: {', '.join(file_types)} ({len(attachments)}ê°œ)")
        
        # í•´ì‹œíƒœê·¸
        if hashtags:
            summary_parts.append(f"ğŸ·ï¸ {hashtags}")
        
        return '\n'.join(summary_parts)
    
    def update_database(self, record_id, attachments, hashtags, summary):
        """DB ì—…ë°ì´íŠ¸"""
        try:
            update_data = {
                'attachment_urls': attachments if attachments else [],
                'attachment_count': len(attachments) if attachments else 0,
                'hash_tag': hashtags,
                'bsns_sumry': summary,
                'attachment_processing_status': {
                    'status': 'completed',
                    'processed_at': datetime.now().isoformat(),
                    'has_safe_filename': True
                }
            }
            
            result = self.supabase.table('kstartup_complete').update(
                update_data
            ).eq('id', record_id).execute()
            
            return bool(result.data)
            
        except Exception as e:
            logging.error(f"DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return False

if __name__ == "__main__":
    processor = KStartupCompleteProcessor()
    processor.run()
