#!/usr/bin/env python3
"""
K-Startup ë°ì´í„° í’ˆì§ˆ ê°œì„  ìŠ¤í¬ë¦½íŠ¸
- ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ìœ¼ë¡œ ì²¨ë¶€íŒŒì¼ ìˆ˜ì§‘
- AI ê¸°ë°˜ ìš”ì•½ ì¬ìƒì„±
- í•´ì‹œíƒœê·¸ ìë™ ìƒì„±
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
from supabase import create_client
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

logging.basicConfig(level=logging.INFO)

class KStartupEnhancer:
    def __init__(self):
        url = os.environ.get('SUPABASE_URL')
        key = os.environ.get('SUPABASE_SERVICE_KEY') or os.environ.get('SUPABASE_KEY')
        
        if not url or not key:
            logging.error("í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”")
            sys.exit(1)
            
        self.supabase = create_client(url, key)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_poor_quality_data(self, limit=100):
        """í’ˆì§ˆì´ ë‚®ì€ ë°ì´í„° ì¡°íšŒ"""
        logging.info("í’ˆì§ˆ ê°œì„  í•„ìš” ë°ì´í„° ì¡°íšŒ ì¤‘...")
        
        # 1. ì²¨ë¶€íŒŒì¼ ì—†ëŠ” ë°ì´í„°
        no_attach = self.supabase.table('kstartup_complete')\
            .select('id,announcement_id,biz_pbanc_nm,detl_pg_url')\
            .or_('attachment_urls.is.null,attachment_urls.eq.[]')\
            .limit(limit)\
            .execute()
        
        # 2. ìš”ì•½ì´ ë¶€ì‹¤í•œ ë°ì´í„°
        poor_summary = self.supabase.table('kstartup_complete')\
            .select('id,announcement_id,biz_pbanc_nm,detl_pg_url,bsns_sumry')\
            .or_('bsns_sumry.is.null,bsns_sumry.eq.')\
            .limit(limit)\
            .execute()
        
        # ì¤‘ë³µ ì œê±°í•˜ì—¬ ë³‘í•©
        all_ids = set()
        items_to_process = []
        
        for item in (no_attach.data or []) + (poor_summary.data or []):
            if item['id'] not in all_ids:
                all_ids.add(item['id'])
                items_to_process.append(item)
        
        logging.info(f"ê°œì„  ëŒ€ìƒ: {len(items_to_process)}ê°œ")
        return items_to_process[:limit]
    
    def crawl_detail_page(self, item):
        """ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            url = item.get('detl_pg_url')
            if not url:
                return None
            
            # URL ì •ë¦¬
            if not url.startswith('http'):
                url = f"https://www.k-startup.go.kr{url}"
            
            response = self.session.get(url, timeout=10)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            result = {
                'id': item['id'],
                'attachments': [],
                'content': '',
                'details': {}
            }
            
            # 1. ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
            file_areas = soup.find_all(['div', 'ul', 'dl'], class_=['file', 'attach', 'download'])
            for area in file_areas:
                links = area.find_all('a')
                for idx, link in enumerate(links, 1):
                    href = link.get('href', '')
                    filename = link.get_text(strip=True) or f"attachment_{idx}"
                    
                    if href and not href.startswith('#'):
                        # K-Startup íŠ¹ìˆ˜ ë‹¤ìš´ë¡œë“œ URL ì²˜ë¦¬
                        if 'fileDown' in href or 'download' in href:
                            file_id = self.extract_file_id(href)
                            if file_id:
                                result['attachments'].append({
                                    'url': f"https://www.k-startup.go.kr/web/module/download.do?fileName={file_id}",
                                    'filename': filename,
                                    'safe_filename': f"KS_{item['announcement_id']}_{idx:02d}.{self.get_extension(filename)}",
                                    'display_filename': filename
                                })
            
            # 2. ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
            content_area = soup.find(['div', 'section'], class_=['content', 'detail', 'view'])
            if content_area:
                # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
                tables = content_area.find_all('table')
                for table in tables:
                    rows = table.find_all('tr')
                    for row in rows:
                        th = row.find('th')
                        td = row.find('td')
                        if th and td:
                            key = th.get_text(strip=True)
                            value = td.get_text(strip=True)
                            if key and value:
                                result['details'][key] = value
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸
                result['content'] = content_area.get_text(strip=True)[:2000]
            
            return result
            
        except Exception as e:
            logging.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜ (ID: {item['id']}): {e}")
            return None
    
    def extract_file_id(self, url):
        """íŒŒì¼ ID ì¶”ì¶œ"""
        import re
        
        patterns = [
            r'fileName=([^&]+)',
            r'fileId=([^&]+)',
            r'atchFileId=([^&]+)',
            r'file/([^/]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None
    
    def get_extension(self, filename):
        """í™•ì¥ì ì¶”ì¶œ"""
        if '.' in filename:
            ext = filename.split('.')[-1].lower()
            if ext in ['pdf', 'hwp', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx', 'zip']:
                return ext
        return 'unknown'
    
    def generate_enhanced_summary(self, item, crawled_data):
        """ê°œì„ ëœ ìš”ì•½ ìƒì„±"""
        title = item.get('biz_pbanc_nm', '')
        content = crawled_data.get('content', '') if crawled_data else ''
        details = crawled_data.get('details', {}) if crawled_data else {}
        
        # í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        summary_parts = []
        
        # ì œëª©ì—ì„œ í•µì‹¬ ì¶”ì¶œ
        if title and title not in ['ëª¨ì§‘ì¤‘', 'URLë³µì‚¬', 'í™ˆí˜ì´ì§€ ë°”ë¡œê°€ê¸°']:
            summary_parts.append(f"ğŸ“‹ {title}")
        
        # ì§€ì› ëŒ€ìƒ
        if 'ì§€ì›ëŒ€ìƒ' in details:
            summary_parts.append(f"ğŸ‘¥ ëŒ€ìƒ: {details['ì§€ì›ëŒ€ìƒ'][:100]}")
        
        # ì§€ì› ë‚´ìš©
        if 'ì§€ì›ë‚´ìš©' in details:
            summary_parts.append(f"ğŸ’° ì§€ì›: {details['ì§€ì›ë‚´ìš©'][:100]}")
        elif 'ì‚¬ì—…ë‚´ìš©' in details:
            summary_parts.append(f"ğŸ’° ë‚´ìš©: {details['ì‚¬ì—…ë‚´ìš©'][:100]}")
        
        # ì‹ ì²­ ê¸°ê°„
        if 'ì‹ ì²­ê¸°ê°„' in details:
            summary_parts.append(f"ğŸ“… ê¸°ê°„: {details['ì‹ ì²­ê¸°ê°„']}")
        elif 'ì ‘ìˆ˜ê¸°ê°„' in details:
            summary_parts.append(f"ğŸ“… ì ‘ìˆ˜: {details['ì ‘ìˆ˜ê¸°ê°„']}")
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ
        if len(summary_parts) < 2 and content:
            summary_parts.append(content[:200] + "...")
        
        return '\n'.join(summary_parts) if summary_parts else f"ğŸ“‹ {title}"
    
    def generate_hashtags(self, item, crawled_data):
        """í•´ì‹œíƒœê·¸ ìë™ ìƒì„±"""
        title = item.get('biz_pbanc_nm', '')
        content = (crawled_data.get('content', '') if crawled_data else '')[:500]
        
        hashtags = []
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ í•´ì‹œíƒœê·¸
        keywords = {
            'ì°½ì—…': '#ì°½ì—…ì§€ì›',
            'ìŠ¤íƒ€íŠ¸ì—…': '#ìŠ¤íƒ€íŠ¸ì—…',
            'R&D': '#ì—°êµ¬ê°œë°œ',
            'ê¸°ìˆ ': '#ê¸°ìˆ ê°œë°œ',
            'íˆ¬ì': '#íˆ¬ììœ ì¹˜',
            'ìˆ˜ì¶œ': '#ìˆ˜ì¶œì§€ì›',
            'ë§ˆì¼€íŒ…': '#ë§ˆì¼€íŒ…ì§€ì›',
            'êµìœ¡': '#êµìœ¡í”„ë¡œê·¸ë¨',
            'ë©˜í† ë§': '#ë©˜í† ë§',
            'ì‚¬ì—…í™”': '#ì‚¬ì—…í™”ì§€ì›',
            'ì‹œì œí’ˆ': '#ì‹œì œí’ˆì œì‘',
            'íŠ¹í—ˆ': '#íŠ¹í—ˆì§€ì›',
            'ì¸ì¦': '#ì¸ì¦ì§€ì›',
            'ì»¨ì„¤íŒ…': '#ì»¨ì„¤íŒ…',
            'ìê¸ˆ': '#ìê¸ˆì§€ì›',
            'ë³´ì¦': '#ë³´ì¦ì§€ì›',
            'ëŒ€ì¶œ': '#ì •ì±…ìê¸ˆ'
        }
        
        text = (title + ' ' + content).lower()
        
        for keyword, tag in keywords.items():
            if keyword.lower() in text:
                hashtags.append(tag)
        
        # ê¸°ë³¸ íƒœê·¸
        if not hashtags:
            hashtags = ['#ì •ë¶€ì§€ì›', '#KìŠ¤íƒ€íŠ¸ì—…']
        
        # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
        return hashtags[:5]
    
    def process_batch(self, items):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        logging.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(items)}ê°œ")
        
        processed = 0
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(self.crawl_detail_page, item): item 
                      for item in items}
            
            for future in as_completed(futures):
                item = futures[future]
                try:
                    crawled = future.result(timeout=15)
                    
                    # ì—…ë°ì´íŠ¸í•  ë°ì´í„° ì¤€ë¹„
                    update_data = {}
                    
                    # ì²¨ë¶€íŒŒì¼ ì—…ë°ì´íŠ¸
                    if crawled and crawled.get('attachments'):
                        update_data['attachment_urls'] = crawled['attachments']
                        update_data['attachment_count'] = len(crawled['attachments'])
                    
                    # ìš”ì•½ ê°œì„ 
                    enhanced_summary = self.generate_enhanced_summary(item, crawled)
                    if enhanced_summary and len(enhanced_summary) > 20:
                        update_data['bsns_sumry'] = enhanced_summary
                    
                    # í•´ì‹œíƒœê·¸ ìƒì„±
                    hashtags = self.generate_hashtags(item, crawled)
                    if hashtags:
                        update_data['hash_tag'] = hashtags
                    
                    # ìƒì„¸ ë‚´ìš© ì—…ë°ì´íŠ¸
                    if crawled and crawled.get('content'):
                        update_data['pbanc_ctnt'] = crawled['content']
                    
                    # DB ì—…ë°ì´íŠ¸
                    if update_data:
                        update_data['updated_at'] = datetime.now().isoformat()
                        update_data['quality_enhanced'] = True
                        
                        self.supabase.table('kstartup_complete')\
                            .update(update_data)\
                            .eq('id', item['id'])\
                            .execute()
                        
                        processed += 1
                        logging.info(f"âœ… ID {item['id']} ê°œì„  ì™„ë£Œ")
                    
                except Exception as e:
                    logging.error(f"ì²˜ë¦¬ ì˜¤ë¥˜ (ID: {item['id']}): {e}")
        
        return processed
    
    def run(self, limit=100):
        """ë©”ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        logging.info("="*60)
        logging.info("   K-Startup ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œì‘")
        logging.info("="*60)
        
        # 1. ê°œì„  í•„ìš” ë°ì´í„° ì¡°íšŒ
        items = self.get_poor_quality_data(limit)
        
        if not items:
            logging.info("ê°œì„ í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬
        batch_size = 20
        total_processed = 0
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i+batch_size]
            processed = self.process_batch(batch)
            total_processed += processed
            
            logging.info(f"ì§„í–‰: {total_processed}/{len(items)}")
        
        # 3. ê²°ê³¼ í†µê³„
        elapsed = time.time() - start_time
        
        logging.info("\n" + "="*60)
        logging.info("   ì²˜ë¦¬ ì™„ë£Œ")
        logging.info("="*60)
        logging.info(f"âœ… ê°œì„  ì™„ë£Œ: {total_processed}ê°œ")
        logging.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        logging.info(f"ğŸ“Š í‰ê·  ì†ë„: {total_processed/elapsed:.1f}ê°œ/ì´ˆ")
        
        # 4. ê°œì„  í›„ í’ˆì§ˆ í™•ì¸
        self.check_quality_after()
    
    def check_quality_after(self):
        """ê°œì„  í›„ í’ˆì§ˆ í™•ì¸"""
        stats = self.supabase.table('kstartup_complete')\
            .select('id', count='exact')\
            .eq('quality_enhanced', True)\
            .execute()
        
        if stats.count:
            logging.info(f"\nâœ¨ í’ˆì§ˆ ê°œì„ ëœ ë°ì´í„°: {stats.count}ê°œ")

if __name__ == "__main__":
    enhancer = KStartupEnhancer()
    enhancer.run(limit=50)  # ë¨¼ì € 50ê°œë§Œ í…ŒìŠ¤íŠ¸
