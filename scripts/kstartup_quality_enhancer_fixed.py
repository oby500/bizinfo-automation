#!/usr/bin/env python3
"""
K-Startup ë°ì´í„° í’ˆì§ˆ ê°œì„  ìŠ¤í¬ë¦½íŠ¸ (ìˆ˜ì • ë²„ì „)
- ìƒì„¸ í˜ì´ì§€ì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ
- unknown í™•ì¥ì ë¬¸ì œ í•´ê²°
- ìš”ì•½ í’ˆì§ˆ ê°œì„ 
- í•´ì‹œíƒœê·¸ ì •ìƒí™”
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
from supabase import create_client
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

logging.basicConfig(level=logging.INFO)

class KStartupEnhancer:
    def __init__(self):
        url = os.environ.get('SUPABASE_URL')
        key = os.environ.get('SUPABASE_SERVICE_KEY') or os.environ.get('SUPABASE_KEY')
        
        if not url or not key:
            logging.error("í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”")
            sys.exit(1)
            
        self.supabase = create_client(url, key)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_data_to_enhance(self, limit=100):
        """ê°œì„ ì´ í•„ìš”í•œ ë°ì´í„° ì¡°íšŒ"""
        logging.info("í’ˆì§ˆ ê°œì„  í•„ìš” ë°ì´í„° ì¡°íšŒ ì¤‘...")
        
        # 1. unknown í™•ì¥ìë¥¼ ê°€ì§„ ë°ì´í„°
        unknown_ext = self.supabase.table('kstartup_complete')\
            .select('*')\
            .like('attachment_urls', '%unknown%')\
            .limit(limit)\
            .execute()
        
        # 2. í’ˆì§ˆ ë‚®ì€ ìš”ì•½ ë°ì´í„°
        poor_summary = self.supabase.table('kstartup_complete')\
            .select('*')\
            .or_('bsns_sumry.like.%ëª¨ì§‘ì¤‘%,bsns_sumry.like.%URLë³µì‚¬%,bsns_sumry.like.%í™ˆí˜ì´ì§€%')\
            .limit(limit)\
            .execute()
        
        # 3. í•´ì‹œíƒœê·¸ ì—†ëŠ” ë°ì´í„°
        no_hashtag = self.supabase.table('kstartup_complete')\
            .select('*')\
            .or_('hash_tag.is.null,hash_tag.eq.')\
            .limit(limit)\
            .execute()
        
        # ì¤‘ë³µ ì œê±°í•˜ì—¬ ë³‘í•©
        all_ids = set()
        items_to_process = []
        
        for item in (unknown_ext.data or []) + (poor_summary.data or []) + (no_hashtag.data or []):
            if item['id'] not in all_ids:
                all_ids.add(item['id'])
                items_to_process.append(item)
        
        logging.info(f"ê°œì„  ëŒ€ìƒ: {len(items_to_process)}ê°œ")
        return items_to_process[:limit]
    
    def crawl_detail_page(self, item):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
        try:
            # detl_pg_urlì´ ì—†ìœ¼ë©´ ìƒì„±
            detail_url = item.get('detl_pg_url')
            if not detail_url and item.get('announcement_id'):
                # K-Startup IDì—ì„œ ì‹¤ì œ ID ì¶”ì¶œ
                ann_id = item['announcement_id']
                if ann_id.startswith('KS_'):
                    # KS_174371 í˜•ì‹ì—ì„œ 174371 ì¶”ì¶œ
                    real_id = ann_id.split('_')[1] if '_' in ann_id else ann_id
                    detail_url = f"http://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do?schM=view&pbancSn={real_id}&page=1&schStr=&pbancEndYn=Y"
            
            if not detail_url:
                return None
            
            # URL ì •ë¦¬
            if not detail_url.startswith('http'):
                detail_url = f"http://www.k-startup.go.kr{detail_url}"
            
            response = self.session.get(detail_url, timeout=10)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            result = {
                'id': item['id'],
                'attachments': [],
                'content': '',
                'details': {}
            }
            
            # 1. ì²¨ë¶€íŒŒì¼ ì˜ì—­ì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ
            # K-Startupì€ file_bg í´ë˜ìŠ¤ ë‚´ì˜ title ì†ì„±ì— ì‹¤ì œ íŒŒì¼ëª…ì´ ìˆìŒ
            file_elements = soup.find_all('span', class_='file_bg')
            for idx, elem in enumerate(file_elements, 1):
                title = elem.get('title', '')
                if title:
                    # ì‹¤ì œ íŒŒì¼ëª…ê³¼ í™•ì¥ì ì¶”ì¶œ
                    file_ext = self.extract_extension(title)
                    safe_name = f"KS_{item['announcement_id']}_{idx:02d}.{file_ext}"
                    
                    # ê¸°ì¡´ attachment_urlsì—ì„œ URL ì°¾ê¸°
                    existing_urls = item.get('attachment_urls', [])
                    if idx <= len(existing_urls):
                        url = existing_urls[idx-1].get('url', '')
                        result['attachments'].append({
                            'url': url,
                            'safe_filename': safe_name,
                            'display_filename': title,
                            'original_filename': title
                        })
            
            # 2. í…Œì´ë¸”ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            detail_table = soup.find('table', class_='view_tbl')
            if detail_table:
                rows = detail_table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.get_text(strip=True)
                        value = td.get_text(strip=True)
                        if key and value:
                            result['details'][key] = value
            
            # 3. ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content_area = soup.find('div', class_='view_cont')
            if content_area:
                result['content'] = content_area.get_text(strip=True)[:2000]
            
            return result
            
        except Exception as e:
            logging.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜ (ID: {item['id']}): {e}")
            return None
    
    def extract_extension(self, filename):
        """íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¶œ"""
        if '.' in filename:
            ext = filename.split('.')[-1].lower()
            # ì¼ë°˜ì ì¸ ë¬¸ì„œ í™•ì¥ì ê²€ì¦
            valid_exts = ['pdf', 'hwp', 'hwpx', 'doc', 'docx', 'xls', 'xlsx', 
                         'ppt', 'pptx', 'zip', 'jpg', 'jpeg', 'png', 'gif']
            if ext in valid_exts:
                return ext
        # ê¸°ë³¸ê°’
        return 'pdf'  # K-Startupì€ ëŒ€ë¶€ë¶„ PDF
    
    def generate_enhanced_summary(self, item, crawled_data):
        """ê°œì„ ëœ ìš”ì•½ ìƒì„±"""
        title = item.get('biz_pbanc_nm', '')
        details = crawled_data.get('details', {}) if crawled_data else {}
        content = crawled_data.get('content', '') if crawled_data else item.get('pbanc_ctnt', '')
        
        # ë¬´ì˜ë¯¸í•œ ì œëª© í•„í„°ë§
        invalid_titles = ['ëª¨ì§‘ì¤‘', 'URLë³µì‚¬', 'í™ˆí˜ì´ì§€ ë°”ë¡œê°€ê¸°', 'ëª¨ì§‘ë§ˆê°']
        if title in invalid_titles:
            # ì‹¤ì œ ì œëª© ì°¾ê¸°
            if details.get('ì‚¬ì—…ëª…'):
                title = details['ì‚¬ì—…ëª…']
            elif details.get('ê³µê³ ëª…'):
                title = details['ê³µê³ ëª…']
        
        summary_parts = []
        
        # ì œëª©
        if title:
            summary_parts.append(f"ğŸ“‹ {title}")
        
        # ì£¼ê´€ê¸°ê´€
        org = item.get('pbanc_ntrp_nm') or details.get('ì£¼ê´€ê¸°ê´€') or details.get('ìš´ì˜ê¸°ê´€')
        if org:
            summary_parts.append(f"ğŸ¢ ì£¼ê´€: {org}")
        
        # ì§€ì›ëŒ€ìƒ
        target = item.get('aply_trgt_ctnt') or details.get('ì§€ì›ëŒ€ìƒ') or details.get('ì‹ ì²­ìê²©')
        if target:
            target_text = target[:100] + "..." if len(target) > 100 else target
            summary_parts.append(f"ğŸ‘¥ ëŒ€ìƒ: {target_text}")
        
        # ì§€ì›ë‚´ìš©
        support = details.get('ì§€ì›ë‚´ìš©') or details.get('ì‚¬ì—…ë‚´ìš©') or details.get('ì§€ì›ê·œëª¨')
        if support:
            support_text = support[:100] + "..." if len(support) > 100 else support
            summary_parts.append(f"ğŸ’° ì§€ì›: {support_text}")
        
        # ì‹ ì²­ê¸°ê°„
        start_date = item.get('pbanc_rcpt_bgng_dt')
        end_date = item.get('pbanc_rcpt_end_dt')
        if end_date:
            summary_parts.append(f"ğŸ“… ë§ˆê°: {end_date}")
        elif details.get('ì‹ ì²­ê¸°ê°„'):
            summary_parts.append(f"ğŸ“… ê¸°ê°„: {details['ì‹ ì²­ê¸°ê°„']}")
        
        # ì²¨ë¶€íŒŒì¼ ê°œìˆ˜
        attach_count = len(crawled_data.get('attachments', [])) if crawled_data else item.get('attachment_count', 0)
        if attach_count > 0:
            summary_parts.append(f"ğŸ“ ì²¨ë¶€: {attach_count}ê°œ")
        
        return '\n'.join(summary_parts) if summary_parts else f"ğŸ“‹ {title}"
    
    def generate_hashtags(self, item, crawled_data):
        """í•´ì‹œíƒœê·¸ ìƒì„± (ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ)"""
        title = item.get('biz_pbanc_nm', '')
        content = (crawled_data.get('content', '') if crawled_data else item.get('pbanc_ctnt', ''))[:1000]
        org = item.get('pbanc_ntrp_nm', '')
        
        hashtags = []
        
        # ë¶„ì•¼ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        keyword_map = {
            'ì°½ì—…': '#ì°½ì—…',
            'ìŠ¤íƒ€íŠ¸ì—…': '#ìŠ¤íƒ€íŠ¸ì—…',
            'R&D': '#ì—°êµ¬ê°œë°œ',
            'ê¸°ìˆ ': '#ê¸°ìˆ ê°œë°œ',
            'íˆ¬ì': '#íˆ¬ììœ ì¹˜',
            'ìˆ˜ì¶œ': '#ìˆ˜ì¶œì§€ì›',
            'ë§ˆì¼€íŒ…': '#ë§ˆì¼€íŒ…',
            'êµìœ¡': '#êµìœ¡',
            'ë©˜í† ë§': '#ë©˜í† ë§',
            'ì»¨ì„¤íŒ…': '#ì»¨ì„¤íŒ…',
            'ì‚¬ì—…í™”': '#ì‚¬ì—…í™”',
            'ì‹œì œí’ˆ': '#ì‹œì œí’ˆì œì‘',
            'íŠ¹í—ˆ': '#íŠ¹í—ˆ',
            'ì¸ì¦': '#ì¸ì¦ì§€ì›',
            'ìê¸ˆ': '#ìê¸ˆì§€ì›',
            'ë³´ì¦': '#ë³´ì¦',
            'ëŒ€ì¶œ': '#ì •ì±…ìê¸ˆ',
            'AI': '#ì¸ê³µì§€ëŠ¥',
            'ë¹…ë°ì´í„°': '#ë¹…ë°ì´í„°',
            'ë¸”ë¡ì²´ì¸': '#ë¸”ë¡ì²´ì¸',
            'ë°”ì´ì˜¤': '#ë°”ì´ì˜¤',
            'í™˜ê²½': '#ê·¸ë¦°ë‰´ë”œ',
            'ì—ë„ˆì§€': '#ì—ë„ˆì§€',
            'ë¬¸í™”': '#ë¬¸í™”ì½˜í…ì¸ '
        }
        
        text = (title + ' ' + content + ' ' + org).lower()
        
        for keyword, tag in keyword_map.items():
            if keyword.lower() in text:
                if tag not in hashtags:
                    hashtags.append(tag)
        
        # ê¸°ê´€ëª… í•´ì‹œíƒœê·¸
        if org and len(org) < 20:
            org_tag = f"#{org.replace(' ', '')}"
            if org_tag not in hashtags:
                hashtags.append(org_tag)
        
        # ê¸°ë³¸ íƒœê·¸
        if not hashtags:
            hashtags = ['#ì •ë¶€ì§€ì›ì‚¬ì—…', '#KìŠ¤íƒ€íŠ¸ì—…']
        
        # ìµœëŒ€ 5ê°œ, ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜
        return ' '.join(hashtags[:5])
    
    def process_item(self, item):
        """ê°œë³„ ì•„ì´í…œ ì²˜ë¦¬"""
        try:
            # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
            crawled = self.crawl_detail_page(item)
            
            update_data = {}
            
            # 1. ì²¨ë¶€íŒŒì¼ ì—…ë°ì´íŠ¸ (unknown í™•ì¥ì ìˆ˜ì •)
            if crawled and crawled.get('attachments'):
                update_data['attachment_urls'] = crawled['attachments']
                update_data['attachment_count'] = len(crawled['attachments'])
            elif item.get('attachment_urls'):
                # í¬ë¡¤ë§ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ unknown íŒŒì¼ëª…ë§Œ ê°œì„ 
                fixed_attachments = []
                for idx, att in enumerate(item['attachment_urls'], 1):
                    if isinstance(att, dict):
                        att['safe_filename'] = f"KS_{item['announcement_id']}_{idx:02d}.pdf"
                        att['display_filename'] = att.get('display_filename', f"ì²¨ë¶€íŒŒì¼_{idx}")
                        fixed_attachments.append(att)
                if fixed_attachments:
                    update_data['attachment_urls'] = fixed_attachments
            
            # 2. ìš”ì•½ ê°œì„ 
            enhanced_summary = self.generate_enhanced_summary(item, crawled)
            if enhanced_summary and len(enhanced_summary) > 20:
                update_data['bsns_sumry'] = enhanced_summary
            
            # 3. í•´ì‹œíƒœê·¸ ìƒì„± (ë¬¸ìì—´ í˜•ì‹)
            hashtags = self.generate_hashtags(item, crawled)
            if hashtags:
                update_data['hash_tag'] = hashtags
            
            # 4. ìƒì„¸ ë‚´ìš© ì—…ë°ì´íŠ¸
            if crawled and crawled.get('content') and len(crawled['content']) > 100:
                update_data['pbanc_ctnt'] = crawled['content']
            
            # DB ì—…ë°ì´íŠ¸
            if update_data:
                self.supabase.table('kstartup_complete')\
                    .update(update_data)\
                    .eq('id', item['id'])\
                    .execute()
                
                logging.info(f"âœ… ID {item['id']} ({item.get('biz_pbanc_nm', 'Unknown')[:30]}) ê°œì„  ì™„ë£Œ")
                return True
            
            return False
            
        except Exception as e:
            logging.error(f"ì²˜ë¦¬ ì˜¤ë¥˜ (ID: {item['id']}): {e}")
            return False
    
    def run(self, limit=100):
        """ë©”ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        logging.info("="*60)
        logging.info("   K-Startup ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œì‘")
        logging.info("="*60)
        
        # 1. ê°œì„  í•„ìš” ë°ì´í„° ì¡°íšŒ
        items = self.get_data_to_enhance(limit)
        
        if not items:
            logging.info("ê°œì„ í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ë³‘ë ¬ ì²˜ë¦¬
        success_count = 0
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(self.process_item, item): item for item in items}
            
            for future in as_completed(futures):
                if future.result():
                    success_count += 1
                
                if success_count % 10 == 0:
                    logging.info(f"ì§„í–‰ ìƒí™©: {success_count}/{len(items)}")
        
        # 3. ê²°ê³¼ í†µê³„
        elapsed = time.time() - start_time
        
        logging.info("\n" + "="*60)
        logging.info("   ì²˜ë¦¬ ì™„ë£Œ")
        logging.info("="*60)
        logging.info(f"âœ… ê°œì„  ì™„ë£Œ: {success_count}/{len(items)}ê°œ")
        logging.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        if success_count > 0:
            logging.info(f"ğŸ“Š í‰ê·  ì†ë„: {success_count/elapsed:.1f}ê°œ/ì´ˆ")
        
        # 4. ê°œì„  í›„ í†µê³„
        self.print_final_stats()
    
    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        # unknown í™•ì¥ì ê°œìˆ˜
        unknown_count = self.supabase.table('kstartup_complete')\
            .select('id', count='exact')\
            .like('attachment_urls', '%unknown%')\
            .execute()
        
        # í•´ì‹œíƒœê·¸ ìˆëŠ” ë°ì´í„°
        with_hashtag = self.supabase.table('kstartup_complete')\
            .select('id', count='exact')\
            .neq('hash_tag', '')\
            .execute()
        
        # ì „ì²´ ë°ì´í„°
        total = self.supabase.table('kstartup_complete')\
            .select('id', count='exact')\
            .execute()
        
        logging.info("\nğŸ“Š í˜„ì¬ ë°ì´í„° í’ˆì§ˆ ìƒíƒœ")
        logging.info(f"ì „ì²´ ë°ì´í„°: {total.count}ê°œ")
        logging.info(f"Unknown í™•ì¥ì ë‚¨ì€ ê°œìˆ˜: {unknown_count.count}ê°œ")
        logging.info(f"í•´ì‹œíƒœê·¸ ë³´ìœ : {with_hashtag.count}ê°œ ({with_hashtag.count/total.count*100:.1f}%)")

if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì²˜ë¦¬ ê°œìˆ˜ ì§€ì • ê°€ëŠ¥
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else 50
    
    enhancer = KStartupEnhancer()
    enhancer.run(limit=limit)
